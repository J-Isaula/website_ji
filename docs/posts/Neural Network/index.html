<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Juan Isaula">
<meta name="dcterms.date" content="2024-01-24">

<title>Juan Isaula - Tipos de Arquitecturas de Redes Neuronales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Juan Isaula - Tipos de Arquitecturas de Redes Neuronales">
<meta property="og:description" content="">
<meta property="og:image" content="https://mm218.dev/posts/Neural Network/arq_rn.jpeg">
<meta property="og:site-name" content="Juan Isaula">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="Juan Isaula - Tipos de Arquitecturas de Redes Neuronales">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://mm218.dev/posts/Neural Network/arq_rn.jpeg">
<meta name="twitter:creator" content="@MikeMahoney218">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Juan Isaula</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers" rel="" target="">
 <span class="menu-text">Courses</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../presentations" rel="" target="">
 <span class="menu-text">UNAH Courses</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Dashboards</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sobre_juan.html" rel="" target="">
 <span class="menu-text">Historias</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#fundamentos-previos-a-la-comprensión-de-redes-neuronales" id="toc-fundamentos-previos-a-la-comprensión-de-redes-neuronales" class="nav-link active" data-scroll-target="#fundamentos-previos-a-la-comprensión-de-redes-neuronales">Fundamentos previos a la comprensión de Redes Neuronales</a>
  <ul class="collapse">
  <li><a href="#función-de-activación" id="toc-función-de-activación" class="nav-link" data-scroll-target="#función-de-activación">Función de Activación</a></li>
  <li><a href="#funciones-de-pérdida-y-optimización" id="toc-funciones-de-pérdida-y-optimización" class="nav-link" data-scroll-target="#funciones-de-pérdida-y-optimización">Funciones de Pérdida y Optimización</a></li>
  </ul></li>
  <li><a href="#tipos-de-redes-neuronales" id="toc-tipos-de-redes-neuronales" class="nav-link" data-scroll-target="#tipos-de-redes-neuronales">Tipos de Redes Neuronales</a>
  <ul class="collapse">
  <li><a href="#recurrent-neural-network-rnn" id="toc-recurrent-neural-network-rnn" class="nav-link" data-scroll-target="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a></li>
  <li><a href="#gated-recurrent-unit-network-gru" id="toc-gated-recurrent-unit-network-gru" class="nav-link" data-scroll-target="#gated-recurrent-unit-network-gru">Gated recurrent unit network (GRU)</a></li>
  <li><a href="#long-short-term-memory-network-lstm" id="toc-long-short-term-memory-network-lstm" class="nav-link" data-scroll-target="#long-short-term-memory-network-lstm">Long short-term memory network (LSTM)</a></li>
  <li><a href="#conclusiones" id="toc-conclusiones" class="nav-link" data-scroll-target="#conclusiones">CONCLUSIONES</a></li>
  <li><a href="#referencias" id="toc-referencias" class="nav-link" data-scroll-target="#referencias">REFERENCIAS</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tipos de Arquitecturas de Redes Neuronales</h1>
  <div class="quarto-categories">
    <div class="quarto-category">RNN</div>
    <div class="quarto-category">GRU</div>
    <div class="quarto-category">LSTM</div>
    <div class="quarto-category">PyTorch</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Juan Isaula </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 24, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Las arquitecturas de redes neuronales se refieren a los diseños estructurales y organizativos de redes neuronales artificiales (RNA). Estas arquitecturas determinan cómo se organiza la red, incluida la cantidad de capas, la cantidad de neuronas en cada capa, las conexiones entre neuoronas y las funciones de activación utilizadas. Se forman diferentes arquitecturas de redes neuronales alterando estos componentes estructurales para adaptarse a tareas o desafíos específicos. Si desea conocer los tipos de arquitectura de redes neuronales que debe conocer, este artículo es para usted. En este artículo, le explicaré los tipos de arquitecturas de redes neuronales en <code>Machine Learning</code> y cuándo elegirlas.</p>
<section id="fundamentos-previos-a-la-comprensión-de-redes-neuronales" class="level2">
<h2 class="anchored" data-anchor-id="fundamentos-previos-a-la-comprensión-de-redes-neuronales">Fundamentos previos a la comprensión de Redes Neuronales</h2>
<section id="función-de-activación" class="level3">
<h3 class="anchored" data-anchor-id="función-de-activación">Función de Activación</h3>
<p>Una función de activación es una función que se agrega a una red neuronal para ayudar a la red a aprender dependencias no lineales complejas. Una función de activación típica debe ser diferenciable y continua en todas partes. A continuación proporcionaré algunos ejemplos de funciones de activación utilizando la biblioteca <a href="https://pytorch.org/">PyTorch</a>.</p>
<section id="función-relu" class="level4">
<h4 class="anchored" data-anchor-id="función-relu">Función ReLU</h4>
<p><code>ReLU</code> o la función ReLU realiza una operación simple: <span class="math inline">\(y = \max (0, x)\)</span>. Aquí te proporcionó un ejemplo de uso de la función ReLU utilizando <code>PyTorch.</code></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>,steps<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>relu <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> relu(x)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"ReLU"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.plot(x.tolist(), y.tolist())</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="566" height="431"></p>
</div>
</div>
</section>
<section id="función-sigmoidea" class="level4">
<h4 class="anchored" data-anchor-id="función-sigmoidea">Función Sigmoidea</h4>
<p>Es una de las funciones de activación no lineal más comunes. La función sigmoidea se representa matemáticamente como:</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^x}
\]</span></p>
<p>Al igual que <code>ReLU</code>, la función <span class="math inline">\(\sigma\)</span> se puede construir simplemente usando <code>PyTorch</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>,steps<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>sigmoid <span class="op">=</span> torch.nn.Sigmoid()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> sigmoid(x)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Sigmoidea"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.plot(x.tolist(), y.tolist())</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="571" height="431"></p>
</div>
</div>
</section>
<section id="función-tanh" class="level4">
<h4 class="anchored" data-anchor-id="función-tanh">Función Tanh</h4>
<p>La función tangente hiperbólica es similar a la función sigmoidea, pero devuelve valores en el rango <span class="math inline">\((-1,1)\)</span>. El beneficio de <code>Tanh</code> sobre <span class="math inline">\(\sigma\)</span> es que las entradas negativas se asignarán estrictamente a negativa, y las entradas positivas se asignarán estrictamente a positivas:</p>
<p><span class="math display">\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</span></p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>torch.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>, steps <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tanh <span class="op">=</span> torch.nn.Tanh()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tanh(x)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Tanh'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x.tolist(),y.tolist())</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="590" height="431"></p>
</div>
</div>
<p>Las funciones de activación no lineales, como la <span class="math inline">\(\sigma\)</span> y <span class="math inline">\(\tanh\)</span> sufren de un gran problema computacional llamado problema de <strong><em>fuga de gradiente.</em></strong></p>
<p>La <strong><em>fuga de gradiente</em></strong> hace que sea muy difícil entrenar y ajustar los parámetros de las capas iniciales en la red. Este problema empeora a medida que aumenta el número de capas en la red.</p>
<p>La fuga de gradiente es la causa principal que hace que las activaciones sigmoideas o Tanh no sean adecuadas para los modelos de Deep Learning (aprendizaje profundo). La función de activación <code>ReLU</code> no sufre de gradiente de fuga porque la derivada siempre es 1 para entradas positivas. Así que siempre considere usar <code>ReLU</code> como la función de activación en los primeros borradores del diseño de su modelo.<br>
<br>
La creación de una arquitectura de red neuronal que se adapte más a un problema en particular es un arte. Existe una dirección de estudio separada en el aprendizaje profundo llamado <em><code>Búsqueda de arquitectura neural</code></em>, que automatiza la ingeniería de arquitectura de red: <a href="https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html." class="uri">https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html.</a> Pero incluso estos motores de búsqueda no pueden competir con las habilidades heurísticas humanas en el diseño todavía. Existen algunas técnicas que aumentan la probabilidad de mejorar el rendimiento de la red neuronal. Por supuesto, estas técnicas no garantizan la mejora en todos los casos. A veces incluso pueden empeorar el rendimiento de la red neuronal. Pero es probable que desarrolle una arquitectura de modelo robusta siguiendo estos enfoques.</p>
</section>
</section>
<section id="funciones-de-pérdida-y-optimización" class="level3">
<h3 class="anchored" data-anchor-id="funciones-de-pérdida-y-optimización">Funciones de Pérdida y Optimización</h3>
<section id="funciones-de-pérdida" class="level4">
<h4 class="anchored" data-anchor-id="funciones-de-pérdida">Funciones de Pérdida</h4>
<p>La función de pérdida calculará un error de red en cada iteración, mientras que la función de optimización determina <em>“cómo y en qué dirección cambiar los parámetros de peso”.</em></p>
<p>Hay una cantidad diversa de funciones de pérdida, cada una de ellas está destinada a una tarea en particular. Para el análisis de series de tiempo, hay tres funciones de pérdida principales:</p>
<ul>
<li><p><strong><em>Pérdida absoluta (L1):</em></strong> La pérdida absoluta es la métrica más simple de la distancia entre dos vectores:</p>
<p><span class="math display">\[
absolute loss = \frac{\sum |y_{actual} - y_{predicción}|}{n}
\]</span></p>
<p>En <code>PyTorch</code>, la función de pérdida absoluta se implementa de la siguiente manera:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([<span class="dv">1</span>,<span class="dv">2</span>]).<span class="bu">float</span>()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">5</span>]).<span class="bu">float</span>()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>abs_loss <span class="op">=</span> torch.nn.L1Loss()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>abs_error <span class="op">=</span> abs_loss(a,b)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'abs: </span><span class="sc">{</span>abs_error<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>abs: 1.5</code></pre>
</div>
</div></li>
<li><p><strong><em>Error cuadrático medio (MSE)</em></strong> <strong><em>(L2):</em></strong> Es la función de pérdida más utilizada para los problemas de predicción de series de tiempo:</p>
<p><span class="math display">\[
mean\_squared\_error =  \frac{\sum(y_{actual} - y_{predicted})^2}{n}
\]</span></p></li>
<li><p><strong><em>Pérdida suave (L1):</em></strong> es algo intermedio entre las funciones de pérdida absoluta y MSE. La pérdida absoluto (L1) es menos sensible a los valores atípicos que MSE:</p>
<p><span class="math display">\[
smooth\_loss(y^{\prime},y) = \frac{1}{n}\sum z_i
\]</span></p>
<p>donde <span class="math inline">\(y\)</span> es valor real, <span class="math inline">\(y\)</span> se predice, <span class="math inline">\(z_i\)</span> se define como:</p>
<p><span class="math display">\[ z =
\begin{equation}
\begin{matrix}
  \frac{0.5(y_{i}^{\prime} - y_i)^2}{\beta}, &amp; |y_{i}^{\prime} - y_i| &lt; \beta\\  
|y_{i}^{\prime} - y_i| - 0.5\beta, &amp; otro\_caso
  \end{matrix}
\end{equation}
\]</span></p></li>
</ul>
<p>La función de pérdida de L1 suave tiene un parámetro <span class="math inline">\(\beta\)</span>, es igual a 1 por defecto.</p>
</section>
<section id="optimizador" class="level4">
<h4 class="anchored" data-anchor-id="optimizador">Optimizador</h4>
<p>El objetivo principal de un optimizador es cambiar los parámetros de pesos del modelo para minimizar la función de pérdida. La selección de un optimizador adecuado depende completamente de la arquitectura de la red neuronal y los datos sobre los que ocurre el entrenamiento.</p>
<ul>
<li><p><code>Adagrad:</code> es un algoritmo de optimización basado en gradiente que adapta la tasa de aprendizaje a los parámetros. Realiza actualizaciones más pequeñas para los parámetros asociados con características frecuentes y actualizaciones más grandes para parámetros asociados con características raras.</p></li>
<li><p><code>Adadelta</code> es la versión avanzada del algoritmo de Adagrad. Adadelta busca minimizar su tasa de aprendizaje agresiva y monotónica que disminuye. En lugar de acumular todos los gradientes pasados.</p></li>
<li><p><code>Adam</code> es otro método de optimización que calcula las tasas de aprendizaje adaptativo para cada parámetro. Además de guardar un promedio exponencialmente en descomposición de gradientes cuadrados anteriores como Adadelta, Adam también mantiene un promedio exponencialmente de disminución de gradientes anteriores.</p></li>
</ul>
</section>
</section>
</section>
<section id="tipos-de-redes-neuronales" class="level2">
<h2 class="anchored" data-anchor-id="tipos-de-redes-neuronales">Tipos de Redes Neuronales</h2>
<p>Comenzaremos explorando algunas de las arquitecturas de redes neuronales más eficientes para el pronóstico de series de tiempo. Nos centraremos en la implementación de redes neuronales recurrentes (RNN), unidad recurrentes cerradas (GRU), redes de memoria a largo plazo (LSTM). Comprender los principios básicos de las RNN será una buena base para su aplicación directa y dominar otras arquitecturas similares. Trataremos de cubrir la lógica y el núcleo de cada arquitectura, su aplicación práctica y pros y contras.</p>
<p>Discutiremos los siguientes temas:</p>
<ul>
<li><p>Recurrent neural network (RNN)</p></li>
<li><p>Gated recurrent unit network (GRU)</p></li>
<li><p>Long short-term memory network (LSTM)</p></li>
</ul>
<section id="recurrent-neural-network-rnn" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
<p>RNN <em>(Red Neuronal Recurrente Estándar)</em> tiene un concepto de un estado oculto. Un estado oculto puede tratarse como memoria interna. El estado oculto no intenta recordar todos los valores pasados de la secuencia sino solo su efecto. Debido a la memoria interna, las RNN pueden recordar cosas importantes sobre su entrada, lo que les permite ser muy preciosos para predecir valores futuros.</p>
<p>Estudiemos la teoría de RNN de una manera más formal. En RNN, la secuencia de entrada se representa a traves de un bucle. Cuando toma una decisión, considera la entrada actual y también lo que ha aprendido de las entradas que recibio anteriormente. Veamos el gráfico computacional de RNN para comprender esta lógica:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figure 4.3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Gráfico Computacional de RNN</figcaption>
</figure>
</div>
<p>donde,</p>
<ul>
<li><p><span class="math inline">\(x_1, x_2, . . . , x_n\)</span> son la secuencia de entrada.</p></li>
<li><p><span class="math inline">\(h_i\)</span> es el estado oculto. <span class="math inline">\(h_i\)</span> es un vector de longitud <span class="math inline">\(h\)</span>.</p></li>
<li><p><code>RNN Cell</code> representa la capa de red neuronal que calcula la siguiente función: <span class="math inline">\(h_t = \tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{(t-1)} + b_{hh})\)</span></p></li>
</ul>
<p>Podemos ver a detalle la RNN Cell:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figure 4.4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Gráfico computacional de RNN Cell</figcaption>
</figure>
</div>
<p>La RNN Cell combina información sobre el valor actual de la secuencia <span class="math inline">\(x_i\)</span> y el estado previamente oculto <span class="math inline">\(h_{i-1}\)</span>. La RNN Cell, devuelve un estado oculto actualizado <span class="math inline">\(h_i\)</span> después de aplicar la función de activación.</p>
<p>La RNN tiene los siguientes parámetros, que se ajustan durante el entrenamiento:</p>
<ul>
<li><p><span class="math inline">\(W_{ih}\)</span> pesos ocultos de entrada</p></li>
<li><p><span class="math inline">\(b_{ih}\)</span> sesgos oculto de entrada</p></li>
<li><p><span class="math inline">\(W_{hh}\)</span> pesos ocultos - ocultos</p></li>
<li><p><span class="math inline">\(B_{hh}\)</span> sesgos oculto - oculto</p></li>
</ul>
<p><strong><em>Nota:</em></strong> <em>Un error común ocurre cuando los subíndices en los parámetros RNN</em> <span class="math inline">\((W_{ih}, b_{ih}, W_{hh}, b_{hh})\)</span> <em>se interpretan como una dimensión de índice o tensor. No, son solo la abreviatura de entrada-oculto</em> <span class="math inline">\((h_í)\)</span> <em>y oculto-oculto</em> <span class="math inline">\((h)\)</span><em>. El mismo principio aplica a los parámetros de otros modelos: <code>GRU</code> y <code>LSTM</code>.</em></p>
<p>En ocasiones, los cientificos de datos utilizan la siguiente representación de las RNN:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figure 4.5.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Visualización alternativa de RNN</figcaption>
</figure>
</div>
<p>El gráfico que se muestra puede dar lugar a algunos malentendidos, y estoy tratando de evitar esto. Pero si este tipo de gráfico se adapta a tu intuición, entonces úsalo sin ninguna duda.<br>
</p>
<p>Ahora estamos listos para examinar una implementación de RNN utilizando <a href="https://pytorch.org/">PyTorch</a></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNN(nn.Module):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                 hidden_size,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                 in_size <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                 out_size <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            input_size <span class="op">=</span> in_size,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> hidden_size,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            batch_first <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_size, out_size)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        out, _ <span class="op">=</span> <span class="va">self</span>.rnn(x, h)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        last_hidden_states <span class="op">=</span> out[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc(last_hidden_states)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, last_hidden_states</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note que nuestro modelo devuelve dos salidas: predicción y estado oculto. Es crucial reutilizar los estados ocultos durante la evaluación RNN. Utilizaremos conjuntos de datos de consumo de energía por hora ( <a href="https://www.kaggle.com/robikscube/Hourly-energy-Consumed" class="uri">https://www.kaggle.com/robikscube/Hourly-energy-Consumed</a>) para la implementación de RNN.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'AEP_hourly.csv'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> df[<span class="st">'AEP_MW'</span>].astype(<span class="bu">int</span>).values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)[<span class="op">-</span><span class="dv">3000</span>:]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'AEP Hourly'</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.plot(ts[:<span class="dv">500</span>])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="592" height="431"></p>
</div>
</div>
<p>Podemos ver en que esta es una serie de tiempo realmente complicada. Tiene varios factores de estacionalidad con picos apenas predecibles.</p>
<p>A continuación, voy a mostrarte como se desempeña RNN en la serie de tiempo AEP Hourly:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametros globales</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> <span class="dv">240</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Longitud del conjunto de datos de prueba</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>test_ts_len <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># tamaño del estado oculto</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>rnn_hidden_size <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># tasa de aprendizaje de optimizador</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>training_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sliding_window(ts, features):</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> []</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> []</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(features <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(ts) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        X.append(ts[i <span class="op">-</span> (features <span class="op">+</span> <span class="dv">1</span>):i <span class="op">-</span> <span class="dv">1</span>])</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        Y.append([ts[i <span class="op">-</span> <span class="dv">1</span>]])</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_training_datasets(ts, features, test_len):</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> sliding_window(ts, features)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    X_train, Y_train, X_test, Y_test <span class="op">=</span> X[<span class="dv">0</span>:<span class="op">-</span>test_len],<span class="op">\</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>                                       Y[<span class="dv">0</span>:<span class="op">-</span>test_len],<span class="op">\</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>                                       X[<span class="op">-</span>test_len:],<span class="op">\</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>                                       Y[<span class="op">-</span>test_len:]</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    train_len <span class="op">=</span> <span class="bu">round</span>(<span class="bu">len</span>(ts) <span class="op">*</span> <span class="fl">0.7</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    X_train, X_val, Y_train, Y_val <span class="op">=</span> X_train[<span class="dv">0</span>:train_len],<span class="op">\</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>                                     X_train[train_len:],<span class="op">\</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>                                     Y_train[<span class="dv">0</span>:train_len],<span class="op">\</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>                                     Y_train[train_len:]</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    x_train <span class="op">=</span> torch.tensor(data <span class="op">=</span> X_train).<span class="bu">float</span>()</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> torch.tensor(data <span class="op">=</span> Y_train).<span class="bu">float</span>()</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    x_val <span class="op">=</span> torch.tensor(data <span class="op">=</span> X_val).<span class="bu">float</span>()</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    y_val <span class="op">=</span> torch.tensor(data <span class="op">=</span> Y_val).<span class="bu">float</span>()</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    x_test <span class="op">=</span> torch.tensor(data <span class="op">=</span> X_test).<span class="bu">float</span>()</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> torch.tensor(data <span class="op">=</span> Y_test).<span class="bu">float</span>()</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_train, x_val, x_test,<span class="op">\</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>           y_train.squeeze(<span class="dv">1</span>), y_val.squeeze(<span class="dv">1</span>), y_test.squeeze(<span class="dv">1</span>)</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Preparando datos para entrenamiento</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>scaled_ts <span class="op">=</span> scaler.fit_transform(ts)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>x_train, x_val, x_test, y_train, y_val, y_test <span class="op">=\</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    get_training_datasets(scaled_ts, features, test_ts_len)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicialización del modelo </span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RNN(hidden_size <span class="op">=</span> rnn_hidden_size)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\juani\AppData\Local\Temp\ipykernel_9444\1156180628.py:50: UserWarning:

Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\b\abs_bao0hdcrdh\croot\pytorch_1675190257512\work\torch\csrc\utils\tensor_new.cpp:204.)
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>RNN(
  (rnn): RNN(1, 24, batch_first=True)
  (fc): Linear(in_features=24, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenamiento</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params <span class="op">=</span> model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>mse_loss <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>min_val_loss <span class="op">=</span> sys.maxsize</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>training_loss <span class="op">=</span> []</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>validation_loss <span class="op">=</span> []</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(training_epochs):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    prediction, _ <span class="op">=</span> model(x_train)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mse_loss(prediction, y_train)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    val_prediction, _ <span class="op">=</span> model(x_val)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> mse_loss(val_prediction, y_val)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    training_loss.append(loss.item())</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(val_loss.item())</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss.item() <span class="op">&lt;</span> min_val_loss:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        best_model <span class="op">=</span> copy.deepcopy(model)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        min_val_loss <span class="op">=</span> val_loss.item()</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: train - </span><span class="sc">{</span><span class="bu">round</span>(loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">, '</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'val: - </span><span class="sc">{</span><span class="bu">round</span>(val_loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 0: train - 0.377, val: - 0.0866
epoch 50: train - 0.0061, val: - 0.0133
epoch 100: train - 0.0021, val: - 0.0044
epoch 150: train - 0.0018, val: - 0.0034
epoch 200: train - 0.0015, val: - 0.003
epoch 250: train - 0.0014, val: - 0.0027
epoch 300: train - 0.0013, val: - 0.0026
epoch 350: train - 0.0012, val: - 0.0025
epoch 400: train - 0.0012, val: - 0.0024
epoch 450: train - 0.0012, val: - 0.0024</code></pre>
</div>
</div>
<p>Y aquí llegamos al punto más difícil. Debe pasar el estado oculto al modelo RNN cuando lo evalua. La forma más sencilla de calentar el estado oculto es ejecutar el modelo en los datos de validación una vez y pasar un estado oculto cálido a través de cada iteración y por último evaluamos el modelo que construimos en el conjunto de datos de prueba.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>best_model.<span class="bu">eval</span>()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>_, h_list <span class="op">=</span> best_model(x_val)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> (h_list[<span class="op">-</span><span class="dv">1</span>, :]).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> []</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> test_seq <span class="kw">in</span> x_test.tolist():</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.Tensor(data <span class="op">=</span> [test_seq])</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    y, h <span class="op">=</span> best_model(x, h.unsqueeze(<span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    unscaled <span class="op">=</span> scaler.inverse_transform(np.array(y.item()).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    predicted.append(unscaled)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> scaler.inverse_transform(y_test.tolist())</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Conjunto de datos prueba - RNN"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.plot(real, label <span class="op">=</span> <span class="st">'real'</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.plot(predicted, label <span class="op">=</span> <span class="st">'predicción'</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="592" height="431"></p>
</div>
</div>
<p>RNN muestra un gran rendimiento en el conjunto de datos de prueba. El modelo que hemos entrenado predice picos estacionales con mucha precisión.</p>
<p>Y finalmente, examinamos el proceso de entrenamiento en sí.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Desempeño RNN'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.plot(training_loss, label <span class="op">=</span> <span class="st">'Entrenamiento'</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.plot(validation_loss, label <span class="op">=</span> <span class="st">'validación'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-1.png" width="599" height="450"></p>
</div>
</div>
<p>El proceso del entrenamiento es suave sin picos agudos e impredecibles.</p>
<p>Ahora, podemos establecer con confianza la promesa y la efectividad de la aplicación de RNN a los problemas de pronósticos de la serie temporal.</p>
<p>A pesar de todas las ventajas de RNN, tiene desventajas significativas:</p>
<ul>
<li><p>Debido a la complejidad computacional, sufren problemas de gradiente de fuga. El proceso de entrenamiento se vuelve demasiado lento. El problema del gradiente de fuga es un problema común a todas las RNN.</p></li>
<li><p>El estado oculto se actualiza en cada iteración, lo que dificulta el almacenamiento de información a largo plazo en RNN. Las arquitecturas <code>GRU</code> y <code>LSTM</code> resuelven este problema. Tienen enfoques similares sobre cómo almacenar información a largo plazo.</p></li>
</ul>
</section>
<section id="gated-recurrent-unit-network-gru" class="level3">
<h3 class="anchored" data-anchor-id="gated-recurrent-unit-network-gru">Gated recurrent unit network (GRU)</h3>
<p>La GRU es es una versión avanzada de la RNN clásica. El propósito principal de GRU es almacenar información a largo plazo. En breve exploraremos como GRU logra esto.</p>
<p>La forma más fácil de almacenar información a largo plazo en un estado oculto es restringir las actualizaciones ocultas sobre cada iteración. Este enfoque evitará sobrescribir información importante a largo plazo.</p>
<p>Puede encontrar la siguiente definición de GRU en internet:</p>
<p>Se comienza calculando la puerta de actualización <span class="math inline">\(z_t\)</span> para el peso de tiempo <span class="math inline">\(t\)</span> usando la fórmula:</p>
<p><span class="math display">\[
\begin{eqnarray*}
z_{t} &amp;=&amp; \sigma(W^{z}x_t + U^{z}h_{t-1}) \hspace{1cm} \mbox{Puerta de actualización}\\[0.2cm]
\end{eqnarray*}
\]</span></p>
<p>lo que sucede aquí es que cuando <span class="math inline">\(x_t\)</span> se conecta a la unidad de red, se multiplica por su propio peso <span class="math inline">\(W^{z}\)</span>. Lo mismo ocurre con <span class="math inline">\(h_{t-1}\)</span>, que contiene la información de las unidades <span class="math inline">\(t-1\)</span> anteriores y se múltiplica por su propio peso <span class="math inline">\(U^{z}\)</span>. Ambos resultados se suman y se aplica una función de activación sigmoidea <span class="math inline">\((\sigma)\)</span> para acotar el resultado entre 0 y 1.</p>
<p><img src="fig_gru_1.png" class="img-fluid"></p>
<p>La puerta de actualización ayuda al modelo a determinar cuánta información pasada (de pasos de tiempo anteriores) debe transmitirse al futuro. Esto es muy poderoso porque el modelo puede decidir copiar toda la la información del pasado y eliminar el riesgo de que desaparezca el problema de fuga del gradiente.</p>
<p>Luego continuamos con Restablecer puerta:</p>
<p>Básicamente, esta puerta se utiliza desde el modelo para decidir cuánta información pasada se debe olvidar. Para calcularlo utilizamos:</p>
<p><span class="math display">\[
r_t = \sigma(W^{r}x_t + U^{r}h_{t-1})\hspace{1cm} \mbox{Restablecer puerta}
\]</span></p>
<p>Esta fórmula es la misma que la de la puerta de actualización. La diferencia viene en los pesos y el uso de la puerta, que veremos en un momento.</p>
<p><img src="fig_2_gru.png" class="img-fluid"></p>
<p>Como antes, conectamos <span class="math inline">\(h_{t-1} - \mbox{linea azul}\)</span> y <span class="math inline">\(x_{t} - \mbox{linea violeta}\)</span>, los multiplicamos con sus pesos correspondientes, sumamos los resultados y aplicamos la función sigmoidea.</p>
<p><strong><em>Contenido de la memoria actual:</em></strong></p>
<p>veamos como afectarán exactamente las puertaas al resultado final. Primero, comenzamos con el uso de la puerta de reinicio. Introducimos un nuevo contenido de memoria que utilizará la puerta de reinicio para almacenar la información del pasado. Se calcula de la siguiente manera:</p>
<p><span class="math display">\[
h_{t}^{\prime} = tanh(Wx_{t} + r_{t}\odot U h_{t-1})
\]</span></p>
<ol type="1">
<li><p>Multiplique la entrada <span class="math inline">\(x_t\)</span> con un peso <span class="math inline">\(W\)</span> y <span class="math inline">\(h_{t-1}\)</span> con un peso <span class="math inline">\(U\)</span>.</p></li>
<li><p>Calcule el producto de Hadamard (por elementos) entre la puerta de reinicio <span class="math inline">\(r_t\)</span> y <span class="math inline">\(Uh_{t-1}\)</span>. Eso determinará qué eliminar de los pasos de tiempo anterior. Digamos que tenemos un problema de análisis de sentimientos para determinar la opinión de una persona sobre un libro a partir de una reseña que escribió. El texto comienza con <em>“Este es un libro de fantasía que ilustra…”</em> y después de un par de párrafos termina con <em>“No disfruté mucho el libro porque creo que captura demasiados detalles”. Para determinar el nivel general de satisfacción con el libro sólo necesitamos la última parte de la reseña. En ese caso, a medida que la red neuronal se acerque al final del texto, aprenderá a asignar un vector</em> <span class="math inline">\(r_t\)</span> cercano a 0, eliminando el pasado y centrándose solo en las últimas oraciones.</p></li>
<li><p>Resuma los resultados de los pasos 1 y 2.</p></li>
<li><p>Aplicar la función de activación no lineal tanh.</p></li>
</ol>
<p>Puedes ver claramente los pasos aquí:</p>
<p><img src="fig_3_gru.png" class="img-fluid"></p>
<p>Hacemos una multiplicación por elementos de <span class="math inline">\(h_{t-1} - \mbox{línea azul}\)</span> y <span class="math inline">\(r_t - \mbox{línea naranja}\)</span> y luego sumamos el resultado - linea rosa con la entrada <span class="math inline">\(x_t -\)</span> línea morada. Finalmente, tanh se usa para producir <span class="math inline">\(h_{t}^{\prime}:\)</span> línea verde brillante.</p>
<p><strong><em>Memoria final en el paso de tiempo actual</em></strong></p>
<p>Como último paso, la red necesita calcular <span class="math inline">\(h_{t}\)</span>, el vector que contiene información para la unidad actual y la transmite a la red. Para hacer eso, se necesita la puerta de actualización. Determina qué recopilar el contenido de la memoria actual <span class="math inline">\((h_t^{\prime})\)</span> y qué de los pasos anteriores <span class="math inline">\((h_{(t-1)})\)</span>. Eso se hace de la siguiente manera:</p>
<p><span class="math display">\[
h_t = z_t\odot h_{t-1} + (1 - z_t)\odot h_{t}^{\prime}
\]</span></p>
<ol type="1">
<li><p>Aplique la multiplicación por elementos a la puerta de actualización <span class="math inline">\(z_t\)</span> y <span class="math inline">\(h_{(t-1)}\)</span>.</p></li>
<li><p>Aplique la multiplicación por elementos a <span class="math inline">\((1- z_t)\)</span> y <span class="math inline">\(h_{t}^{\prime}\)</span>.</p></li>
<li><p>Sume los resultados de los pasos 1 y 2.</p></li>
</ol>
<p>Pongamos el ejemplo de la reseña del equilibrio. En esta ocasión, la información más relevante se situa al inicio del texto. El modelo puede aprender a establecer el vector <span class="math inline">\(z_t\)</span> cerca de 1 y conservar la mayor parte de la información anterior. Dado que <span class="math inline">\(z_t\)</span> estará cerca de 1 en este paso de tiempo, <span class="math inline">\((1-z_t)\)</span> estará cerca de 0, lo que ignorará gran parte del contenido actual (en este caso, la última parte de la reseña que explica la trama del libro), lo cual es irrelevante para nuestra predicción.</p>
<p>Aquí hay una ilustración que enfatiza la ecuación anterior:</p>
<p><img src="fig_4_gru.png" class="img-fluid"></p>
<p>A continuación, puede ver cómo <span class="math inline">\(z_t\)</span> (línea verde) para calcular <span class="math inline">\(1 - z_t\)</span> que combinado con <span class="math inline">\(h_{t}^{\prime}\)</span> (línea verde brillante), produce un resultado en la línea roja oscura. <span class="math inline">\(z_t\)</span> también se usa con <span class="math inline">\(h_{t-1} - \mbox{línea azul}\)</span> en una multiplicación de elementos. Finalmente, <span class="math inline">\(h_{t}:\)</span> la línea azul es el resultado de la suma de las salidas correspondientes a las líneas rojas brillantes y oscuras.</p>
<p>Ahora puede ver cómo las GRU pueden almacenar y filtrar la información utilizando sus puertas de actualización y reinicio. Eso elimina el problema del gradiente de fuga, ya que el modelo no elimina la nueva entrada cada vez, sino que mantiene la información relevante y la pasa a los siguientes pasos de la red. <strong><em>Si se les entrena cuidadosamente, pueden desempeñarse extremadamente bien incluso en escenarios complejos.</em></strong></p>
<p>El modelo de predicción <code>GRU</code> es muy similar al <code>RNN</code>. Veamos su desempeño utilizando la misma data que el casa <code>RNN</code>.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> <span class="dv">240</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>test_ts_len <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>gru_hidden_size <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>training_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GRU(nn.Module):</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>                 hidden_size,</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>                 in_size <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>                 out_size <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GRU, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gru <span class="op">=</span> nn.GRU(</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            input_size <span class="op">=</span> in_size,</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> hidden_size,</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>            batch_first <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_size, out_size)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        out, _ <span class="op">=</span> <span class="va">self</span>.gru(x, h)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        last_hidden_states <span class="op">=</span> out[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc(last_hidden_states)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, last_hidden_states</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicializando el modelo GRU</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GRU(hidden_size <span class="op">=</span> gru_hidden_size)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenamiento</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params <span class="op">=</span> model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>mse_loss <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>min_val_loss <span class="op">=</span> sys.maxsize</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>training_loss <span class="op">=</span> []</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>validation_loss <span class="op">=</span> []</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(training_epochs):</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    prediction, _ <span class="op">=</span> model(x_train)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mse_loss(prediction, y_train)</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>    val_prediction, _ <span class="op">=</span> model(x_val)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> mse_loss(val_prediction, y_val)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>    training_loss.append(loss.item())</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(val_loss.item())</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss.item() <span class="op">&lt;</span> min_val_loss:</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        best_model <span class="op">=</span> copy.deepcopy(model)</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        min_val_loss <span class="op">=</span> val_loss.item()</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: train - </span><span class="sc">{</span><span class="bu">round</span>(loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">, '</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'val: - </span><span class="sc">{</span><span class="bu">round</span>(val_loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>best_model.<span class="bu">eval</span>()</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>_, h_list <span class="op">=</span> best_model(x_val)</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> (h_list[<span class="op">-</span><span class="dv">1</span>, :]).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> []</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> test_seq <span class="kw">in</span> x_test.tolist():</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.Tensor(data <span class="op">=</span> [test_seq])</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>    y, h <span class="op">=</span> best_model(x, h.unsqueeze(<span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>    unscaled <span class="op">=</span> scaler.inverse_transform(np.array(y.item()).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>    predicted.append(unscaled)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 0: train - 0.0626, val: - 0.0326
epoch 50: train - 0.0017, val: - 0.0026
epoch 100: train - 0.0012, val: - 0.0024
epoch 150: train - 0.0012, val: - 0.0023
epoch 200: train - 0.0011, val: - 0.0022
epoch 250: train - 0.0011, val: - 0.0022
epoch 300: train - 0.0011, val: - 0.0023
epoch 350: train - 0.0011, val: - 0.0023
epoch 400: train - 0.0011, val: - 0.0022
epoch 450: train - 0.0011, val: - 0.0022</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> scaler.inverse_transform(y_test.tolist())</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Conjunto de datos prueba - GRU"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.plot(real, label <span class="op">=</span> <span class="st">'real'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.plot(predicted, label <span class="op">=</span> <span class="st">'predicción'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="592" height="431"></p>
</div>
</div>
<p>Vemos que el modelo <code>GRU</code> imita el comportamiento original de la serie temporal con bastante precisión.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Desempeño GRU'</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>plt.plot(training_loss, label <span class="op">=</span> <span class="st">'Entrenamiengto'</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plt.plot(validation_loss, label <span class="op">=</span> <span class="st">'validación'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-1.png" width="599" height="450"></p>
</div>
</div>
<p>Las pérdidas de entrenamiento y validación tienen descenso asintótico con un brecha natural constante entre ellas. Podemos concluir que el modelo realmente aprende el comportamiento de la serie temporal.</p>
</section>
<section id="long-short-term-memory-network-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-network-lstm">Long short-term memory network (LSTM)</h3>
<p>La red <code>LSTM</code> se ha desarrollado para superar el problema de fuga de gradiente en <code>RNN</code> al mejorar el flujo de gradiente de la red. Debe mencionarse que la arquitectura apareció mucho antes que la GRU. La arquitectura LSTM se desarrolló en 1997, y el GRU se propueso en 2014. El diseño GRU es más simple y más comprensible que LSTM. Es por eso que comenzamos nuestro estudio examinando primero GRU.</p>
<p>Como su nombre lo índica, LSTM aborda los mismos problemas de memoria a corto y largo plazo que GRU. A nivel global, el flujo computacional del LSTM se ve de la siguiente manera:</p>
<p><img src="Figure 4.16.png" class="img-fluid"></p>
<p>LSTM funciona sobre los principios similares que GRU pero tiene más variables. RNN y GRU solo pasan un estado oculto <span class="math inline">\(h_t\)</span> a través de cada iteración. Pero LSTM pasa dos vectores:</p>
<ul>
<li><p><span class="math inline">\(h_t\)</span> estado oculto (memoria a corto plazo)</p></li>
<li><p><span class="math inline">\(c_t\)</span> estado de celda (memoria a largo plazo)</p></li>
</ul>
<p>Las salidas de <code>LSTM Cell</code> se calculan a través de las fórmulas:</p>
<p><span class="math display">\[
\begin{eqnarray*}
i_t &amp;=&amp; \sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})\\[0.2cm]
f_t &amp;=&amp; \sigma(W_{ii}x_{t} + b_{if} + W_{hf}h_{t-1} + b_{hf})\\[0.2cm]
g_t &amp;=&amp; tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hn})\\[0.2cm]
o_t &amp;=&amp; \sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})\\[0.2cm]
c_t &amp;=&amp; f_t \circ c_{t-1} + i_t\circ g_t\\[0.2cm]
h_t &amp;=&amp; o_t \circ tanh(c_t)
\end{eqnarray*}
\]</span></p>
<p>donde:</p>
<ul>
<li><p><span class="math inline">\(\sigma\)</span> es la función sigmoidea</p></li>
<li><p><span class="math inline">\(\circ\)</span> es el producto de Hadamard</p></li>
</ul>
<p>En cuanto a las variables:</p>
<ul>
<li><p><span class="math inline">\(i_t~(puerta de entrada)\)</span> es la variable que se utiliza para actualizar el estado <span class="math inline">\(c_t\)</span>. El estado previamente oculto <span class="math inline">\(h_t\)</span> y la secuencia <span class="math inline">\(x_t\)</span> se dan como entradas a una función sigmoidea <span class="math inline">\((\sigma)\)</span>. Si la salida está cerca de 1, entonces la información es más importante.</p></li>
<li><p><span class="math inline">\(f_t ~ (puerta~de~olvido)\)</span> es la variable que decide que información debe olvidarse en el estado <span class="math inline">\(c_t\)</span>. El estado <span class="math inline">\(h_t\)</span> de estado previamente oculto y la secuencia <span class="math inline">\(x_t\)</span> se dan como entradas a una función sigmoidea. Si la salida <span class="math inline">\(f_t\)</span> está cerca de cero, la información se puede olvidar, mientras que si la salida está cerca de 1, la información debe almacenarse o recordarse.</p></li>
<li><p><span class="math inline">\(g_t\)</span> representa información importante potencialmente nueva para el estado <span class="math inline">\(c_t\)</span>.</p></li>
<li><p><span class="math inline">\(c_t ~ (estado~celda)\)</span> es una suma de:</p>
<ul>
<li><p>estado de celda anterior <span class="math inline">\(c_{t-1}\)</span> con información olvidada <span class="math inline">\(f_t\)</span>.</p></li>
<li><p>nueva información de <span class="math inline">\(g_t\)</span> seleccionada por <span class="math inline">\(i_t\)</span></p></li>
</ul></li>
<li><p><span class="math inline">\(o_t ~ (puerta~de~salida)\)</span> es la variable para actualizar el estado oculto <span class="math inline">\(h_t\)</span>.</p></li>
<li><p><span class="math inline">\(h_t ~(estado~oculto)\)</span> es el siguiente estado oculto que se calcula eligiendo la información importante del estado de celda o celular <span class="math inline">\(c_t\)</span>.</p></li>
</ul>
<p>A continuación te muestro el gráfico computacional de la celda LSTM:</p>
<p><img src="Figure 4.17 .png" class="img-fluid"></p>
<p>LSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:</p>
<ul>
<li><p><span class="math inline">\(W_{ii}, W_{hi}, W_{if}, W_{hf}, W_{ig}, W_{hg}, W_{io}, W_{ho}\)</span> estos son los pesos.</p></li>
<li><p><span class="math inline">\(b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{hg}, b_{io}, b_{ho}\)</span> estos son sesgos.</p></li>
</ul>
<p>Ahora examinemos la implementación de Pytorch del modelo de predicción LSTM:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTM(nn.Module):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                 hidden_size,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                 in_size <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                 out_size <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LSTM, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            input_size <span class="op">=</span> in_size,</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            hidden_size <span class="op">=</span> hidden_size,</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            batch_first <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_size, out_size)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        out, h <span class="op">=</span> <span class="va">self</span>.lstm(x, h)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        last_hidden_states <span class="op">=</span> out[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc(last_hidden_states)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Como vemos, la implementación del modelo <code>LSTM</code> es bastante similar a las implementaciones de <code>RNN</code> y <code>GRU</code>.</p>
<p>Probaremos el modelo LSTM con el siguiente conjunto de datos de la serie tiempo de consumo de energía por hora).</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'NI_hourly.csv'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> df[<span class="st">'NI_MW'</span>].astype(<span class="bu">int</span>).values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)[<span class="op">-</span><span class="dv">3000</span>:]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'NI Hourly'</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>plt.plot(ts[:<span class="dv">500</span>])</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-16-output-1.png" width="592" height="431"></p>
</div>
</div>
<p>Veamos el modelo en acción:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> <span class="dv">240</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>test_ts_len <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>lstm_hidden_size <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>training_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Preparar el conjunto de datos para el entrenamiento </span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>scaled_ts <span class="op">=</span> scaler.fit_transform(ts)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>x_train, x_val, x_test, y_train, y_val, y_test <span class="op">=\</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    get_training_datasets(scaled_ts, features, test_ts_len)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicializando el modelo </span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LSTM(hidden_size <span class="op">=</span> lstm_hidden_size)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenamiento </span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params <span class="op">=</span> model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>mse_loss <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>min_val_loss <span class="op">=</span> sys.maxsize</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>training_loss <span class="op">=</span> []</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>validation_loss <span class="op">=</span> []</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(training_epochs):</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    prediction, _ <span class="op">=</span> model(x_train)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mse_loss(prediction, y_train)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>    val_prediction, _ <span class="op">=</span> model(x_val)</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> mse_loss(val_prediction, y_val)</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    training_loss.append(loss.item())</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(val_loss.item())</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss.item() <span class="op">&lt;</span> min_val_loss:</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>        best_model <span class="op">=</span> copy.deepcopy(model)</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        min_val_loss <span class="op">=</span> val_loss.item()</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: train - </span><span class="sc">{</span><span class="bu">round</span>(loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">, '</span></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f'val: - </span><span class="sc">{</span><span class="bu">round</span>(val_loss.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 0: train - 0.0979, val: - 0.087
epoch 10: train - 0.0253, val: - 0.0255
epoch 20: train - 0.0131, val: - 0.0119
epoch 30: train - 0.0056, val: - 0.0059
epoch 40: train - 0.0032, val: - 0.0043
epoch 50: train - 0.0026, val: - 0.0029
epoch 60: train - 0.002, val: - 0.0025
epoch 70: train - 0.0018, val: - 0.0023
epoch 80: train - 0.0016, val: - 0.0021
epoch 90: train - 0.0014, val: - 0.0019</code></pre>
</div>
</div>
<p>Para una evaluación del modelo LSTM, necesitamos pasar un estado celular y estado oculto.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>best_model.<span class="bu">eval</span>()</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    _, h_list <span class="op">=</span> best_model(x_val)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="bu">tuple</span>([(h[<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, :]).unsqueeze(<span class="op">-</span><span class="dv">2</span>).unsqueeze(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>               <span class="cf">for</span> h <span class="kw">in</span> h_list])</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    predicted <span class="op">=</span> []</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> test_seq <span class="kw">in</span> x_test.tolist():</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.Tensor(data <span class="op">=</span> [test_seq])</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        y, h <span class="op">=</span> best_model(x, h)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        unscaled <span class="op">=</span> scaler.inverse_transform(</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>            np.array(y.item()).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        predicted.append(unscaled)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> scaler.inverse_transform(y_test.tolist())</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Conjunto de prueba - LSTM"</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>plt.plot(real, label <span class="op">=</span> <span class="st">'real'</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>plt.plot(predicted, label <span class="op">=</span> <span class="st">'predicción'</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-18-output-1.png" width="592" height="431"></p>
</div>
</div>
<p>LSTM captura muy bien el comportamiento de las series temporales para hacer predicciones precisas.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Desempeño LSTM'</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.plot(training_loss, label <span class="op">=</span> <span class="st">'Entrenamiento'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.plot(validation_loss, label <span class="op">=</span> <span class="st">'validación'</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" width="599" height="450"></p>
</div>
</div>
<p>Mirando, concluimos que detuvimos el proceso de entrenamiento demasiado temprano. Obtenemos modelos más precisos si establecemos más epocas (epoch) para el entrenamiento.</p>
</section>
<section id="conclusiones" class="level3">
<h3 class="anchored" data-anchor-id="conclusiones">CONCLUSIONES</h3>
<p>Pudimos ver que las redes neuronales recurrentes muestran excelentes resultados y son adecuadas para problemas de pronósticos de series de tiempo.</p>
<p>Las Redes Neuronales Recurrentes son la técnica muy popular de aprendizaje profundo (Deep Learning) para el pronóstico de series de tiempo, ya que permiten producir predicciones confiables en series de tiempo en diversos problemas. El principal problema con RNN es que sufre el problema de fuga de gradiente cuando se aplica a secuencia largas, y no tiene una herramienta de memoria a largo plazo. Se desarrollaron LSTM y GRU para evitar el problema de gradiente de RNN con el uso de puertas que regulan el flujo de información e implementan el almacenamiento de memoria a largo plazo. El uso de LSTM y GRU ofrece resultados notables, pero LSTM y GRU no siempre funcionan mejor que RNN.</p>
<ul>
<li><p><code>RNN</code> tiene un estado oculto que puede tratarse como una memoria interna de la secuencia de entrada.</p></li>
<li><p><code>RNN</code> vuelve a calcular el estado oculto después de procesar cada nuevo valor de entrada de forma recurrente.</p></li>
<li><p><code>RNN</code> sufre un problema de fuga de gradiente.</p></li>
<li><p><code>RNN</code> actualiza un estado oculto en cada iteración. Por tanto, no tiene memoria a largo plazo.</p></li>
<li><p><code>GRU</code> implementa la puerta de reinicio, que rechaza algunas actualizaciones en un estado oculto.</p></li>
<li><p><code>LSTM</code> pasa dos vectores a través de cada iteración: <em>estado oculto</em> y <em>estado de celda.</em></p></li>
</ul>
</section>
<section id="referencias" class="level3">
<h3 class="anchored" data-anchor-id="referencias">REFERENCIAS</h3>
<ul>
<li><p>Time Series Forecasting Using Deep Learning - Ivan Gridin</p></li>
<li><p><a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be">Understanding GRU Networks</a></p></li>
</ul>
</section>
<section id="section" class="level3">
<h3 class="anchored" data-anchor-id="section"></h3>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="Isaula/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© Copyright 2022 Juan Isaula</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>