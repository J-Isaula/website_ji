[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Hola!\nSoy Juan, un matemático orientado al área estadística, especialista en ciencia de datos y analista actuarial que ama programar en Python y R, disfruto tomar café acompañado de pláticas atrayentes. Me fascina el campo del Machine Learning y Deep Learning, la mayor parte de mis trabajos están orientados al desarrollo de modelos estadísticos aplicados a diferentes áreas (economía, medicina, finanzas & actuaría).\nEn este sitio mantengo información sobre cursos en los cuales soy colaborador, algunos papers que llaman mucho mi atención, mi blog personal donde podra encontrar post que le puden despertar su interes en el modelado estadístico con el uso de herramientas de aprendizaje automático."
  },
  {
    "objectID": "posts/estructuras_mercado/index.html",
    "href": "posts/estructuras_mercado/index.html",
    "title": "Estructuras de Mercado con Python",
    "section": "",
    "text": "Este post tiene como objetivo dar a conocer la importancia del software de Python en el ambito microeconomico, particularmente en este caso hablamos de las diferentes estructuras de mercado; competencia perfecta, monopolio y oligopolio."
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "href": "posts/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "title": "Estructuras de Mercado con Python",
    "section": "Condiciones necesaria para la competencia perfecta",
    "text": "Condiciones necesaria para la competencia perfecta\n\nMuchos productores, ninguno de los cuales tiene una gran cuota de mercado.\nUna industria puede ser perfectamente competitiva sólo si los consumidores consideran como equivalentes a los productos de todos los productores (producto homogéneo)"
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#libre-entrada-y-salida",
    "href": "posts/estructuras_mercado/index.html#libre-entrada-y-salida",
    "title": "Estructuras de Mercado con Python",
    "section": "Libre entrada y salida",
    "text": "Libre entrada y salida\nExiste libre entrada y salida en una industria cuando nuevos productores pueden entrar facilmente en esa industria a los que ya estan en ella pueden abondonarla sin coste alguno.\n\nRegla de Producción Optima\nLa regla de producción optima dice que el beneficio se maximiza cuando se produce la cantidad de output para la cual el ingreso marginal de la última unidad de output producida es igual a su coste marginal.\n\\[\nIMg = CMg\n\\]\n\n\nFunción de Benenficios\nLa función de beneficios \\((\\pi)\\) representa las diferencias entre los costos totales, \\(C(Q)\\) e ingresos totales,\\(R(Q)\\) , de las empresas\n\\[\n\\pi = R(Q) - C(Q)\n\\]\n\n\nTomador de Precios\nPrecio igual al costo marginal\n\\[\n\\begin{eqnarray*}\nCMg = IMg = P\n\\end{eqnarray*}\n\\]\nPor tanto, se dice que el beneficio de una empresa precio-aceptante se maximiza produciendo la cantidad de output para la cual el costo marginal de la última unidad producida es igual al precio de mercado, tal como se aprecia en el siguiente gráfico\n\n\n\nCantidad de producto que maximiza el beneficio de una empresa precio-aceptante"
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "href": "posts/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Costes y Producción en el Corto Plazo",
    "text": "Costes y Producción en el Corto Plazo\nEn el corto plazo tenemos las siguientes condiciones de producción de empresas competitivas\n\n\n\n\n\n\n\nCondiciones\nResultados\n\n\n\n\nP > CVMe mínimo\nLa empresa produce en el corto plazo. Si P < CTMe mínimo, la empresa cubre sus costos variables y parte de sus costes fijos pero no todos. Si P > CTMe mínimo, la empresa cubre todos sus costes variables y sus costes fijos.\n\n\nP = CVMe mínimo\nLa empresa es indiferente entre producir en el corto plazo o no producir. Cubre exactamente sus costes variables.\n\n\nP < CVMe mínimo\nLa empresa cierra en el corto plazo. No cubre sus costes variables."
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "href": "posts/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Ejemplo # 1- Corto Plazo",
    "text": "Ejemplo # 1- Corto Plazo\nPrimero resolveremos el siguiente ejercicio de manera manual y posteriormente lo resolveremos en Python.\nSuponga que la empresa tiene una curva de costos de corto plazo dada por\n\\[\nC(Q) = 100 + 20Q + Q^2\n\\]\n\n¿Cuál es la ecuación para el costo variable Medio?\n¿Cuál es el valor mínimo para el costo variable promedio?\n¿Cuál es la curva de oferta de corto plazo?\n\nSolución\n\nDada la función de costo \\(C(Q) = 100 + 20Q + Q^2\\) es claro que el costo variable, CV, esta dado por \\[CV = 20Q + Q^2\\] por tanto su costo variable promedio es \\[CVMe = \\frac{CV}{Q} = 20 + Q\\]\nAhora bien, su costo marginal sabemos que unicamente requiere aplicar la regla de diferenciación, ya que \\[CMg = \\frac{\\partial C(Q)}{\\partial Q} = 20 + 2Q\\]\nSi queremos encontrar el costo variable promedio mínimo, \\[CVMe_{\\min}\\], se obtiene como \\[CMg = CVMe \\longrightarrow Q = \\fbox{0}\\]\nEntonces la función de oferta es: \\[\\begin{eqnarray*}CMg &=& p\\\\[0.2cm] 20 + 2Q &=& P\\\\[0.2cm] Q(P) &=& \\frac{P}{2} - 10 \\end{eqnarray*}\\]\n\nPor tanto, también podemos obtener el precio de equilibrio, ya que \\[0 = \\frac{P}{2} - 10 \\longrightarrow P = \\fbox{20}\\]\nAhora, encontremos estos resultados en Python:\n\n# Paquete previo \nfrom sympy import *\nQ = symbols(\"Q\")\n\n\n# función de costo de corto plazo \nCT = 100 + 20*Q + Q**2\n# costo variale promedio \nCV = 20 + Q \n# Encontrar el costo variable minimo \n# Primero: costo marginal\n\nCM = diff(CT,Q)\n\n\n# igualar costo marginal y costo variable promedio \nsolve(Eq(CM,CV))\n\n[0]\n\n\n\ncantidad = solve(Eq(CM,CV))\ncantidad[0]\n\n\\(\\displaystyle 0\\)\n\n\n\nP = CV.subs({Q:cantidad[0]})\nP\n\n\\(\\displaystyle 20\\)\n\n\n\nplot(CT, CT/Q, CV, CM, (Q,0,100), xlim = (0, 100), ylim = (0,100), xlabel = \"Q\", ylabel = \"P\")\n\n\n\n\n<sympy.plotting.plot.Plot at 0x1997ad7ef40>\n\n\nPuedes notar lo rápido y fácil que resulta realizar estos procedimientos con Python y la utilidad que puede brindarte en caso de que trabajes con volumnes de datos.\n\nEjemplo # 2 - Corto Plazo\nAhora suponga que la empresa tiene una curva costos en el corto plazo de la siguiente forma:\n\\[\nC(Q) = 1 + 10Q + Q^2\n\\]\nSi la empresa opera en un mercado perfectamente competitivo, donde \\(P = 12\\), ¿Cuál será los beneficios de la empresa en el corto plazo?\nSolución\nSabemos que la función de beneficios esta dada por\n\\[\n\\pi = R - C\n\\]\nentonces,\n\\[\n\\frac{\\partial \\pi}{\\partial Q} = IMg - CMg = 0\n\\]\nasí pues,\n\\[\nCMg = 10 + 2Q \\hspace{1cm}y\\hspace{1cm} IMg = P\n\\]\npor tanto,\n\\[\n\\begin{eqnarray*}\nCMg &=& IMg\\\\[0.2cm]\n10 + 2Q &=& P\\\\[0.2cm]\nQ &=& \\frac{P}{2} - 5\\\\[0.2cm]\nQ &=& \\frac{12}{2} - 5, \\hspace{2cm}\\mbox{ya que P = 12}\\\\[0.2cm]\nQ &=& \\fbox{1}\n\\end{eqnarray*}\n\\]\nentonces,\n\\[\n\\pi = 12 - (1 + 10 +1) = \\fbox{0}\n\\]\nAhora veamos esta solución en Python:\n\n# Función de costos a corto plazo \nQ = symbols(\"Q\")\nCT = Q**2 + 10*Q + 1\nP = 12\nR = P*Q\n# costo marginal\nCM = diff(CT,Q)\nCM\nIM = diff(R,Q)\nIM\ncantidad = solve(Eq(IM,CM))\nprint(\"El valor de la producción que garantiza un equilibrio será:\", cantidad[0])\n\nEl valor de la producción que garantiza un equilibrio será: 1\n\n\nEste resultado lo que nos dice es que la empresa oferta una unidad de producción \\(Q = 1\\).\n\n# Beneficio = IT - CT\ncosto = CT.subs({Q:cantidad[0]})\ncosto\n\n\\(\\displaystyle 12\\)\n\n\n\ningreso = R.subs({Q:cantidad[0]})\ningreso \n\n\\(\\displaystyle 12\\)\n\n\n\nBeneficios = R - CT\npi = Beneficios.subs({Q:cantidad[0]})\npi\n\n\\(\\displaystyle 0\\)\n\n\n\nplot(CT,CM,CT/Q,(Q,0,60), xlim=(0,5), ylim=(0,30), xlabel='Q', ylabel='CT,CM')\n\n\n\n\n<sympy.plotting.plot.Plot at 0x1997b621bb0>\n\n\nRecuerde que todo este análisis se realizo para un mercado en competencia perfecta a corto plazo.\nPronto actualizare para el mercado en competencia perfecta a largo plazo, monopolio, e introducirnos un poco a la teoria de juegos."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Redes Neuronales Recurrentes (LSTM)\n\n\nPronósticos de índice AAPL\n\n\n\n\nRNN\n\n\nLSTM\n\n\nTensorflow\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nEstructuras de Mercado con Python\n\n\nCompetencia Perfecta, Monopolio y Oligopolio\n\n\n\n\nCMg\n\n\nCVP\n\n\nCTP\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nPortfolio management\n\n\nGeneralidades\n\n\n\n\nDiversificación\n\n\nPortafolio\n\n\nVolatilidad\n\n\nFrontera eficiente\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning en Tidyverse\n\n\nMúltiples Modulos con broom\n\n\n\n\nMachine Learning\n\n\nForecasting\n\n\nR\n\n\nRStudio\n\n\nbroom\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nMicroeconomía Intermedia con R\n\n\nUtilidad y sus Curvas de Indiferencia\n\n\n\n\nEconomía\n\n\nMicroeconomía\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nModelo Prophet de Facebook\n\n\nPronóstico aplicado al tipo de cambio USD/HNL\n\n\n\n\nProphet\n\n\nForecasting\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2022\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nIntroducción a R\n\n\nGeneralidades\n\n\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2022\n\n\nJuan Isaula\n\n\n\n\n\n\n  \n\n\n\n\nSeries de Tiempo\n\n\n\n\n\n\n\nTime Series\n\n\nRStudio\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\nJuan Isaula\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/port_mg/index.html",
    "href": "posts/port_mg/index.html",
    "title": "Portfolio management",
    "section": "",
    "text": "La teoría de la fijación de precios de derivados es una teoría de rendimientos deterministas: cubrimos nuestros derivados con el subyacente para eliminar el riesgo, y nuestra cartera libre de riesgo resultante gana la tasa de interés libre de riesgo. Los bancos ganan dinero con este proceso de cobertura; venden algo por un poco más de lo que vale y cubrir el riesgo para obtener una ganancia garantizada. Los gestores de fondos compran y venden activos (incluidos los derivados) con el objetivo de superar la tasa de rendimiento del banco. Al hacerlo, se arriesgan. En este artículo explico algunas de la teorías detrás del riesgo y la recompensa de la inversión y, como optimizar una cartera para obtener el mejor valor por dinero."
  },
  {
    "objectID": "posts/port_mg/index.html#diversificación",
    "href": "posts/port_mg/index.html#diversificación",
    "title": "Portfolio management",
    "section": "Diversificación",
    "text": "Diversificación\nIntroduciremos algo de notación y muestro el efecto de la diversificación sobre la rentabilidad de la cartera. Supongamos que tenemos una cartera de \\(N\\) activos. El valor hoy del i-ésimo activo es \\(S_i\\) y su rendimiento aleatorio es \\(R_i\\) sobre nuestro horizonte de tiempo \\(T\\). Las \\(R_i \\sim N(\\mu_iT, \\sigma_i\\sqrt{T})\\). La correlación entre los rendimientos de la i-ésima y j-ésimo activo es \\(\\rho_{ij}\\) (con \\(\\rho_{ii} = 1\\)).\nLos parámetros \\(\\mu, \\sigma\\) y \\(\\rho\\) corresponden a la media, volatilidad y correlación a la que estamos acostumbrados. Tenga en cuenta la escala con el horizonte de tiempo.\nSi tenemos \\(w_i\\) del i-ésimo activo, entonces nuestra cartera tiene valor\n\\[\n\\Pi = \\sum_{i=1}^{N} w_iS_i\n\\]\nAl final de nuestro horizonte temporal, el valor es\n\\[\\Pi + \\delta\\Pi = \\sum_{i=1}^{N} w_iS_i(1+R_i)\\]\nPodemos escribir el cambio relativo en el valor de la cartera como\n\\[\n\\frac{\\delta\\Pi}{\\Pi} = \\sum_{i=1}^N W_iR_i\\hspace{1.5cm} (1)\n\\]\ndonde\n\\[\nW_i = \\frac{w_iS_i}{\\sum_{i=1}^N w_iS_i}\n\\]\nLos pesos \\(W_i\\) suman uno.\nA partir de (1) es sencillo calcular el rendimiento esperado de la cartera\n\\[\n\\mu_{\\Pi} = \\frac{1}{T}E\\left[\\begin{array}{c}\\frac{\\delta\\Pi}{\\Pi}\\end{array}\\right] = \\sum_{i=1}^{N}W_i \\mu_i\\hspace{6cm} (2)\n\\]\nY la desviación estándar de los retornos son\n\\[\n\\sigma_{\\Pi} = \\frac{1}{\\sqrt{T}}\\sqrt{var\\left[\\begin{array}{0} \\frac{\\delta \\Pi}{\\Pi}\\end{array}\\right]} = \\sqrt{\\sum_{i=1}^{N}\\sum_{j=1}^N W_iW_j \\rho_{ij}\\sigma_i\\sigma_j}\\hspace{1.5cm} (3)\n\\]\nEn ellos hemos relacionado los parámetros de los activos individuales con la rentabilidad esperada y la desviación estándar de toda la cartera.\nSupongamos que tenemos activos en nuestra cartera que no están correlacionados, es decir, \\(\\rho_{ij} = 0\\), \\(i = j\\). Para simplificar las cosas, suponga que tienen el mismo peso, de modo que \\(W_i = \\frac{1}{N}\\). El rendimiento esperado de la cartera está representado por\n\\[\n\\mu_{\\Pi} = \\frac{1}{N}\\sum_{i=1}^N \\mu_i\n\\]\nEl promedio de los rendimientos esperados de todos los activos, y la volatilidad se convierte en\n\\[\n\\sigma_{\\Pi} = \\sqrt{\\frac{1}{N^2}\\sum_{i=1}^N \\sigma_i^2}\n\\]\nEsta volatilidad es \\(O(N^{-1/2})\\) ya que hay \\(N\\) términos en la suma. A medida que aumentamos el número de activos en cartera, la desviación estándar de los rendimientos tiende a cero.\nSupongamos que todos los activos no están correlacionados, pero veremos algo similar cuando describa el Modelo de fijación de precios de activos de capital; la diversificación reduce la volatilidad sin perjudicar las expectativas de rendimientos.\nAhora me voy a referir a la volatilidad o desviación estándar como riesgo, algo malo que debe evitarse (dentro de lo razonable), y el rendimiento esperado como recompensa, algo bueno que queremos tanto como sea posible."
  },
  {
    "objectID": "posts/port_mg/index.html#teoría-moderna-del-portafolio",
    "href": "posts/port_mg/index.html#teoría-moderna-del-portafolio",
    "title": "Portfolio management",
    "section": "Teoría Moderna del Portafolio",
    "text": "Teoría Moderna del Portafolio\nPödemos usar el marco anterior para discutir la “mejor” cartera. La definición de “mejor” fue abordada con mucho éxito por el Premio Nobel Harry Markowitz. Su modelo proporciona una manera de definir carteras que sean eficientes.\nUna cartera eficiente es aquella que tiene la recompensa más alta para un nivel de riesgo, o el riesgo más bajo para una recompensa dada. Para ver cómo funciona esto imagina que hay cuatro activos en el mundo \\(A, B, C\\) y \\(D\\) con recompensa y riesgo como se muestra en la figura 1 (ignore E por el momento). Si pudieras comprar alguno de estos (pero de momentos no te permiten más de uno), ¿cuál comprarías? eliges D? No, porque tiene el mismo riesgo que B pero menos recompensa, tiene la misma recompensa que como C pero pun mayor riesgo. Entonces, podemos descartar \\(D\\). ¿Qué pasa con B o C? Ambos son atractivos cuando se comparan con D, pero entre si no estan claro, B tiene un mayor riesgo, pero obtiene una mayor recompensa. Sin embargo, comparándolos a ambos con A vemos que no hay competencia, ya que A es la elección preferida. Si introducimos el activo E con el mismo riesgo que B y una recompensa mayor que A, entonces no podemos decir objetivamente cuál de A y E es mejor; esta es una elección subjetiva y depende de las preferencias de riesgo de un inversor.\n\n\n\nFigura 1: Riesgo y recompensa de cinco activos\n\n\nAhora suponga que tengo los dos activos A y E de la figura 2, y puedo combinar en mi cartera, ¿qué efecto tiene esto en mi riesgo/recompensa?\n\n\n\nFigura 2: Dos activos y cualquier combinación\n\n\nDe (2) y (3) tenemos\n\\[\\mu_{\\Pi} = W\\mu_{A} + (1-W)\\mu_{E}\\]\ny\n\\[\n\\sigma_{\\Pi}^2 = W^2\\sigma_{A}^2 + 2W(1-W)\\rho\\sigma_{A}\\sigma_{E} + (1-W)^2\\sigma_{E}^2\n\\]\nAquí \\(w\\) es el peso del activo A y, recordando que los pesos deben sumar uno, el peso del activo E es \\(1 - E\\).\nA medida que variamos W, también cambian el riesgo y la recompensa. La linea en el espacio de riesgo/recompensa que es parametrizada por W es una hipérbola, como se muestra en la figura 2. La parte de esta curva en negrita es eficiente, y es preferible al resto de la curva. Una ves más, las preferencias de riesgo de un individuo dirá dónde quiere estar en la curva audaz. Cuando una de las volatilidades es cero la línea se vuelvve recta. en cualquier lugar de la curva entre los dos puntos se requiere una posición larga en cada activo. Fuera de esta región, uno de los activos se vende al descubierto para financiar la compra del otro. Todo lo que sigue asume que podemos vender al descubierto tanto activo como queramos. Los resultados cambian ligeramente cuando hay restricciones.\nSi tenemos muchos activos en nuestra cartera, ya no tenemos una simple hipérbola para nuestros posibles perfiles de riesgo/recompensa; en cambio obtenemos algo como lo que se muestra en la Figura 3.\n\n\n\nFigura 3: Posibilidades de cartera y la frontera eficiente\n\n\nEsta figura ahora usa todo A, B, C, D y E, no solo A y E. Aunque B, C y D no son individualmente atractivos, bien pueden ser útiles en un portafolio, dependiendo de como se correlacionen, o no, con otras inversiones. En esta figura podemos ver la frontera eficiente marcada en negrita. Dado cualquier elección de cartera elegiríamos tener una que se encuentre en esta frontera eficiente."
  },
  {
    "objectID": "posts/port_mg/index.html#incluir-una-inversión-sin-riesgo",
    "href": "posts/port_mg/index.html#incluir-una-inversión-sin-riesgo",
    "title": "Portfolio management",
    "section": "Incluir una inversión sin riesgo",
    "text": "Incluir una inversión sin riesgo\nUna inversión sin riesgo que gana una tasa de rendimiento garantizada \\(r\\) sería el punto F en la Figura 3. Si se nos permite mantener este activo en nuestra cartera, dado que la volatilidad de este activo es cero, obtenemos la nueva frontera eficiente que es la línea recta en la Figura 3. El portafolio para el que la línea recta toca la frontera eficiente original se denomina cartera de mercado. La linea recta en sí misma se llama la línea del mercado de capitales.\n\nDonde quiero estar en la frontera eficiente?\nHabiendo encontrado la frontera eficiente, queremos aber dónde debemos estar. Esta es una elección personal, la frontera eficiente es objetiva, dados los datos, pero la “mejor” posición en ella es subjetiva.\nLa siguiente es una forma de interpretar el diagrama de riesgo/recompensa que puede ser útil en la elección de la mejor cartera.\nEl rendimiento de la cartera se distribuye normalmente porque está compuesto por activos que se distribuyen normalmente. Tiene media \\(\\mu\\) y desviación estándar \\(\\sigma\\) (he ignorado la dependencia del horizonte T). La pendiente de la línea que une la cartera con el activo libre de riesgo es\n\\[\ns = \\frac{\\mu_{\\Pi} - r}{\\sigma_{\\Pi}}\n\\]\nEsta es una cantidad importante; es una medida de la probabilidad de tenter un rendimiento que exceda r. Si \\(C(.)\\) es la función acumulada para la distribución normal estandarizada, entonces \\(C(s)\\) es la probabilidad de que el rendimiento en \\(\\Pi\\) sea al menos q \\(r\\). Mas generalmente\n\\[\nC\\left(\\begin{array}{0}\\frac{\\mu_{\\Pi} - r^*}{\\sigma_{\\Pi}}\\end{array}\\right)\n\\]\nes la probabilidad de que el rendimiento exceda \\(r^*\\). Esto sugiere que si queremos minimizar la posibilidad de una rentabilidad inferior a \\(r^*\\) debemos elegir la cartera del conjunto de fronteras eficientes, \\(\\Pi_{eff}\\) con el mayor valor de la pendiente\n\\[\n\\frac{\\mu_{\\Pi_{eff}} - r^*}{\\sigma_{\\Pi_{eff}}}\n\\]\nPor el contrario, si mantenemos la pendiente de esta línea fija en \\(s\\), entonces podemos decir que con una confianza de \\(C(s)\\) no perderemos más que\n\\[\n\\mu_{\\Pi_{eff}} - s\\sigma_{\\Pi_{eff}}\n\\]\nNuestra elección de cartera podría determinarse maximizando esta cantidad. Estas dos estrategías se muestran esquemáticamente en la Figura 5.\n\n\n\nFigura 5: Dos sencillas formas de elegir la mejor cartera eficiente\n\n\nNinguno de estos métodos da resultados satisfactorios cuando existe inversión libre de riesgo entre los activos y hay ventas cortas sin restricciones, ya que dan como resultado un endeudamiento infinito.\nOtra forma de elegir la cartera óptima es con la ayuda de una función de utilidad. Este enfoque es popular entre los economistas. En la Figura 6 muestro las curvas de indiferencia y la frontera eficiente.\n\n\n\nFigura 6: Frontera eficiente y las curvas de indiferencia\n\n\nLas curvas reciben este nombre porque representan lineas a las cual el inversionista es indiferente al trade-off riesgo/recompensa. Un inversionista quiere un alto rendimiento y riesgo bajo. Frente a las carteras A y B en la Figura, ve a A con bajo rendimiento y bajo riesgo, pero B tiene una mejor recompensa a costa de un mayor riesgo. El inversor es indiferente entre estos dos. Sin embargo, C es mejor que ambos, estando en una curva preferida."
  },
  {
    "objectID": "posts/port_mg/index.html#markowitz-en-la-práctica",
    "href": "posts/port_mg/index.html#markowitz-en-la-práctica",
    "title": "Portfolio management",
    "section": "Markowitz en la Práctica",
    "text": "Markowitz en la Práctica\nLas entradas al modelo de Markowitz son rendimientos esperados, volatilidades y correlaciones. Con \\(N\\) activos esto significa \\(N + N + N(N-1)/2\\) parámetros. La mayoría de estos no se pueden conocer con precisión (¿existen siquiera?); sólo las volatilidades son en absoluto confiable. Habiendo ingresado estos parámetros, debemos optimizar sobre todos los pesos de los activvos en la cartera: Elija un riesgo de cartera y encuentre los pesos que haqcen que el rendimiento de la cartera sea máximo sujeto a esta volatilidad. Este es un proceso que consume mucho tiempo computacionalmente a menos que uno solo tenga una pequeña cantidad de activos.\nEl problema con la implementación práctica de este modelo lo realizaré enm otro post que publicare posterioremente, usando Python. Por los momentos es importante comprender la lógica del modelo."
  },
  {
    "objectID": "posts/port_mg/index.html#modelo-de-precios-de-activos-de-capital-capital-asset-pricing-model",
    "href": "posts/port_mg/index.html#modelo-de-precios-de-activos-de-capital-capital-asset-pricing-model",
    "title": "Portfolio management",
    "section": "Modelo de Precios de Activos de Capital (Capital Asset Pricing Model)",
    "text": "Modelo de Precios de Activos de Capital (Capital Asset Pricing Model)\nAntes de discutir el modelo de fijación de precios de activos de capital o CAPM debemos introducir la idea del valor \\(\\beta\\). El parámetro \\(\\beta_i\\), de un activo en relación con una cartera \\(M\\) es la relación de la covarianza entre el rendimiento del valor y el rendimiento de la cartera a la varianza de la cartera. Del siguiente modo\n\\[\n\\beta_i = \\frac{Cov[R_iR_M]}{Var[R_M]}\n\\]\n\nEl Modelo de Indice Unico\nAhora construire un modelo de índices único y describiré las extenciones más adelante. Voy a relacionar el rendimiento de todos los activos al rendimiento de un índice representativo, \\(M\\). Este índice suele ser tomado como un índice bursátil de amplio rango en el modelo de índice único. Nosotros escribimos el rendimiento del i-ésimo activo como\n\\[\nR_i = \\alpha_i + \\beta_iR_M + \\epsilon_i\n\\]\nUsando esta representación podemos ver que el rendimiento de un activo se puede descomponer en tres partes:\n\nUna media constante\nUna parte aleatoria común con el índice \\(M\\) y,\nuna parte aleatoria no correlacionada con el índice, \\(\\epsilon_i\\).\n\nLa parte aleatoria \\(\\epsilon_i\\) es única para el i-ésimo activo, y tiene media cero. Observe cómo todos los activos están relacionados con el índice \\(M\\) pero son de contrario completamente sin correlación.\nEn la Figura 7 se muestra un gráfico de rendimiento de las acciones de Walt Disney frente a los rendimientos del S&P 500; \\(\\alpha\\) y \\(\\beta\\) se pueden determinar a partir de un análisis de regresión lineal. Los datos utilizados en este gráfico abarcaron desde enero de 1985 hasta casi finales de 1997.\n\n\n\nFigura 7: Rentabilidad de las acciones de Walt Disney frente a la rentabilidad del S&P 500.\n\n\nEl rendimiento esperado del índice se denotará por \\(\\mu_M\\) y su desviación estándar por \\(\\sigma_M\\). El rendimiento esperado del i-ésimo activo es entonces:\n\\[\n\\mu_i = \\alpha_i + \\beta_i\\mu_M\n\\]\ny la desviación estandar\n\\[\n\\sigma_i = \\sqrt{\\beta_i^2\\sigma_M^2 + e_i^2}\n\\]\ndonde \\(e_i\\) es la desviación estándar de \\(\\epsilon_i\\).\nSi tenemos una cartera de dichos activos, el rendimiento viene dado por\n\\[\n\\begin{eqnarray}\n\\frac{\\delta \\Pi}{\\Pi} = \\sum_{i=1}^N W_iR_i = \\left(\\begin{array}{0}\\sum_{i=1}^N W_i\\alpha_i\\end{array}\\right) + R_M\\left(\\begin{array}{0} \\sum_{i=1}^N W_i\\beta_i\\end{array}\\right) + \\sum_{i=1}^N W_i\\epsilon_i\n\\end{eqnarray}\n\\]\nDe esto se sigue que\n\\[\n\\mu_\\Pi = \\left(\\begin{array}{0}\\sum_{i=1}^N W_i\\alpha_i\\end{array}\\right) + E[R_M]\\left(\\begin{array}{0}\\sum_{i=1}^N W_i\\beta_i\\end{array}\\right)\n\\]\nPodemos escribir\n\\[\n\\alpha_\\Pi = \\sum_{i=1}^NW_i\\alpha_i \\hspace{1cm}y \\hspace{1cm} \\beta_\\Pi = \\sum_{i=1}^NW_i\\beta_i\n\\]\nEntonces,\n\\[\n\\mu_\\Pi = \\alpha_\\Pi + \\beta_\\Pi\nE[R_M] = \\alpha_\\Pi + \\beta_\\Pi \\mu_M\\]\nDe manera similar, el riesgo se mide por\n\\[\n\\sigma_\\Pi = \\sqrt{\\sum_{i=1}^N\\sum_{j=1}^N W_iW_j\\beta_i\\beta_j\\sigma_M^2 + \\sum_{i=1}^N W_i^2 e_i^2}\n\\]\nSi los pesos son casi iguales, \\(N^{-1}\\), entonces los términos finales dentro de la raíz cuadrada también son \\(O(N^{-1})\\). Por lo tanto, esta expresión es, al orden principal como \\(N \\longrightarrow \\infty\\),\n\\[\n\\sigma_\\Pi = \\left|\\begin{array}{0}\\sum_{i=1}^N W_i\\beta_i\\end{array}\\right|\\sigma_M = \\left|\\begin{array}{0}\\beta_\\Pi\\end{array}\\right|\\sigma_M\n\\]\nObserve que la contribución de los no correlacionados a la cartera se desvanece a medida que aumentamos le número de activos en la cartera: El riesgo asociado con el \\(\\epsilon_s\\) se llama riesgo diversificable. El riesgo restante, que esta correlacionado con el índice, se denomina riesgo sistematico.\n\n\nElegir la Cartera Optima\nEl principal es el mismo que el modelo de Markowitz para la elección óptima de cartera. La diferencia es que hay muchos menos parámetros para ingresar, y el cálculo es mucho más rapido.\nEl procedimiento es el siguiente. Elija un valor para el rendimiento de la cartera \\(\\mu_\\Pi\\). Sujeto a esta restricción, minimizar \\(\\sigma_\\Pi\\). Repita esta minimización para diferentes rendimientos de carteera para obtener la frontera eficiente. La posición es esta curva es entonces una elección subjetiva."
  },
  {
    "objectID": "posts/port_mg/index.html#medición-del-desempeño",
    "href": "posts/port_mg/index.html#medición-del-desempeño",
    "title": "Portfolio management",
    "section": "Medición del Desempeño",
    "text": "Medición del Desempeño\nSi uno ha seguido una de las estratetias de asignación de activos, o simplemente ha negociado en instinto, ¿puede uno decir qué tan bien lo ha hecho? ¿Fueron los resultados sobresalientes deido a un extraño instinto natural, o los horribles resultados fueron simplemente mala suerte?\nEl rendimiento ideal sería uno en el que los rendimientos superaran la tasa libre de riesgo, pero en una moda consistente. No solo es importante obtener un alto rendimiento de la gestión de la cartera, pero uno debe lograr esto con la menor aleatoriedad posible.\nLas dos medidas más comunes de rendimiento por unidad de riesgo son\n\nRelación de Sharpe de recompensa a variabilidad y el\níndice de Treynor de recompensa a la volatilidad.\n\nEstos se definen como\n\\[\n\\begin{eqnarray}\nratio Sharpe &=& \\frac{\\mu_\\Pi - r}{\\sigma_\\Pi}\\\\[0.2cm]\nratio Treynor &=& \\frac{\\mu_\\Pi - r}{\\beta_\\Pi}\n\\end{eqnarray}\n\\]\nEn estos \\(\\mu\\) y \\(\\sigma\\) son el rendimiento realizado y la desviación estándar de la cartera durante el período. El \\(\\beta\\) es una medida de la volatilidad de la cartera. El ratio de Sharpe generalmente se usa cuando la cartera es la totalidad de la inversión de uno y el ratio de Treynor cuando se examina el rendimiento de un componente de la cartera de toda la empresa, digamos.\nCuando la cartera baja las dos medidas son las mismas (hasta un factor del mercado Desviación Estándar)\n\n\n\nFigura 8: un buen y un gerente (manager); mismos rendimientos, distintas volatilidades\n\n\nLa Figura 8 muestra el valor de la cartera frente al tiempo para un buen administrador y un mal administrador."
  },
  {
    "objectID": "posts/port_mg/index.html#resumen",
    "href": "posts/port_mg/index.html#resumen",
    "title": "Portfolio management",
    "section": "Resumen",
    "text": "Resumen\n\nLa gestión de carteras y la asignación de activos consisten en asumir riesgos a cambio de una recompensa.\nLas preguntas son, ¿como decidir cuánto riesgo tomar? ¿cómo obtener el mejor rendimiento? Pero la teoría de los derivados se basa en no correr ningún riesgo en absoluto, por lo que he dedicado tiempo a gestión de cartera en este post.\nExiste tanta incertidumbre en el tema de las finanzas que la eliminación del riesgo es casi imposible y las ideas detrás de la gestión de carteras deben ser apreciadas por cualquier persona involucrada en derivados."
  },
  {
    "objectID": "posts/port_mg/index.html#definición",
    "href": "posts/port_mg/index.html#definición",
    "title": "Portfolio management",
    "section": "Definición",
    "text": "Definición\nEl valor en riesgo es una estimación, con un cierto grado de confianza, de cuánto se puede perder de la cartera, en un horizonte de tiempo dado.\n\nEl grado de confianza normalmente se establece en 95%, 87.5%, 99%, etc. El horizonte temporal de interés puede ser de un día, por ejemplo, para actividades comerciales o de meses para gestión de cartera.\nComo ejemplo de VaR, podemos calcular (mediane los métodos que e describirán aquí) que durante la próxima semana hay un 95% de probabilidad de que no perdamos más de $10 millones. Podemos escribir esto como\n\\[\nProb\\{\\delta V \\leq -\\$10 m\\} = 0.05\n\\]\ndonde \\(\\delta V\\) es el cambio en el valor de la cartera. (uso \\(\\delta\\) para “el cambio en” para enfatizar que estamos considerando cambios en un tiempo finito.) En símbolos, esto es\n\\[\nProb\\{\\delta V \\leq -VaR\\}= 1- c\n\\]\ndonde el grado de confianza es \\(c\\).\nEl VaR se calcula asumiendo circustancias de mercado normales, lo que significa que el mercado extremo no se consideran condiciones como choques, o se examinan por separado. Así, efectivamente, el VaR mide lo que se puede esperar que suceda durante la operación diaria de una institución.\nPara el computo del VaR requerimos disponer al menos de los siguientes datos:\n\nLos precios vigentes de todos los activos en cartera y,\nsus volatilidadesa y correlaciones entre ellos\n\nSi los bienes son negociados podemos tomar los precios del mercado (marking to market).\nPor lo general, se supone que el movimiento de los componentes de la cartera son aleatorias y extraídas de distribuciones normales."
  },
  {
    "objectID": "posts/port_mg/index.html#var-para-un-único-activo",
    "href": "posts/port_mg/index.html#var-para-un-único-activo",
    "title": "Portfolio management",
    "section": "VaR para un único activo",
    "text": "VaR para un único activo\nEmpecemos por estimar el VaR de una cartera compuesta por un único activo.\nSupongamos que tenemos una cantidad de una acción con precio \\(S\\) y volatilidad \\(\\sigma\\). Queremos saber con el 99% de confianza cuál es el máximo que podemos perder durante la próxima semana, estoy usando notación deliberadamente similar al del mundo de los derivados\n\\[\n\\sigma S\\left(\\begin{array}{0}\\frac{1}{52}\\end{array}\\right)^{1/2}\n\\]\nya que el paso de tiempo es \\(1/52\\) de un año. Finalmente, debemos calcular la posición de la cola extrema izquierda de esta distribución correspondiente a\n\\[\n1\\% = 100 - 99\\%\n\\]\nSolo necesitamos hacer esto para la distribución normal estandarizada, porque podemos llegar a cualquier otra distribución escalando. En la siguiente tabla, vemos que el intervalo de confianza del 99% corresponde a 2.33 desviaciones estándar de la media.\n\nGrado de confianza y la relación con la desviación de la media.\n\n\nGrado de Confianza (%)\nNúmero de desviaciones estándar de la media\n\n\n\n\n99\n2.326342\n\n\n98\n2.053748\n\n\n97\n1.88079\n\n\n96\n1.750686\n\n\n95\n1.644853\n\n\n90\n1.281551\n\n\n\n\n\n\n\nDado que tenemos una cantidad de acciones, el VaR es dado por\n\\[\n2.33\\sigma\\triangle S(1/52)^{1/2}\n\\]\nDe manera general, si el horizonte de tiempo es \\(\\delta t\\) y el grado de confianza es \\(c\\), tenemos\n\\[\nVaR = -\\sigma \\triangle S(\\delta t)^{1/2}\\alpha(1-c)\n\\]\ndonde \\(\\alpha(.)\\) es la función de distribución acumulativa inverdsa para la distribución Normal estandarizada, que se muestra en la Figura 9,\n\n\n\nFigura 9: Función de distribución acumulativa inversa para la distribución Normal Estandarizada.\n\n\nEn la Figura 10 hemos supuesto que el rendimiento del activo se distribuye normalmente con una media cero. La suposición de media cero es válida para horizontes temporales cortos: La desviación estándar del rendimiento escala con la raíz cuadrada del tiempo pero la media escala con el tiempo mismo.\n\n\n\nFigura 10: La distribución de los rendimientos futuros de las acciones.\n\n\nPara horizontes a más largo plazo, el rendimiento se desplaza hacia la derecha (es de esperar) en una cantidad proporcional al horizonte de tiempo. Por lo tanto, para escalas de tiempo más largas, la ecuación anterior debe modificarse para tener en cuenta el derivado del valor del activo. Si la tasa de este derivado es \\(\\mu\\) entonces la ecuación anterior se convierte en\n\\[\nVaR = \\triangle S(\\mu \\delta t - \\sigma \\delta t^{1/2} \\alpha(1-c))\n\\]"
  },
  {
    "objectID": "posts/port_mg/index.html#var-para-un-portafolio-o-cartera",
    "href": "posts/port_mg/index.html#var-para-un-portafolio-o-cartera",
    "title": "Portfolio management",
    "section": "VaR para un Portafolio o Cartera",
    "text": "VaR para un Portafolio o Cartera\nSi conocemos las volatilidades de todos los activos de nuestra cartera y las correlaciones entre ellos entonces podemos calcular el VaR para toda la cartera.\nSi la volatilidad del i-ésimo activo es \\(\\sigma_i\\) y la correlación entre el i-ésimo y el j-ésimo activo es \\(\\rho_{ij}\\) (con \\(\\rho_{ii} = 1\\)), entonces el VaR para la cartera compuesta por M activos con una participación de i del i-ésimo activo es\n\\[\n-\\alpha(1-c)\\delta t^{1/2}\\sqrt{\\sum_{j=1}^M\\sum_{i=1}^M \\triangle_i\\triangle_j\\sigma_i\\sigma_j\\rho_{ij}S_iS_j}\n\\]"
  },
  {
    "objectID": "posts/port_mg/index.html#uso-del-var-como-medida-de-rendimiento",
    "href": "posts/port_mg/index.html#uso-del-var-como-medida-de-rendimiento",
    "title": "Portfolio management",
    "section": "Uso del VaR como medida de rendimiento",
    "text": "Uso del VaR como medida de rendimiento\nUno de los usos del VaR en la medición del desempeño de bancos, mesas o comerciantes. En el pasado, el “talento comercial” se ha medido únicamente en términos de ganancias; la bonificación de un comerciante esta relacionado con esa ganancia. Esto anima a los comerciante a asumir riesgos; piensa en lanzar una moneda al aire y recibes un porcentaje de la ganancia pero sin la desventaja (que se lleva el banco), ¿cuánto apostarías? Una mejor medida del talento comercial podría tener en cuenta el riesgo en tal apuesta, y premiar una buena relación rendimiento-roesgo. El ratio\n\\[\n\\frac{\\mbox{Retorno superior al libre de riesgo}}{volatilidad} = \\frac{\\mu - r}{\\sigma}\n\\]\nLa relación de Sharpe, es una medida de este tipo. Alternativamente, use VaR como la medida de riesgo y ganancia/pérdida como medida de rendimiento.\n\\[\n\\frac{\\mbox{Perdidads y ganancias diarias}}{\\mbox{VaR diaria}}\n\\]"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Microeconomía Intermedia con R",
    "section": "",
    "text": "Una forma de iniciar el análisis de los individuos es plantear un conjunto básico de postulados, o axiomas, que describen el comportamiento racional del mismo. Supondremos que dadas tres canastas de consumo cualesquiera \\((x_1,x_2)\\), \\((y_1,y_2)\\) y \\((z_1,z_2)\\). El consumidor puede ordenarlas según su atractivo. Es decir, puede decidir que una de ellas es estrictamente mejor que la otra o bien que le son indiferentes.\nUtilizaremos la notación:\n\n\\(\\succ\\) Para indicar que una canasta se prefiere estrictamente a otra, es decir, \\((x_1,x_2) \\succ (y_1,y_2)\\).\n\\(\\sim\\) Para indicar que al consumidor le resulta indiferente elegir una u otra de las dos canastas de bienes y lo representamos matemáticamente como \\((x_1,x_2)\\sim (y_1,y_2)\\).\n\\(\\succeq\\) Para indicar si el individuo prefiere una de las dos canastas o es indiferente entre ellas, decimos que prefiere debilmente la canasta \\((x_1,x_2)\\) a la \\((y_1,y_2)\\) y escribimos \\((x_1,x_2)\\succeq (y_1,y_2)\\).\n\n\n\nCon base en lo anterior, ya estamos preparados para conocer los tres axiomas de la teoría del consumidor. Decimos que las preferencias son:\n\nCompletas: suponemos que es posible comprar dos canastas cualesquiera, es decir, dada cualquier canasta \\(\\textbf{X}\\) y cualquier canasrta \\(\\textbf{Y}\\), suponemos que \\((x_1,x_2)\\succeq (y_1,y_2)\\) o \\((y_1,y_2) \\succeq (x_1,x_2)\\) o las dos cosas, en cuyo caso el consumidor es indiferente entre las dos canastas.\nReflexivas: suponemos que cualquier canasta es al menos tan buena como ella misma: \\((x_1,x_2)\\succeq (y_1,y_2)\\).\nTransitiva: si \\((x_1,x_2)\\succeq (y_1,y_2)\\) y \\((y_1,y_2)\\succeq (z_1,z_2) \\Longrightarrow (x_1,x_2)\\succeq (z_1,z_2)\\). Es decir, si el consumidor piensa que la canasta \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Y}\\) y que la \\(\\textbf{Y}\\) es al menos tan buena como la \\(\\textbf{Z}\\), piensa que la \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Z}\\).\n\nConsidere que cuando nos referimos a las canastas \\(\\textbf{X}, \\textbf{Y}\\) o \\(\\textbf{Z}\\) estamos haciendo referencia a:\n\n\\(\\textbf{X} = (x_1,x_2)\\)\n\\(\\textbf{Y} = (y_1.y_2)\\)\n\\(\\textbf{Z} = (z_1,z_2)\\)\n\nSi las preferencias no fueran transitivas, podría muy bien haber un conjunto de canastas tal que ninguna de las elecciones fuera la mejor. Sin embargo, en el curso de microeconomía II estamos trabajando bajo el modelo tradicional, donde asumimos que el individuo es razonal, tomando en cuenta que siempre va a preferir mas que menos.\n\n\n\nEl primer axioma, la completitud, es dificilmente criticable, al menos en el caso de los tipos de elecciones que suelen analizar los economistas. Decir que pueden compararse dos canastas cualesquiera es decir simplemente que el consumidor es capaz de elegir entre dos canasas cualesquiera.\nEl segundo axioma, la reflexividad, plantea más problemas. Una canasta cualquiera es ciertamente tan buena como una canasta idéntica.\nEl tercer axioma, la transitividad, plantea más problemas. No esta claro que las preferencias deban tener necesariamente esta propiedad. El supuesto de que son transitivas no parece evidente desde un punto de vista puramente lógico, y, de hecho, no lo es. La transitividad es una hipótesis sobre la conducta de los individuos en sus elecciones y no una afirmación lógica. Sin embargo, no importa que sea o no un hecho lógico básico; lo que importa es que sea o no una descripción razonablemente exacta del comportamiento de los individuos.\n¿Qué pensarías de una persona que dijera que prefiere la canasta \\(\\textbf{X}\\) a la \\(\\textbf{Y}\\) y la \\(\\textbf{Y}\\) a la \\(\\textbf{Z}\\), pero que también dijera que prefiere la \\(\\textbf{Z}\\) a la \\(\\textbf{X}\\)? Desde luego, lo consideraríamos como prueba de una conducta particular. Y lo que es más importante, ¿Cómo se comportaría este consumidor si tuviera que elegir entre las tres canastas \\(\\textbf{X}, \\textbf{Y}\\) y \\(\\textbf{Z}\\)?"
  },
  {
    "objectID": "posts/post-with-code/index.html#curvas-de-indiferencia",
    "href": "posts/post-with-code/index.html#curvas-de-indiferencia",
    "title": "Microeconomía Intermedia con R",
    "section": "Curvas de Indiferencia",
    "text": "Curvas de Indiferencia\nCon base en la definición previa de utilidad, podemos concluir, una función de utilidad es la que explica la cantidad de utilidad que posee un consumidor dado su consumo de dos bienes diferentes. \\(x, y\\). Una curva de indiferencia es solo una rebanada infenitesimal de esa función que describe todas las diferentes combinaciones entre dos bienes que producen la misma cantidad de utilidad (es decir, a la que una persona sería indiferente).\nSupongamos que una persona clasifica las hamburguesas \\((y)\\) y las bebidas \\((x)\\) de acuerdo con la función de utilidad\n\\[\nU(x,y) = \\sqrt{xy}\n\\]\nEn el caso de esta función, obtenemos la curva de indiferencia identificando un conjunto de combinaciones de \\(x,y\\) en el cual la utilidad tiene el mismo valor. Suponga que arbitrariamente decimos que la utilidad tiene un valor de 10. Entonces, la ecuación de esta curva sera:\n\\[\nU(x,y) = 10 = \\sqrt{xy}\n\\]Note que si elevamos esta función al cuadrado se mantiene el mismo orden, por lo cual también podemos representar esta curva de indiferencia como\n\\[\n100 = xy\n\\]\nEs importante siempre despejar este tipo de ecuaciones para \\(y\\) la importancia esta en que será mucho más facil posteriormente encontrar su tasa marginal de sustitución ( en otra sección de esta publicación estudiaremos a detalle esto), entonces, al despejar obtenemos:\n\\[\ny = \\frac{100}{x}\n\\]\nPara trazar su curva de indiferencia, lo haremos en R , a continuación les muestro como hacerlo. Puedes realizar este ejercicio en tu PC tu mismo.\n\n# 1. Primero cargamos las librerias que utilizaremos, en caso que nos las tengas \n#    instaladas sugiero lo hagas usando install.package(\"libreria\") en su consola\n#    de Rstudio.\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(tidyverse)\nlibrary(plotly)\n\n# 2. Creamos la función de utilidad del ejemplo \nutilidad <- function(x,y){\n  sqrt(x*y) \n}\n\n# 3. Creamos una matriz para hacer un bucle en la función de utilidad\nvalores_matriz <- matrix(0,nrow = 200, ncol = 200)\n\n# 3.1 Llenamos la matriz con usando la función de utilidad \nfor(fila in 1:nrow(valores_matriz)){\n  for(columna in 1:ncol(valores_matriz)){\n    valores_matriz[fila,columna] <- utilidad(fila,columna)\n  }\n}\n\n# 4. Función que nos permitira graficar las curvas de indiferencia \n\nC_indiferencia <- function(entrada_utilidad){\n  y <- c()\n  \n  for(i in 1:50){\n    y_coord <- entrada_utilidad^2/i\n    y       <- c(y,y_coord)\n  }\n  \n  data <- data.frame(\n    x = 1:50,\n    y = y,\n    z = rep(entrada_utilidad,50)\n  )\n  \n  return(data)\n}\n\n\n# 4.1 Resultado de utilidades obtenidas \nlista_utilidades <- lapply(10, C_indiferencia)\n\nfull_df <- do.call(rbind, lista_utilidades)\n\nAhora si ya estamos preparados para graficar nuestras curvas de indiferencia para \\(10 = \\sqrt{xy}\\)\n\n# 5. Gráfico\n\nggplot() + \n  geom_point(data = full_df, aes(x = x, y = y, color = z)) + \n  geom_path(data = full_df, aes(x = x, y = y, color = z)) +\n  theme_minimal()+\n  ylim(0,100) + \n  labs(x = \"Bebidad\", y = \"Hamburguesas\") + \n  scale_color_continuous(name = \"Utilidad\")\n\n\n\n\nNote que la curva previa representa una utilidad = 10.\nA continuación te muestro un grafico animado de la curva de indiferencia previa. Para generar el gráfico presiona el boton PLAY.\n\n\n\n\n\n\nVeamos que sucede cuando tenemos diferentes niveles de utilidad, en base al resultado usted puede deducir su propio análisis.\n\nutilidad <- function(x,y){\n  sqrt(x*y) \n}\n\nvalores_matriz <- matrix(0,nrow = 200, ncol = 200)\n\nfor(fila in 1:nrow(valores_matriz)){\n  for(columna in 1:ncol(valores_matriz)){\n    valores_matriz[fila,columna] <- utilidad(fila,columna)\n  }\n}\n\nC_indiferencia <- function(entrada_utilidad){\n  y <- c()\n  \n  for(i in 1:100){\n    y_coord <- entrada_utilidad^2/i\n    y       <- c(y,y_coord)\n  }\n  \n  data <- data.frame(\n    x = 1:100,\n    y = y,\n    z = rep(entrada_utilidad,100)\n  )\n  \n  return(data)\n}\n\nlista_utilidades <- lapply(seq(from =10, to = 60, by = 10), C_indiferencia)\n\nfull_df <- do.call(rbind, lista_utilidades)\n\n\nggplot() +\n  geom_point(data = full_df, aes(x = x, y = y, color = z)) +\n  geom_path(data = full_df, aes(x = x, y = y, color = z)) +\n  theme_minimal() +\n  ylim(0,200) +\n  labs(x = \"Bebidas\", y = \"Hamburguesas\") +\n  scale_color_continuous(name = \"Utilidad\")\n\n\n\n\nAquí podemos señalar lo siguiente:\n\nA medida que aumenta la utilidad, las curvas se desplazan hacia la derecha y hacia la izquierda a medida que disminuye la utilidad.\nObserve que las curvas se inclinan hacia abajo, esto debe ser necesariamente el caso; a medida que uno aumenta su consumo de bebidas renuncia al otro bien que les era indiferente, hamburguesa.\nTodo lo que está debajo de la curva representa paquetes con menos utilidad. La teoría de la utilidad asume que un consumidor siempre buscará maximizar la utilidad.\nComprende que la pendiente no es lineal. En genera, cuanto más se tiene de algo, menos utilidad se obtendrá de otra unidad y, por el contrario, más se renunciaría a adquirir el otro bien. Esta pendiente tiene un nombre oficial: Tasa Marginal de Sustitución o TMS hablaremos de esto en una sección posterior.\n\nPero esas son solo algunas rebanadas que ya he señalado como infinitesimalmente pequeñas.\nPara concluir esta sección te dejo un gráfico animado de los diferentes niveles de utilidad, por favor presiona el boton PLAY para que logres verlo."
  },
  {
    "objectID": "posts/post-with-code/index.html#tasa-marginal-de-sustitución-tms",
    "href": "posts/post-with-code/index.html#tasa-marginal-de-sustitución-tms",
    "title": "Microeconomía Intermedia con R",
    "section": "Tasa Marginal de Sustitución (TMS)",
    "text": "Tasa Marginal de Sustitución (TMS)\nOtro concepto importante en la teoría del consumidor es la Tasa Marginal de Sustitución (TMS). Matemáticamente esto es la pendiente de la curva de indiferencia, sin embargo, en términos microecnómicos esta pendiente se refiere a la relación de cambio entre un bien \\(x\\) y el bien \\(y\\), es decir, cuanto del bien x se tiene que sacrificar (aumentar) para aumentar (disminuir) el consumo del bien y para aumentarse en el mismo nivel de utilidad.\nEn términos matemáticos la TMS se define como:\n\\[\nTMS = -\\left.\\begin{array}{c}\\frac{dy}{dx} \\end{array}\\right|_{U = U_1}\n\\]\ndonde la notación indica que la pendiente se debe calcular a lo largo de la cuva de indiferencia \\(U_1\\).\nUn ejercicio interesante para el lector, seria intentar probar la identidad de \\(TMS\\) que acabamos de definir.\n\nMúltiples Curvas de Indiferencia\nHay una curva de indiferencia que pasa por cada punto del plano \\(xy\\). Cada una de estas curvas muestra combinaciones de \\(x\\) y \\(y\\) que proporcionan al individuo determinado nivel de satisfación como se vio en graficos anteriores. Los movimientos en dirección noreste representan movimientos hacia niveles más altos de satisfación.\n\n\n\nEl cambio de la pendiente a lo largo de U1 muestra que la canasta de consumo disponible afecta los intercambios que esta persona realizará libremente\n\n\nDado que ya conocemos en que consiste la TMS, y a este punto asumo que el lector ya tiene idea de como realizar el computo manual de la TMS para una función de utilidad dada. Entonces, procederemos a realizar el computo de la TMS en R, para el computo se procederá a crear una función que resuelva el problema más rapidamente, ya que como usted se ha podido dar cuenta se necesita y usar calculo diferencial (derivadas).\nLa función que crearemos para realizar el computo de la TMS la llamaremos TMS, veamos como hacerlo en R.\n\nTMS <- function(fun.utilidad, bien_x) {\n  U  <- parse(text = fun.utilidad)\n  v1 <- D(U, \"x\")                       # D() función que realiza la derivada de U\n  print(paste(\"TMS = \", \n              eval(v1, envir = list(x = bien_x)), \"considerando\", \n              bien_x, \"unidades del bien x\"))\n}\n\nDel bloque de código previo:\n\nTMS: función que calcula la tasa marginal de sustitución.\nfun.utilidad: función de la curva de indiferencia (y en función de x). Tiene que especificarse en caracteres.\nbien_x: unidades del bien x en donde se evaluara la TMS.\n\n\n\nEjemplo\nconsideremos la siguiente función de utilidad con su respectivo nivel de utilidad\n\\[\nU(x,y) = 10 = \\sqrt{xy}\n\\]\nSi calculamos la TMS de esta curva de indiferencia de manera manual tendremos:\n\\[\nTMS = -\\frac{\\frac{\\partial U}{\\partial x}}{\\frac{\\partial U}{\\partial y}} = -\\frac{100}{x^2}\n\\]\nComo se puede dar cuenta la TMS depende de “x”, lo que indica que tenemos que variar “x” positiva o negativamente , con el fin de obtener menos o más de “y” y mantenernos en la misma utilidad de 10.\nSi evaluamos la TMS cuando el bien “x” es igual a 5, entonces, la TMS sera de\n\\[\nTMS = -\\frac{100}{5^2} = -4\n\\]\nEsto implica, que si aumentamos el consumo del bien “x” en 1 tendremos que disminuir el consumo del bien y en 4. Veamos este ejemplo en R.\n\n# bien_x = 5\n# Utilizamos la función TMS que creamos previamente \n\nTMS(fun.utilidad = \"100/x\", 5)\n\n[1] \"TMS =  -4 considerando 5 unidades del bien x\"\n\n\nNote que el resultado es el deseado. Pero si queremos ver como varía la TMS para distintas cantidades del bien “x”, podemos hacer pequeñas variaciones a la función (TMS) que definimos en el primer bloque de código de esta sección.\n\nVar_TMS <- function(fun.utilidad, bien_x){\n  U  <- parse(text = fun.utilidad)\n  v1 <- D(U, \"x\")\n  eval(v1, envir = list(x = bien_x))\n}\n\n\n# Veamos el comportamiento de la TMS cuando variamos el bien x\n\nw <- c()\nfor (i in seq(60, 10, -10)){\n  t <- Var_TMS(fun.utilidad = \"100/x\",i)\n  w <- c(w,t)\n}\n\nw\n\n[1] -0.02777778 -0.04000000 -0.06250000 -0.11111111 -0.25000000 -1.00000000\n\n\nVeamos que a medida que el bien x pasa de ser abundante a ser un bien escazo cada vez le resulta al consumidor más relevante y si desea obtener una unidad adicional del bien x tendrá que renunciar a más cantidad del bien y. Es así que si el consumidor tiene solo un bien, cambiará este bien siempre y cuando reciba cien unidades del bien y.\nEn la siguiente sección de este post verá el tema de maximización de la utilidad dado una restricción presupuestaria. Es decir, desarrollamaremos el cálculo óptimo de los bienes, que maximizan la función de utilidad."
  },
  {
    "objectID": "posts/r/index.html",
    "href": "posts/r/index.html",
    "title": "Introducción a R",
    "section": "",
    "text": "R es un lenguaje y un ambiente para el manejo de datos, cálculos, y gráficos en código libre. Dada estas características los desarrollos que se han realizado en R son abiertos y están disponibles gratuitamente, por lo cual su uso se ha difundido ampliamente. R es difundido libremente por una gran diversidad de sitios espejo del CRAN (The comprehensive R Archive Network: red de servidores en todo el mundo que almacenan versiones id’enticas y actualizadas de código y documentación para R). Además, de ser gratuitos, los desarrollos en R se actualizan más rápido que cualquier otro de los costosos softwares comerciales que se encuentran en el mercado. Esto es así debido a que los usuarios hacen desarrollos, los documentan y los difunden en su red especializada de manera cotidiana (Quintana y Mendoza, 2016,p.23).\nAntes de comenzar a programar es bueno conocer los aspectos básicos del software que se esta utilizando como son: el ambiente, el funcionamiento de las herramientas de ayuda y la sintaxis básica, necesaria para el desarrollo de cualquier proyecto. En la práctica, la programación en R no es dificil solo hace falta acostumbrarse al ambiente y familiarizarse a la sintaxis, la cual trataremos en este material.\n\n\n\nEs software libre y por tanto su costo es nulo. \nEs multiplataforma: existen versiones para LinuX, Mac y Windows. Los procedimientos y análisis desarrollados en una plataforma son perfectamente desarrollables en otra. \nImplementa una enorme cantidad de métodos estadísticos, desde los más clasicos a los más modernos. Los métodos se organizan en librerías cuyo número se encuentra en constante crecimiento.\n\nCapacidad para acceder a datos en múltiples formatos. Dispone de librerías para leer datos desdes SPSS,SAS,Access, MySQL,Excel, etc. A si mismo permite también la generació de informes de resultados en diversos formatos.\n\nEnorme capacidad para manipular y modificar datos y funciones.\n\nGeneración de gráficos de alta calidad.\n\nExistencia de una comunidad de usuarios muy activa, en la que participan estadísticos de renombre.\n\nAmplia disponibilidad de documentación, tanto en internet como en libros publicados por editoriales de prestigio. \nFacilidad de integración con actividad de formación en técnicas y métodos estadísticos en todos los ámbitos del conocimiento.\n\nExistencia de extensiones específicas para nuevas áreas como modelos gráficos o análisis de mercados financieros.\n\nTodos los algoritmos implementados en R pueden ser vistos e interpretados por cualquier usuario, por lo que este puede saber exactamente que es lo que hace el ordenador cuando ejecuta un comando.\n\n\n\n\n\nHay empresas que por políticas no pueden instalar software libre en sus maquinas cada una tiene su politica, sus software de preferencia, sus necesidades, etc.\n\nAlgunas de las instituciones del sector público y privado tienen un dilema. por parte necesitan ahorrar recursos y por otra parte tienen que contar con soporte técnico por el que pagan fortunas. La idea del soporte es tener el apoyo y mantenimiento por si algo sale mal tanto en la aplicación del software como en la administración de los sistemas. Por eso pagan licencias costosas por SAS, STATA y otros paquetes.\n\nUna de las principales desventajas es que hasta hace poco el uso de R estaba limitado a entornos universitarios y de usuarios con gran conocimiento de la estadística y la programación. Junto a esto, su primera impresión entre los usuarios principiantes, es de dureza y poca amigabilidad, aunque esto queda superado con el uso.\n\nNo hay nadie a quien reclamar si algo falla, ni hay un departamento de atención al cliente que nos diga qué podemos hacer si algo va mal, si alguién procedimiento nos da un error, o simplemente si no sabemos qué sintaxis utilizar. Pero a cambio existe una comunidad de usuarios organizada en foros y dispuesta a colaborar desinteresadamente en la resolución de problemas.\n\nA todos los puntos anteriores podemos añadir el siguiente, que será considerado por unos una ventaja y por otros un inconveniente: Para hacer un buen uso de R hay que tener un buen conocimiento de los métodos estadísticos. En realidad esta afirmación es cierta no sólo para R, sino para cualquier paquete estadístico.\n\n\n\n\nPara realizar la instalación de R y RStudio en Windows,Mac, Ubuntu o Linux se debe ingresar a los siguientes sitios web:\n\nInstalación de R\nInstalación de RStudio\n\n\n\n\nRPermite obtener ayuda para conocer toda la información (qué hace, cuál es la sintaxis correcta, qué parámetros tiene, algunos ejemplos de uso, etcétera) sobre una función, objeto o librería.\nExisten cinco funciones para obtener ayuda las cuales son:\n\nhelt.start()\n\n\nUtilizando esta función se encuentra un menú de recursos, entre los cuales existen manuales, referencias y demás material para comenzar a aprender R.\n\nescribe en tu consola de RStudio help.start()\n\nhelp(¨nombre del objeto¨)\n\n\nEsta función facilita obtener información acerca de las funciones de los paquetes ya instalados en R. Si se desea obtener información acerca de una función, por ejemplo de la función plot(), se debe escribir help(“plot”) o ?plot en la línea de comandos.\n\nexample(\"nombre de la función\")\n\n\nPara obtener ejemplos del uso de funciones, se utiliza la función example (). Porejemplo, escribeexample(“array”).\n\nexample(\"array\")\n\n\narray> dim(as.array(letters))\n[1] 26\n\narray> array(1:3, c(2,4)) # recycle 1:3 \"2 2/3 times\"\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    2    1\n[2,]    2    1    3    2\n\narray> #     [,1] [,2] [,3] [,4]\narray> #[1,]    1    3    2    1\narray> #[2,]    2    1    3    2\narray> \narray> \narray> \n\n\n\nlibrary(help = \"nombre\")\n\n\nOtra manera de obtener información de ayuda sobre un paquete es usar la opción help para el comando library(), con lo cual tendrás información más completa. Un ejemplo es library(help=“stats”).\n\nlibrary(help=\"stats\")\n\n\nvignette(“nombre de la librer ́ıa”)\n\n\nAlgunos paquetes ya instalados en R incluyen viñetas dentro del ordenador. Una viñeta es un documento corto que describe como se usa un paquete. Se puede ver una viñeta usando la función vignette(). Escribe vignette(“Sweave”) en la línea de comandos.\n\nvignette(\"Sweave\")\n\n\n\n\nLa forma correcta de almacenar valores, es a través de una asignación la cual se realiza especificando el símbolo <-. Del lado izquierdo del símbolo se especifica el nombre de la variable y del lado derecho se introduce el valor u operación.\n\nSe puede trabajar con una gran cantidad de operadores matemáticos que utiliza R y que permite realizar cálculos matemáticos, por mencionar algunos, se pueden observar en el siguiente cuadro\n\n\n\nOperador Matemático\nFunción en R\n\n\n\n\n\\(\\sqrt{x}\\)\nsqrt()\n\n\n\\(e^x\\)\nexp(x)\n\n\n\\(x!\\)\nfactorial(x)\n\n\n\\(logaritmo(x)\\)\nlog(x)\n\n\n\\(\\pi\\)\nPi\n\n\n\\(|x|\\)\nabs(x)\n\n\n\\(seno(x)\\)\nsin(x)\n\n\n\\(coseno(x)\\)\ncos(x)\n\n\n\\(tangente(x)\\)\ntan(x)\n\n\n\\(cos^{-1}(x)\\)\nacos(x)\n\n\n\\(sen^{-1}(x)\\)\nasin(x)\n\n\n\\(tan^{-1}(x)\\)\natan(x)\n\n\n\nAsignar un valor a cierta cantidad de variables por ejemplo: a una variable \\(w\\) el valor 3, a la variable \\(y\\) el valor 7 y a la variable \\(z\\) el valor 90, a una variable \\(suma\\) la adición de las variables anteriores y finalmente obtendremos la raíz cuadrada de la variable \\(suma\\) guardándola en una variable con el nombre raíz.\nA continuació le muestro el ejemplo en R\n\nw <- 3   # Para evaluar la instrucción se debe presionar la tecla Control + ENTER.\nw        # Para observar el valor de la variable nombra la variable.\n\n[1] 3\n\ny <- 7\ny\n\n[1] 7\n\nz <- 90 \nz\n\n[1] 90\n\nsuma <- w + y + z\nsuma\n\n[1] 100\n\nraiz <- sqrt(suma)\nraiz\n\n[1] 10\n\n\nEn la primera línea se observa el simbolo (#), el cual permite comentar el código, para tomar notas de interés.\nEn R tamién se puede almacenar cadenas de caracteres como se muestra en el siguiente ejemplo:\n\na <- \"Cálculo\"\na\n\n[1] \"Cálculo\"\n\nb <- \"Microeconomía\"\nb\n\n[1] \"Microeconomía\"\n\n\nPara obtener un listado o desplegado de las variables que han sido definidas en la sesio ́n se debe de escribir el comando ls().\n\nls()\n\n[1] \"a\"    \"b\"    \"raiz\" \"suma\" \"w\"    \"y\"    \"z\"   \n\n\n\n\n\nUn vector es una secuencia ordenada de datos, los cuales han de ser del mismo tipo, es decir, todos deben de ser números, caracteres, cadenas de caracteres, valores lógicos, etc. Los tipos de datos que se pueden almacenar en un vector se destacan los siguientes:\n\nlogical (lógicos: TRUE, verdadero, o FALSE, falso)\ninteger (números enteros)\nnumeric (números reales)\ncharacter (palabras)\n\n\n\nLa forma correcta de almacenar un conjunto de datos, es a través de una asignación utilizando el comando c, donde dicha lista de números se almacenan bajo nombre, y así mismo este se utiliza para referirse a los datos que almacena, la asignación se realiza especificando el símbolo <-.\n\nPara generar un vector utilizamos la función c separado cada uno de los elementos por medio de una coma (,) por ejemplo si se quisiera almacenar la secuencia \\(0,1,2,3,4,5,6,7,8,9\\) dentro de un vector llamado \\(vector\\)\n\nvector <- c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nvector\n\n [1] 0 1 2 3 4 5 6 7 8 9\n\n\nSi se desea crear un vector de letras, palabras o cadenas de caracteres llamadas string, se tiene que nombrar cada cadena de caracteres entre comillas de manera obligatoria\n\nvectorletra <- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nvectorletra\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nvectorpalabra <- c(\"Micro\", \"Economía\", \"en\", \"R\")\nvectorpalabra\n\n[1] \"Micro\"    \"Economía\" \"en\"       \"R\"       \n\n\nSe puede facilitar la creación de vectores podemos utilizar c(a:b) para datos de manera consecutiva, el comando seq(a, b, by = p) de manera aritmética, donde \\(a\\) es el primer elemento, \\(b\\) es el último elemento y \\(p\\) es la diferencia de cada elemento.\n\nw <- c(0:10)\nw\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\ny <- seq(0, 100, by = 10)\ny\n\n [1]   0  10  20  30  40  50  60  70  80  90 100\n\n\n\n\n\nSe pueden realizar operaciones como suma, resta, producto de vectores, se utilizaran los vectores \\(w\\) e \\(y\\) para ejemplificar las operaciones.\n\nsuma <- w + y\nsuma \n\n [1]   0  11  22  33  44  55  66  77  88  99 110\n\nresta <- w - y\nresta\n\n [1]   0  -9 -18 -27 -36 -45 -54 -63 -72 -81 -90\n\nproducto <- w*y\nproducto\n\n [1]    0   10   40   90  160  250  360  490  640  810 1000\n\n\nEl manejo de vectores en R tiene una propiedad muy útil: podemos aplicar una función a todos los elementos de un vector en un solo paso.\n\nw + 5\n\n [1]  5  6  7  8  9 10 11 12 13 14 15\n\nw - 2\n\n [1] -2 -1  0  1  2  3  4  5  6  7  8\n\n10*w\n\n [1]   0  10  20  30  40  50  60  70  80  90 100\n\nsqrt(w)\n\n [1] 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n [9] 2.828427 3.000000 3.162278\n\nw^2\n\n [1]   0   1   4   9  16  25  36  49  64  81 100\n\n\nEntre otras funciones para aplicar a vectores, y de gran importancia son las relacionadas principalmente con la estadística. Por ejemplo\n\nmax y min calculan sus valores maximos y minimos respectivamente\nsum calcula la suma\nprod calcula el producto\nmean calcula la media\ndiff calcula el vector formado por las diferencias sucesivas entre entradas del vector original.\nsort ordena los elementos del vector en el orden natural creciente del tipo de datos que lo forman, se puede incluir en su argumento el parámetro decreasing = TRUE.\n\n\nw\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nmax(w)\n\n[1] 10\n\nmin(w)\n\n[1] 0\n\nsum(w)\n\n[1] 55\n\nprod(w)\n\n[1] 0\n\nmean(w)\n\n[1] 5\n\ndiff(w)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nsort(w)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nsort(w, decreasing = TRUE)\n\n [1] 10  9  8  7  6  5  4  3  2  1  0\n\n\n\n\n\n\nLas matrices son un tipo de vector particular, es un vector con un atributo especial, llamado dimensión. La dimensión establece el número de renglones y el número de columnas que tendrá una matriz, se debe recordar que una matriz no es más que un arreglo de números en \\(m\\) renglones y \\(n\\) columnas.\nPor ejemplo una matriz de 3 renglones y 3 columnas\n\\[\\left[\\begin{array}{ccc}1 & 2 & 3 \\\\2 & 4 & 5 \\\\3 & 5 & 6\\end{array}\\right]\\] Se dispone de dos maneras básicas de definir una matriz en R. En primer lugar, la instrucción:\n\\[matrix(vector,nrow = n, byrow = valorlogico)\\]\nDefine una matriz de \\(n\\) filas (rows) formada por las entradas del vector. Si se captura byrow = TRUE, la matriz se construye por filas, mientras que con byrow = FALSE se construye por columnas; este último es el valor por defecto, por lo que no hace falta especificarlo. En vez de emplear nrow, se puede indicar el número de columnas con ncol. Veamos algunos ejemplos:\n\nmatrix(1:6,nrow = 2)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmatrix(1:6, nrow = 3)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nmatrix(1:6, nrow = 2, byrow = TRUE)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\nmatrix(1:6, nrow = 3, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nObserve cómo muestra R las matrices: indica las filas con \\([i,]\\), donde \\(i\\) es el índice de la fila, y las columnas con \\([,j]\\), donde \\(j\\) es el índice de la columna. Otra posible manera de definir matrices es combinando filas o columnas. La instrucción:\n\\[rbind(vector1,vector2, vector3)\\] construya la matriz de filas \\(vector1, vector2, . . . , vector N\\) que han de tener la misma longitud en este orden. Si en lugar de rbind se usa cbind, se obtiene la matriz cuyas columnas son los vectores a los que se aplica.\n\nrbind(c(1, 0, 2), c(2, 3, 6), c(1, 2, 0))\n\n     [,1] [,2] [,3]\n[1,]    1    0    2\n[2,]    2    3    6\n[3,]    1    2    0\n\ncbind(c(1, 0, 2), c(2, 3, 6), c(1, 2, 0))\n\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    0    3    2\n[3,]    2    6    0\n\n\n\n\n\nLa manera más conveniente de guardar una tabla de datos en R es en forma de \\(dataframe\\). En concreto, un \\(data\\) \\(frame\\) es una tabla de doble entrada, formada por variables en las columnas y observaciones de estas variables en las filas, de manera que cada fila contiene los valores de las variables para un mismo caso o individuo. En ese sentido, un \\(data\\) \\(frame\\) tiene la apariencia de una matriz, pero con la diferencia de que cada columna de un \\(data\\) \\(frame\\) puede contener datos de un tipo diferente siempre que todos los datos de una misma columna sean del mismo tipo porque corresponden a observaciones de una misma propiedad: así, una columna puede estar formada por números, por palabras, por valores lógicos, etcétera. De esta manera, las columnas de un data frame son vectores, mientras que las filas son listas.\n\n\nPara construir un \\(data\\) \\(frame\\) a partir de unos vectores, se usa la función data.frame aplicada a los vectores en el orden en el que queramos disponer las columnas de la tabla; de esta manera, las variables tomarán los nombres de los vectores. Estos nombres también se pueden especificar en el argumento de la función data.frame, entrando cada columna con una construcción de la forma:\n\\[Nombre~variable = vector~con~el~contenido~de~la~variable\\]\nPara ilustrar esta función usemos un ejemplo sencillo:\n\nUna compañía de seguros desea crear una base de datos para la gestión de las pólizas de sus asegurados. Para ello, los datos de los que dispone son los siguientes:\n\nDe cada póliza se guarda el número de póliza.\nEl tipo que puede ser “Hogar” o “Auto”.\nLa fecha de creación de la póliza.\ny el conjunto de coberturas incluidas en la póliza ( a elegir entre Incendio, Robo, Terceros y Responsabilidad Civil).\nPara cada póliza guardamos los atos de sus titulares, y sabemos que cada poliza tiene un único titular.\nDe los titulares guardamos nombre, sexo, edad y estado de providencia.\n\n\nPoliza <- c(1:9)\n\nTipo <- c(\"Hogar\", \"Auto\", \"Auto\", \"Auto\", \"Hogar\", \"Hogar\", \"Auto\",\n           \"Auto\", \"Hogar\")\n\nFecha <- c(\"12/12/2016\", \"08/02/2014\", \"10/08/2012\", \"01/01/2015\",\n           \"21/11/2011\", \"18/01/2016\", \"12/04/2005\", \"29/03/2007\",\n           \"18/02/2009\")\n\nCoberturas <- c(\"Incendio\", \"Robo\", \"Terceros\", \"Robo\", \"Robo\",\n                \"Incendio\", \"Terceros\", \"R. Civil\", \"Incendio\")\n\nNombre <- c(\"Carlos\", \"Nancy\", \"Pedro\", \"Cecilia\", \"Ricardo\", \"Sofia\",\n            \"Armando\", \"Vicente\", \"Fernando\")\n\nSexo <- c(\"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\")\n\nEdad <- c(25, 35, 45, 47, 24, 43, 33, 31, 40)\n\nEstado <- c(\"Campeche\", \"Chiapas\", \"Ciudad de M ́exico\", \"Coahuila\",\n            \"Durango\", \"Guanajuato\", \"Guerrero\", \"Hidalgo\", \"Jalisco\")\n\ndataframe= data.frame(Poliza, Tipo, Fecha, Coberturas, Nombre, Sexo, Edad)\n\ndataframe\n\n  Poliza  Tipo      Fecha Coberturas   Nombre Sexo Edad\n1      1 Hogar 12/12/2016   Incendio   Carlos    M   25\n2      2  Auto 08/02/2014       Robo    Nancy    F   35\n3      3  Auto 10/08/2012   Terceros    Pedro    M   45\n4      4  Auto 01/01/2015       Robo  Cecilia    F   47\n5      5 Hogar 21/11/2011       Robo  Ricardo    M   24\n6      6 Hogar 18/01/2016   Incendio    Sofia    F   43\n7      7  Auto 12/04/2005   Terceros  Armando    M   33\n8      8  Auto 29/03/2007   R. Civil  Vicente    M   31\n9      9 Hogar 18/02/2009   Incendio Fernando    M   40\n\n\n\n\n\n\nR es un lenguaje que permite la implementación de paquetes adicionales que le dan una capacidad de gestión de datos más amplia y permiten la implementación de nuevas funciones que harán de R un programa que se adapte a las necesidades.\n\nEl procedimiento para instalar un paquete depende del sistema operativo usado y de la manera como se instalo R: ya sea desde el código fuente o desde o por medio de archivos binarios pre-compilados. Existen varias funciones para manejar paquetes tales como:\n\ninstalled.packages()\nCRAN.package()\ndownload.packages()\n\nPara verificar la versión de paquetes ya instalados en el sistema y actualizarlos a la versión más reciente utilizamos la siguiente función:\n\nupdate.packages()"
  },
  {
    "objectID": "posts/tidyverse/index.html",
    "href": "posts/tidyverse/index.html",
    "title": "Machine Learning en Tidyverse",
    "section": "",
    "text": "Asumire que el lector tiene cierto conocimiento de la teoría de modelos lineales, en caso de no ser así, no te preocupes visita este link para que puedas ir a leer las generalidades de estos modelos y su uso en R, principalmente los tres paquetes de broom que le permiten explorar estos modelos. En este post trataremos de combinar estas técnica para aprender más sobre estos modelos y sus datos.\nA continuación cargaremos algunos de los paquetes que nos ayudaran para poder realizar nuestra tarea,\n\nlibrary(tidyverse) # para manipulación de datos\nlibrary(gapminder) # marco de datos que utilizaremos \nlibrary(dslabs)    # conjunto de datos y funciones para analisis de datos\nlibrary(broom)     # Resumen informacion sobre objetos estadisticos en tibbles\n\nRecuerde que el marco de gapminder contiene informacion sobbre cada país desde 1960 hasta 2016. Crearemos una variable que llamaremos gap_anidado para obtener que las características de cada país este anidadas como un tibble. La ventaja de usar estos tibble es que podemos construir modelos lineales simples que predicen la esperanza de vida por año para cada país. Nos centraremos en aprender a usar los coeficientes de estos modelos para obtener nuevos conocimientos sobre los datos de gapminder.\n\ngap_anidado <- gapminder %>% group_by(country) %>% nest()\n\nhead(gap_anidado)\n\n# A tibble: 6 × 2\n# Groups:   country [6]\n  country             data             \n  <fct>               <list>           \n1 Albania             <tibble [57 × 8]>\n2 Algeria             <tibble [57 × 8]>\n3 Angola              <tibble [57 × 8]>\n4 Antigua and Barbuda <tibble [57 × 8]>\n5 Argentina           <tibble [57 × 8]>\n6 Armenia             <tibble [57 × 8]>\n\n\nTal como lo mencionamos previamente, gap_anidado contiene las características de cada país anidadas como un tibble. Con esto hecho, procederemos a construir modelos lineales para cada país, para ello usaremos la función map() del paquete purrr\n\ngap_models <- gap_anidado %>% \n  mutate(model = map(data, ~lm(life_expectancy~year,data = .x)))\n\ngap_models\n\n# A tibble: 185 × 3\n# Groups:   country [185]\n   country             data              model \n   <fct>               <list>            <list>\n 1 Albania             <tibble [57 × 8]> <lm>  \n 2 Algeria             <tibble [57 × 8]> <lm>  \n 3 Angola              <tibble [57 × 8]> <lm>  \n 4 Antigua and Barbuda <tibble [57 × 8]> <lm>  \n 5 Argentina           <tibble [57 × 8]> <lm>  \n 6 Armenia             <tibble [57 × 8]> <lm>  \n 7 Aruba               <tibble [57 × 8]> <lm>  \n 8 Australia           <tibble [57 × 8]> <lm>  \n 9 Austria             <tibble [57 × 8]> <lm>  \n10 Azerbaijan          <tibble [57 × 8]> <lm>  \n# … with 175 more rows\n\n\n\n\n\\[y = \\alpha + \\beta x\\]\nRepasemos brevemente cómo interpretar los coeficientes para un modelo de regresión lineal simple. Recuerda que esto implica calcular dos términos de coeficientes que relacionan la variable dependiente con la variable independiente \\(x\\).\nPara nuestros modelos, las variales:\n\n\\(y\\): Representa la esperanza de vida en relación con el año (variable \\(x\\)).\n\\(\\alpha\\): Representa el coeficiente del intercepto, nos dice la esperanza en el año 0. Esto no es significativo para nuestros datos, por lo que lo pasaremos por alto.\n\\(\\beta\\): Es el coeficiente del año (variable \\(x\\)), que para un modelo de regresión lineal simple corresponde directamente a la pendiente del mismo.\n\nUsando la función tidy() del paquete broom en el primer modelo, aprenderemos que con cada año que pasa la esperanza de vida promedio de la población de este país en particular aumenta aproximadamente 0.23 años. Este enfoque puede brindarle información sobre el crecimiento o la falta de crecimiento en la esperanza de vida a lo largo del tiempo para los países que esta modelando.\n\ntidy(gap_models$model[[1]])\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) -397.     12.4         -32.1 2.48e-37\n2 year           0.236   0.00622      38.0 3.72e-41\n\n\n\n\n\nPuede generar estos coeficientes mapeando la funcíon tidy() para cada uno de nuestros modelos y luego simplificando el nuevo marco de datos usando la función unnest(). Esto da como resultado un tibble que contiene la estimación para cada coeficiente de cada país.\n\ngap_models %>% \n  mutate(coef = map(model, ~tidy(.x))) %>% \n  unnest(coef)\n\n# A tibble: 370 × 8\n# Groups:   country [185]\n   country             data     model  term    estimate std.e…¹ stati…²  p.value\n   <fct>               <list>   <list> <chr>      <dbl>   <dbl>   <dbl>    <dbl>\n 1 Albania             <tibble> <lm>   (Inter… -3.97e+2 1.24e+1   -32.1 2.48e-37\n 2 Albania             <tibble> <lm>   year     2.36e-1 6.22e-3    38.0 3.72e-41\n 3 Algeria             <tibble> <lm>   (Inter… -1.10e+3 4.05e+1   -27.2 1.51e-33\n 4 Algeria             <tibble> <lm>   year     5.86e-1 2.04e-2    28.8 7.84e-35\n 5 Angola              <tibble> <lm>   (Inter… -7.48e+2 1.12e+1   -67.0 2.03e-54\n 6 Angola              <tibble> <lm>   year     4.01e-1 5.62e-3    71.4 6.69e-56\n 7 Antigua and Barbuda <tibble> <lm>   (Inter… -3.79e+2 1.56e+1   -24.2 5.19e-31\n 8 Antigua and Barbuda <tibble> <lm>   year     2.26e-1 7.87e-3    28.8 7.64e-35\n 9 Argentina           <tibble> <lm>   (Inter… -3.56e+2 7.67e+0   -46.4 8.83e-46\n10 Argentina           <tibble> <lm>   year     2.15e-1 3.86e-3    55.7 4.58e-50\n# … with 360 more rows, and abbreviated variable names ¹​std.error, ²​statistic\n\n\n\n\n\nAnteriormente aprovechamos la función tidy() de broom para explorar los coeficientes de nuestros modelos. Al hacerlo, obtuvimos información sobre cómo cambió la esperanza de vida con el tiempo para cada uno de los países en nuestro conjunto de datos. Ahora, aprenderá a usar la función glance() de broom para medir que tan bien se ajusta cada uno de estos modelos a sus datos subyacentes.\nUna forma de medir el ajuste de un modelo de regresión lineal es calcular su métrica \\(R^2\\)\n\\[\nR^2 = \\frac{\\%~variación~explicada~por~el~modelo}{\\%~variación~total~de~los~datos}\n\\]\nLa métrica \\(R^2\\) mide la relación entre la variación explicada por el modelo de regresión y la variación total de los datos. Toma valores entre 0 y 1.\nEn la siguiente figura, le muestro dos ejemplos, el primero con un valor alto y el segundo con un valor bajo de su \\(R^2\\) respectivamente. Note que en el caso donde el \\(R^2 = 0.009\\) es bajo o cercano a cero, esto nos indica que un modelo lineal esta capturando una cantidad proporcionalmente pequeña de la variación en los datos y. por lo tanto, no se ajusta bien. Por el contrario el modelo con \\(R^2 = 0.965\\) valor que es más cercano a 1, lo que indica que este modelo lineal se ajusta bien a los datos. Puede evaluar el ajuste de los modelos midiendo el valor del \\(R^2\\) para cada modelo.\n\nMuy bien, con el conocimiento previo, hechemos un vistazo a nuestros modelos. Para ello usamos map() y glance() para crear un marco de datos de estadísticas de resumen para cada modelo almacenado como la columna coef. Luego, puede simplificar estos marcos de datos usando la función unnest(). Esto nos dará como resultado un tibble que contendrá las estadisticas del modelo para cada modelo de país.\n\nmodel_perf <- gap_models %>% \n  mutate(coef = map(model,~glance(.x))) %>% \n  unnest(coef)\n\nmodel_perf\n\n# A tibble: 185 × 15\n# Groups:   country [185]\n   country    data     model r.squ…¹ adj.r…² sigma stati…³  p.value    df logLik\n   <fct>      <list>   <lis>   <dbl>   <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl>\n 1 Albania    <tibble> <lm>    0.963   0.963 0.772  1443.  3.72e-41     1  -65.1\n 2 Algeria    <tibble> <lm>    0.938   0.937 2.53    828.  7.84e-35     1 -133. \n 3 Angola     <tibble> <lm>    0.989   0.989 0.698  5091.  6.69e-56     1  -59.3\n 4 Antigua a… <tibble> <lm>    0.938   0.937 0.977   828.  7.64e-35     1  -78.6\n 5 Argentina  <tibble> <lm>    0.983   0.982 0.479  3103.  4.58e-50     1  -37.9\n 6 Armenia    <tibble> <lm>    0.288   0.275 1.57     22.2 1.70e- 5     1 -106. \n 7 Aruba      <tibble> <lm>    0.882   0.880 0.964   412.  3.28e-27     1  -77.8\n 8 Australia  <tibble> <lm>    0.983   0.983 0.540  3240.  1.42e-50     1  -44.7\n 9 Austria    <tibble> <lm>    0.989   0.989 0.430  4949.  1.45e-55     1  -31.7\n10 Azerbaijan <tibble> <lm>    0.679   0.673 1.54    116.  3.48e-15     1 -105. \n# … with 175 more rows, 5 more variables: AIC <dbl>, BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>, and abbreviated variable names ¹​r.squared,\n#   ²​adj.r.squared, ³​statistic\n\n\nSi observamos los valores de \\(R^2\\) de los primeros 5 modelos notamos que tienen un \\(R^2\\) alto, lo que nos dice que los modelos para estos países se han ajustado bien a los datos de esos países en particular.\n\n\n\nSiendo un poco más curiosos, tratemos de explorar el ajuste de los modelos. Para ver esto, podemos filtrar los valores más alto de r.squared, tomaremos como un r.squared alto 0.995 en adelante.\nPor ejemplo, podemos usar la función slice_max() de dplyr para encontrar los modelos que mejor se ajustan. Asimismo, podemos encontrar los modelos con el peor ajuste utilizando la función slice_min(). Hechemos un vistazo al código y los resultados generados,\n\nmejores_models <- model_perf %>% filter(r.squared > 0.995)\nmejores_models\n\n# A tibble: 3 × 15\n# Groups:   country [3]\n  country     data     model r.squ…¹ adj.r…² sigma stati…³  p.value    df logLik\n  <fct>       <list>   <lis>   <dbl>   <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl>\n1 Bahamas     <tibble> <lm>    0.995   0.995 0.235  11428. 1.74e-65     1  2.67 \n2 Israel      <tibble> <lm>    0.996   0.996 0.250  15626. 3.29e-69     1 -0.826\n3 Switzerland <tibble> <lm>    0.996   0.995 0.244  12349. 2.08e-66     1  0.508\n# … with 5 more variables: AIC <dbl>, BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>, and abbreviated variable names ¹​r.squared,\n#   ²​adj.r.squared, ³​statistic\n\n\ny para los peores filtremos los países que tienen un modelo con un r.squared menor a 0.3\n\npeores_modelos <- model_perf %>% filter(r.squared < 0.3) \npeores_modelos\n\n# A tibble: 12 × 15\n# Groups:   country [12]\n   country    data     model r.squ…¹ adj.r.…² sigma stati…³ p.value    df logLik\n   <fct>      <list>   <lis>   <dbl>    <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl>\n 1 Armenia    <tibble> <lm>  2.88e-1  0.275    1.57 2.22e+1 1.70e-5     1 -106. \n 2 Botswana   <tibble> <lm>  8.21e-4 -0.0173   4.98 4.52e-2 8.32e-1     1 -171. \n 3 Central A… <tibble> <lm>  2.76e-1  0.262    3.13 2.09e+1 2.76e-5     1 -145. \n 4 Latvia     <tibble> <lm>  1.92e-1  0.177    1.88 1.31e+1 6.49e-4     1 -116. \n 5 Lesotho    <tibble> <lm>  3.46e-2  0.0170   5.23 1.97e+0 1.66e-1     1 -174. \n 6 Lithuania  <tibble> <lm>  2.94e-1  0.281    1.24 2.29e+1 1.32e-5     1  -92.0\n 7 Russia     <tibble> <lm>  2.63e-2  0.00856  1.80 1.48e+0 2.28e-1     1 -113. \n 8 South Afr… <tibble> <lm>  2.69e-1  0.256    3.57 2.03e+1 3.54e-5     1 -152. \n 9 Swaziland  <tibble> <lm>  6.92e-5 -0.0181   5.83 3.80e-3 9.51e-1     1 -180. \n10 Ukraine    <tibble> <lm>  1.73e-1  0.158    1.41 1.15e+1 1.30e-3     1  -99.4\n11 Zambia     <tibble> <lm>  4.13e-2  0.0239   4.13 2.37e+0 1.30e-1     1 -161. \n12 Zimbabwe   <tibble> <lm>  1.37e-1  0.122    5.75 8.75e+0 4.55e-3     1 -180. \n# … with 5 more variables: AIC <dbl>, BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>, and abbreviated variable names ¹​r.squared,\n#   ²​adj.r.squared, ³​statistic\n\n\n\n\n\nPara hacer esto, primero debe crear un marco de datos que contenga tanto los valores predichos como los originales. Esto requiere primero usar map() y augment() para trabajar en la columna de la lista que contiene los modelos para crear marcos de datos anidados que contengan tanto los valores originales como los predichos. Luego, puede usar unnest() en esta nueva columna para simplificar estos marcos de datos y permitir una mayor exploración.\n\naugment_models <- gap_models %>% \n  mutate(augmented = map(model,~augment(.x))) %>% \n  unnest(augmented)\n\naugment_models\n\n# A tibble: 10,545 × 11\n# Groups:   country [185]\n   country data     model  life_exp…¹  year .fitted .resid   .hat .sigma .cooksd\n   <fct>   <list>   <list>      <dbl> <int>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1 Albania <tibble> <lm>         62.9  1960    65.7 -2.80  0.0684  0.672 0.517  \n 2 Albania <tibble> <lm>         63.9  1961    65.9 -1.99  0.0648  0.728 0.245  \n 3 Albania <tibble> <lm>         64.8  1962    66.1 -1.30  0.0614  0.758 0.0989 \n 4 Albania <tibble> <lm>         65.6  1963    66.4 -0.778 0.0581  0.772 0.0332 \n 5 Albania <tibble> <lm>         66.2  1964    66.6 -0.434 0.0549  0.777 0.00970\n 6 Albania <tibble> <lm>         66.6  1965    66.9 -0.260 0.0518  0.779 0.00327\n 7 Albania <tibble> <lm>         66.9  1966    67.1 -0.206 0.0489  0.779 0.00193\n 8 Albania <tibble> <lm>         67.1  1967    67.3 -0.213 0.0461  0.779 0.00192\n 9 Albania <tibble> <lm>         67.3  1968    67.6 -0.239 0.0435  0.779 0.00227\n10 Albania <tibble> <lm>         67.6  1969    67.8 -0.245 0.0409  0.779 0.00224\n# … with 10,535 more rows, 1 more variable: .std.resid <dbl>, and abbreviated\n#   variable name ¹​life_expectancy\n\n\nAhora, visualizaremos algunos de estos modelos.\n\n\n\nNote que dado que su \\(R^2\\) es bastante alto, podemos asumir que el modelo lineal se ajusta bien a los datos. Puede comparar el ajuste del modelo con los datos originales trazando ambos en el mismo gráfico. En este ejemplo, usaremos ggplot2 para trazar los valores originales de la esperanza de vida como un diagrama de dispensión usando geom_point() y agregué el ajuste del modelo lineal como una línea roja usando geom_line()\n\naugment_models %>% filter(country == \"Bahamas\") %>% \n  ggplot(aes(x = year, y = life_expectancy)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"red\") + \n  labs(title = \"Modelo Regresión Lineal para Bahamas\",\n       x = \"Año\",\n       y = \"Esperanza de Vida\") + \n  theme_minimal()\n\n\n\n\nSi observa el gráfico, podemos pensar que un modelo de regresión lineal se va ajustando bien a los datos de este país en particular.\n\n\n\nAhora veamos el modelo correspondiente al país Ukraine, que tiene un valor de \\(R^2\\) super más bajo que el de Bahamas. Claramente, esperariamos encontrarnos con un modelo que no se ajuste bien a los datos dado el antecedente del \\(R^2\\)\n\naugment_models %>% filter(country == \"Ukraine\") %>% \n  ggplot(aes(x = year, y = life_expectancy)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"red\") + \n  labs(title = \"Modelo de Regresión Lineal para Ukraine\", \n       x = \"Año\", \n       y = \"Esperanza de  Vida\") + \n  theme_minimal()\n\n\n\n\nComo pudo ver en estos dos ejemplos, augment() y ggplot() facilitan la exploración visual del ajuste de un modelo.\n\n\n\nEn este caso, prepararemos los cuatro mejores modelos que consideramos anteriormente y los peores y los visualizaremos,\n\nmejores_augment <- mejores_models %>% \n  mutate(augmented = map(model, ~augment(.x))) %>% \n  unnest(augmented)\n\npeores_augment <- peores_modelos %>% \n  mutate(augmented = map(model, ~augment(.x))) %>% \n  unnest(augmented)\n\nBien, ahora visualizamos los modelos\n\nmejores_augment %>% \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_expectancy)) + \n  geom_line(aes(y = .fitted), color = \"red\") + \n  facet_wrap(~country, scales = \"free_y\") + \n  theme_minimal()\n\n\n\n\n\npeores_augment %>% \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_expectancy)) + \n  geom_line(aes(y = .fitted), color = \"red\") + \n  facet_wrap(~country, scales = \"free_y\") + \n  theme_minimal()\n\n\n\n\nParcelas geniales! Puede ver que un modelo lineal hace un gran trabajo para los mejores 3 modelos de ajuste, pero los peores modelos de ajuste no parecen tener una relación lineal. Trabajaremos para mejorar este ajuste en la proxima serie de ejercicios mediante la incorporación de funciones adicionales.\n\n\n\nCon la información que reunimos con Augment() y glance(), aprendimos que algunos de los modelos de regresión lineal simple no se ajustan adecuadamente a las tendencias subyacentes de nuestros datos. Para separar esto emplearemos un modelo de regresión múltiple.\n\n\n\\[Y = \\alpha + \\beta_1x_1 9 \\beta_2x_2 + . . . \\]\nEste modelo es una extensión natural del modelo de regresión lineal simple. La diferencia clave es que usa más variables explicativas para explicar el resultados, lo que significa que, en lugar de ajustar una linea de mejor ajuste, estamos ajustando un plano multidimensional. En el conjunto de datos gapminder, podemos usar características adicionales de nuestras observaciones para modelr la esperanza de vida. Entonces, vamos a usarlo,\nLa elección de que características usar se puede controlar en el campo de fórmula de la función lm(). Recuerde que para un modelo simple usamos la fórmula de la esperanza de vida explicada por año. De manera similar para un modelo de regresión múltiple, puede definir explícitamente la fórmula incluyendo el nombre de cada característica separada por un signo + o si sabe que desa incluir todas las características, puede capturarlas usando un punto,como veremos posteriormente.\n\n\n\nEl comportamiento de las funciones de broom sigue siendo el mismo. tidy() devuelve las estimaciones de los coeficientes de los modelos, esto ahora incluye estimaciones para las cuatro características adicionales. Lo mismo ocurre con augment(), además, de los valores ajustados para cada observación, se devuelven los valores de cuatro caracteristicas nuevas. y aunque la salida esperada de glance() sigue siendo la misma, tenemos que cambiar nuestro enfoque del valor de r cuadrado al valor de r cuadrado ajustado al evaluar el ajuste de nuestros modelos o comparar modelos de regresión lineal simple y múltiple.\n\n\n\nRecuerde que r.squared mide la variación explicada por el modelo. Agregar cualquier característica nueva a un modelo, independientemente de su relación con la variable dependiente, siempre aumentará el valor de r.squared del modelo. Esto se vuelve problemático cuando se compara el ajuste de modelos con diferente número de características explicativas utilizadas. Para compensar esto, en su lugar, utilizará el valor adj.r.squared (r cuadrado ajustado) esta es una métrica r cuadrada modificada cuyo cálculo tiene en cuenta la cantidad de características utilizadas en el modelo.\nLa interpretación del adj.r.squared es muy similar al r.squared y lo usaremos para evaluar el ajuste de nuestros modelos y compararlos con los modelos lineales simples creados anteriormente.\n\n\n\n\nAnteriormente, creamos una colección de modelos simples para ajustarse a la expectativa de vida usando la característica de año. Su análisis anterior mostro que algunos de estos modelos no encajaban muy bien.\nEn esta sección, construiremos modelos de regresión múltiple para cada país utilizando todas las funciones disponibles. Puede que le interese comparar el rendimiento de los 12 modelos con el peor ajuste\n\n\n\nPaís\nAdj.r.squared\n\n\n\n\nArmenia\n0.274831633\n\n\nBotswana\n-0.017346290\n\n\nCentral African Republic\n0.262392009\n\n\nLatvia\n0.177428933\n\n\nLesotho\n0.017078583\n\n\nLithuania\n0.281255888\n\n\nRussia\n0.008564872\n\n\nSouth Africa\n0.255968853\n\n\nSwaziland\n-0.018111402\n\n\nUkraine\n0.157855451\n\n\nZambia\n0.023859596\n\n\nZimbabwe\n0.1216212616\n\n\n\nAhora si, apliquemos un modelo lineal generalizado para ver si mejorar estos datos\n\n# Creamos un modelo lineal para cada país\ngap_fullmodel <- gap_anidado %>% \n  mutate(model = map(data, \n                     ~lm(life_expectancy~year+population+fertility+gdp, data = .x)))\n\nfullmodel_perf <- gap_fullmodel %>% \n  # Extraigaimos las estadísticas de ajuste de cada modelo en marcos de datos\n  mutate(fit = map(model, ~glance(.x))) %>% \n  # Simplifiquemos los marcos de datos de ajuste para cada modelo\n  unnest(fit)\n\n# Vea el rendimiento de los 12 países con el peor ajuste, es decir, \n# los dos modelos simples que viste antes\nfullmodel_perf %>% \n  filter(country %in% peores_modelos$country) %>% \n  select(country, adj.r.squared)\n\n# A tibble: 12 × 2\n# Groups:   country [12]\n   country                  adj.r.squared\n   <fct>                            <dbl>\n 1 Armenia                          0.923\n 2 Botswana                         0.736\n 3 Central African Republic         0.931\n 4 Latvia                           0.687\n 5 Lesotho                          0.855\n 6 Lithuania                        0.893\n 7 Russia                           0.652\n 8 South Africa                     0.896\n 9 Swaziland                        0.905\n10 Ukraine                          0.692\n11 Zambia                           0.872\n12 Zimbabwe                         0.978\n\n\nNote que los valores para adj.r.squared mejoraron considerablemente. Si bien adj.r.squared nos dice qué tan bien se ajusta el modelo a nuestros datos, no da ninguna indicación sobre cómo se desempeñaria con nuevos datos. En otro post, les mostraré como estimar el rendimiento del modelo utilizando los datos retenidos de la construcción del modelo."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Modelo Prophet de Facebook",
    "section": "",
    "text": "En este post, te presento la implementación de Facebook Prophet, así como sus principales hiperparámetros ajustados para generar el modelo predictivo, dicho modelo lo implementaremos para predecir el tipo de cambio de Honduras versus el dólar EE:UU (USD/HNL), considerando que esto será unicamente para conocer el funcionamiento del modelo, no para justificar que es el mejor modelo para realizar la actividad descrita previamente."
  },
  {
    "objectID": "posts/welcome/index.html#descripción-general-del-modelo",
    "href": "posts/welcome/index.html#descripción-general-del-modelo",
    "title": "Modelo Prophet de Facebook",
    "section": "Descripción General del Modelo",
    "text": "Descripción General del Modelo\nFacebook Prophet es un modelo y una biblioteca que proporciona características tanto de modelos lineales generalizados (MLG) como de modelos aditivos (MA), principalmente extendiendo el MLG mediante el uso de funciones de suavizado no lineal. Fue especificado por Taylor y Letham en 2017.\nProphet es un software de código abierto lanzado por el equipo Core Data Science de Facebook. Está disponible para su descarga en CRAN y PyPI. En esta ocasión usaremos el lenguaje R para implementar el modelo, sin embargo, tu puedes hacerlo en Python si es de tu preferencia.\nProphet funciona mejor con series temporales que tienen fuertes efectos estacionales y varias temporadas de datos históricos. Prophet es resistente a los datos faltantes y los cambios en la tendencia, y por lo general maneja bien los valores atípicos. Prophet esta diseñado especificamente para la predicción de series temporales de negocios.\nSu modelo aditivo que consta de cuatro componentes, esta dado por:\n\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_{t}\n\\]\ndonde,\n\n\\(g(t)\\): Representa la tendencia y el objetivo es capturar la tendencia de la serie. Por ejemplo, es probable que la cantidad de vistas de anuncios de Facebook aumente con el tiempo a medida que más personas se unen a la red. Pero, ¿cuál sería la función exacta del aumento?\n\\(s(t)\\): Es el componente de Estacionalidad. El número de anuncios también puede depender de la temporada. Por ejemplo, en el hemisferio norte durante los meses de verano, es probable que las personas pasen más tiempo al aire libre y menos tiempo frente a sus computadoras. Tales fluctuaciones pueden ser muy diferentes para diferentes series temporales de negocios. El segundo componente es, por lo tanto, una función que modela las tendencias estacionales.\n\\(h(t)\\): Representa los efectos de las vaciones. Usamos la información para días festivos que tienen claro impacto en la mayoria de las series temporales comerciales. Tenga en cuenta que las vaciones varían entre años, países, etc. Y, por lo tanto, la información debe proporcionarse explícitamente al modelo.\n\\(\\epsilon_{t}\\): Es el término de error. Representa fluctuaciones aleatorias que el modelo no puede explicar. Como de costumbre, se supone que \\(\\epsilon_{t}\\) sigue una distribución \\(N(0,1)\\) con media cero y varianza desconocida \\(\\sigma\\) que debe derivarse de los datos ."
  },
  {
    "objectID": "posts/welcome/index.html#hiperparámetros",
    "href": "posts/welcome/index.html#hiperparámetros",
    "title": "Modelo Prophet de Facebook",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\nHay varios parámetros personalizables en la implementación de Facebook Prophet (revisar), siendo los principales:\n\nPuntos de cambio: definen los cambios de tendencia. Estos pueden ser encontrados por el propio algoritmo o también pueden ser definidos y ajustados por el analista.\nEstacionalidad: define las funciones periódicas que pueden afectar a la serie temporal. De forma predeterminada, Prophet considera la estacionalidad anual, semanal y diaria e intenta encontrar tendencias que representan esos efectos periódicos en los datos.\nDías festivos: los días especiales (días festivos o cualquier otro evento recurrente) también pueden ser modelados por el modelo aditivo.\n\nEn R, se usa la API de ajuste de modelo normal. Proporcionamos una función prophet que realiza el ajuste y devuelve un objeto de modelo. Posteriormeente usted puede llamar a la función predict y plot en este objeto modelo."
  },
  {
    "objectID": "posts/welcome/index.html#datos-y-preparación",
    "href": "posts/welcome/index.html#datos-y-preparación",
    "title": "Modelo Prophet de Facebook",
    "section": "Datos y Preparación",
    "text": "Datos y Preparación\nLos datos que utilizaremos los encontramos en Yahoo! Finance. Así como Python tiene un paquete para importar datos directamente de Yahoo Finance, R también cuenta con sus paquetes particular que nos permiten realizar una tarea similar. Necesitamos los siguiente paquetes:\n\nlibrary(TTR)\n\nWarning: package 'TTR' was built under R version 4.2.3\n\nlibrary(quantmod)\n\nWarning: package 'quantmod' was built under R version 4.2.3\n\n\nLoading required package: xts\n\n\nWarning: package 'xts' was built under R version 4.2.3\n\n\nLoading required package: zoo\n\n\nWarning: package 'zoo' was built under R version 4.2.3\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nSi aún no los tienes instalados sugiero los instales usando install.packages(\"name paquete\"). Muy bien, ahora si estamos listos para poder extraer nuestros datos de yahoo finance y para ello usaremos la funcion getSymbols del paquete quantmod. Veamos,\n\ndf <- getSymbols('HNL=X',src = 'yahoo',\n                 from = \"2010-01-01\",\n                 to = \"2022-12-20\",\n                 auto.assign = FALSE)\n\nTenga en cuenta que from =  \"2010-01-01\" y to = \"2022-12-20\" nos ayudan a indicar desde que fecha quiero comenzar a tomar mis datos y hasta que fecha quiero tomarlos. Además, auto.assign = FALSE indica a getSymbols que devuelva los datos.\nAhora, conozcamos nuestros datos\n\nhead(df)\n\n           HNL=X.Open HNL=X.High HNL=X.Low HNL=X.Close HNL=X.Volume\n2010-01-04     18.690     18.691    18.517      18.518            0\n2010-01-05     18.550     18.550    18.550      18.550            0\n2010-01-06     18.572     18.645    18.544      18.545            0\n2010-01-07     18.451     18.550    18.451      18.539            0\n2010-01-08     18.556     18.556    18.556      18.556            0\n2010-01-11     18.550     18.550    18.550      18.550            0\n           HNL=X.Adjusted\n2010-01-04         18.518\n2010-01-05         18.550\n2010-01-06         18.545\n2010-01-07         18.539\n2010-01-08         18.556\n2010-01-11         18.550\n\n\nDe estos datos únicamente usaremos el valor de cierre (HNL=X.Close) de manera diaria del lempira hondureño contra el dólar, para enfocarnos solo en esos datos, primero convertiremos nuestro conjunto de datos df en un dataframe, dado que inicialmente es un objeto de tipo xts,\n\nclass(df)\n\n[1] \"xts\" \"zoo\"\n\n\npara realizar el cambio a un dataframe, considere la siguiente función\n\nxts_to_datframe<-function(data_xts){\n  df_t<-data.frame(fecha=(index(data_xts)),\n                   value=coredata(data_xts))\n  colnames(df_t)<-c(\"ds\", \"y\")\n  df_t\n}\n\nTiene que tener cuidado con el nombramiento de sus columnas, dado que prophet reconoce unicamente marcos de datos con columnas nombras como ds y y, qu contienen la fecha y el valor numérico de sus observaciones respectivamente. Con esto en mente, pasemos a transformar df a un objeto de clase dataframe por medio de la función que construimos previamente:\n\nHNL <- xts_to_datframe(df$`HNL=X.Close`) \nclass(HNL)\n\n[1] \"data.frame\"\n\n\nPuede apreciar que ya tenemos nuestro marco de datos como un dataframe, y estamos listos para comenzar a crear nuestro modelo."
  },
  {
    "objectID": "posts/welcome/index.html#implementación-del-modelo",
    "href": "posts/welcome/index.html#implementación-del-modelo",
    "title": "Modelo Prophet de Facebook",
    "section": "Implementación del Modelo",
    "text": "Implementación del Modelo\nPrimero visualicemos nuestros datos\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\nHNL %>% ggplot(aes(x = ds, y = y))+\n  geom_line()+\n  theme_minimal()+\n   labs(title = 'Datos Historicos del Tipo de Cambio del USD/HNL',\n       subtitle = '2010 - 2022',\n       x = 'Fecha',\n       y = 'HNL',\n       caption = 'Elaboracion propia con datos de yahoo finance')\n\n\n\n\n\nlibrary(prophet)\n\nLoading required package: Rcpp\n\n\nLoading required package: rlang\n\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\nm <- prophet(HNL,daily.seasonality = TRUE)\n\nfuture <- make_future_dataframe(m,periods = 3,freq = 'day')\ntail(future)\n\n             ds\n3380 2022-12-16\n3381 2022-12-19\n3382 2022-12-20\n3383 2022-12-21\n3384 2022-12-22\n3385 2022-12-23\n\n\n\nforecast <- predict(m, future)\n\ndyplot.prophet(m, forecast)\n\n\n\n\n\nDe la figura previa,\n\nLos puntos negros representan medidas reales\nLa linea azul el pronóstico de Prophet\nLa banda azul representa el intervalo de incertidumbre"
  },
  {
    "objectID": "posts/welcome/index.html#desglose-del-pronóstico",
    "href": "posts/welcome/index.html#desglose-del-pronóstico",
    "title": "Modelo Prophet de Facebook",
    "section": "Desglose del Pronóstico",
    "text": "Desglose del Pronóstico\nSi bien el pronóstico arroja muchas cosas, podemos centrarnos en algunas como:\n\nds fecha que se pronostica\nyhat predicción para el valor y (tipo de cambio) ese día en particular.\nyhat_lower valor esperado más bajo para el rango del valor y previsto ese día\nyhat_upper valor esperado más alto para el rango de valor y previsto de ese día\n\nCon tail() podemos ver la salida de los últimos días pronosticados los cuales son 21, 22 y 23 de diciembre 2022.\n\ntail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])\n\n             ds     yhat yhat_lower yhat_upper\n3380 2022-12-16 24.11296   23.36023   24.84664\n3381 2022-12-19 24.06289   23.38277   24.78892\n3382 2022-12-20 24.07617   23.36034   24.81428\n3383 2022-12-21 24.11976   23.36930   24.88180\n3384 2022-12-22 24.12084   23.42593   24.86375\n3385 2022-12-23 24.14074   23.39429   24.83643\n\n\nSegun nuestros resultados, nuestro modelo nos ve obteniendo para el día 21 de diciembre entre 23.35901 (yhat_lower) y 24.83438 (yhat_upper) lempiras por un dolar de EE.UU.\nPara entender el pronóstico más a detalle, podemos gráficar sus componentes con:\n\nprophet_plot_components(m,forecast)\n\n\n\n\nRecuerde que el fin de este post, no es abogar por el uso indiscriminado de Prophet como el mejor modelo para pronosticar el tipo de cambio hondureño vs el dólar. Espero hayas conocido las generalidades de este modelo y su utilidad en el ambito predictivo."
  },
  {
    "objectID": "software_citations.html",
    "href": "software_citations.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "On this page I collect citations to software I maintain or am an author of. Software citations are particularly poorly captured by things like Google Scholar, both because software exists outside of standard citation formats and because software citations are often poorly formatted in publications. For that reason, I find it useful to keep my own list of projects which have cited my work.\nIf you cited any of my packages in a publication, please let me know – either on Mastodon or via email at mike.mahoney.218+site@gmail.com . I can’t guarantee I’ll add them to this list, for secret reasons, but it always makes me happy to see."
  },
  {
    "objectID": "software_citations.html#how-to-cite-software",
    "href": "software_citations.html#how-to-cite-software",
    "title": "Juan Isaula",
    "section": "How to cite software",
    "text": "How to cite software\nThe rOpenSci blog has a fantastic post on how to cite R and R packages, which I quickly summarize below. I highly recommend their post, however, even if you aren’t an R user; most of the topic generalizes to other software pretty easily.\nA software citation serves two functions for your readers:\n\nTells them what you did; for instance, if I see someone cite the sf package, I suddenly know more about the details of their spatial calculations than I can probably get from the text alone.\nTells them how you did it; package version information and source information helps others to replicate your work, and also can help readers assess an analysis. If you use sf >= 1.0.0, for instance, I know that you probably didn’t run into issues doing distance calculations with geographic coordinates, so I don’t need to consider that when I’m reading your results.\n\nSoftware citations also help the developers of your packages justify developing packages further; if you’re a user of scientific software and want there to still be scientific software in the future, you should cite your software. It also makes me, personally, feel great to see people benefiting from my work, and to see them credit me for any help my software gave them!\nAs such, you should cite any package that was relevant to the study you performed. If uninstalling the package (or deleting the relevant function calls) would change your results or make your code not run, you should cite it.\nIf you are an R user, you can get the citations for both R and your packages using the citation() function:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation(\"rsample\")\n\n\nTo cite package 'rsample' in publications use:\n\n  Frick H, Chow F, Kuhn M, Mahoney M, Silge J, Wickham H (2022).\n  _rsample: General Resampling Infrastructure_. R package version\n  1.1.1, <https://CRAN.R-project.org/package=rsample>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {rsample: General Resampling Infrastructure},\n    author = {Hannah Frick and Fanny Chow and Max Kuhn and Michael Mahoney and Julia Silge and Hadley Wickham},\n    year = {2022},\n    note = {R package version 1.1.1},\n    url = {https://CRAN.R-project.org/package=rsample},\n  }\n\n\nIf you aren’t an R user, many scientific software libraries will still provide information on how to cite them – for instance, here’s the instructions for citing numpy. If Google and such return nothing, then I think your best option is to style citations after the official R citation: mimic book citation style, using authors, year, title, publication location, and an accessible URL."
  },
  {
    "objectID": "software_citations.html#how-i-track-citations",
    "href": "software_citations.html#how-i-track-citations",
    "title": "Juan Isaula",
    "section": "How I track citations",
    "text": "How I track citations\nAs suggested in a different rOpenSci post, I find the best general tool I have for finding software citations is to set up a Google Scholar alert for my packages and investigate each alert as it comes in. This is not a foolproof method, and I’m sure I miss things; as mentioned above, if you cited any of my packages in a publication, please let me know either on Mastodon or via email at mike.mahoney.218+site@gmail.com .\nI only track citations for packages I’m an author on, once I’m an author on them. In cases where a paper mentions the package but doesn’t cite it, I only list the article if it was published after I became an author on the package.\nWhile I track both citations and mentions of my packages, I really want to stress that citations are fantastic, help me justify maintaining open source scientific software (both to my boss and to myself), and make me really happy. Mentions do not. If you are going to mention one of these packages, please give it a full citation.\nThe citations below are in a variety of formats; I’ll confess to copying journal “cite this article” citations without reformatting for this list. If I messed up my citation for your article, please get in touch to let me know."
  },
  {
    "objectID": "software_citations.html#software-citations-1",
    "href": "software_citations.html#software-citations-1",
    "title": "Juan Isaula",
    "section": "Software citations",
    "text": "Software citations\n\nterrainr\n\nFox, N., Serrano-Vergel, R., Van Berkel, D., Lindquist, M. 2022. Towards Gamified Decision Support Systems: In-game 3D Representation of Real-word Landscapes from GIS Datasets. Journal of Digital Landscape Architecture. https://doi.org/10.14627/537724035\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L. 2022. Mapping Two Decades of New York State Forest Aboveground Biomass Change Using Remote Sensing. Remote Sensing 14(16): 4097. https://doi.org/10.3390/rs14164097\n\nThe following are self-citations:\n\nJohnson, L. K., Mahoney, M. J., Desrochers, M. L., & Beier, C. M. 2023. Mapping historical forest biomass for stock-change assessments at parcel to landscape scales. arXiv:2304.02632. https://doi.org/10.48550/arXiv.2304.02632\nJohnson, L. K., Mahoney, M. J., Bevilacqua, E., Stehman, S. V., Domke, G. M., & Beier, C. M. (2022). Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages. International Journal of Applied Earth Observation and Geoinformation, 114, 103059. https://doi.org/10.1016/j.jag.2022.103059\nMahoney, M. J., Johnson, L. K., Guinan, A. Z., & Beier, C. M. (2022). Classification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. International Journal of Remote Sensing, 43(19-24), 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nMahoney et al., (2022). unifir: A Unifying API for Working with Unity in R. Journal of Open Source Software, 7(73), 4388, https://doi.org/10.21105/joss.04388\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L., Phoenix, D. B., and Mahoney, M. 2022. Decision tree-based machine learning models for above-ground biomass estimation using multi-source remote sensing data and object-based image analysis. Geocarto International 37(26): 12763-12791. https://doi.org/10.1080/10106049.2022.2071475\n\n\ncitation(\"terrainr\")\n\n\nThe United States Geological Survey provides guidelines for citing USGS\ndata products (as downloaded from 'get_tiles') at:\nhttps://www.usgs.gov/faqs/how-should-i-cite-datasets-and-services-national-map\n\nTo cite terrainr in publications please use:\n\n  Mahoney M. J., Beier C. M., and Ackerman, A. C. (2022). terrainr: An\n  R package for creating immersive virtual environments. Journal of\n  Open Source Software, 7(69), 4060,\n  https://doi.org/10.21105/joss.04060\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {69},\n    pages = {4060},\n    author = {Michael J. Mahoney and Colin M. Beier and Aidan C. Ackerman},\n    title = {{terrainr}: An R package for creating immersive virtual environments},\n    journal = {Journal of Open Source Software},\n    doi = {10.21105/joss.04060},\n    url = {https://doi.org/10.21105/joss.04060},\n  }\n\n\n\n\nunifir\n\ncitation(\"unifir\")\n\n\nTo cite unifir in publications please use:\n\n  Mahoney M. J., Beier C. M., and Ackerman, A. C. (2022). unifir: A\n  Unifying API for Working with Unity in R. Journal of Open Source\n  Software, 7(73), 4388, https://doi.org/10.21105/joss.04388\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {73},\n    pages = {4388},\n    author = {Michael J. Mahoney and Colin M. Beier and Aidan C. Ackerman},\n    title = {{unifir:} A Unifying {API} for Working with {Unity} in {R}},\n    journal = {Journal of Open Source Software},\n    doi = {10.21105/joss.04388},\n    url = {https://doi.org/10.21105/joss.04388},\n  }\n\n\n\n\nwaywiser\n\ncitation(\"waywiser\")\n\n\nTo cite waywiser in publications please use:\n\n  Mahoney M. J. (2023). waywiser: Ergonomic Methods for Assessing\n  Spatial Models. arXiv:2303.11312 [cs.MS].\n  https://doi.org/10.48550/arXiv.2303.11312\n\nA BibTeX entry for LaTeX users is\n\n  @Misc{,\n    title = {waywiser: Ergonomic Methods for Assessing Spatial Models},\n    author = {Michael J Mahoney},\n    year = {2023},\n    eprint = {2303.11312},\n    archiveprefix = {arXiv},\n    primaryclass = {cs.MS},\n    doi = {10.48550/arXiv.2303.11312},\n    url = {https://arxiv.org/abs/2303.11312},\n  }\n\n\n\n\nspatialsample\nI took over maintenance of spatialsample in mid-2022, after spending the summer interning with the tidymodels crew at Posit working on spatialsample and rsample. I’m responsible for the majority of package functionality. I only track citations here for versions of the package I’m an author on (>= 0.2.0).\n\nPebesma, E., Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016\nde Lara, A., Mieno, T., Luck, J.D. et al. Predicting site-specific economic optimal nitrogen rate using machine learning methods and on-farm precision experimentation. Precision Agric (2023). https://doi.org/10.1007/s11119-023-10018-8\nTutland, Niko J., Rodman, Kyle C., Andrus, Robert A., and Hart, Sarah J. 2023. “Overlapping Outbreaks of Multiple Bark Beetle Species are Rarely More Severe than Single-Species Outbreaks.” Ecosphere 14(3): e4478. https://doi.org/10.1002/ecs2.4478\nSkinner, E.B., Glidden, C.K., MacDonald, A.J. et al. Human footprint is associated with shifts in the assemblages of major vector-borne diseases. Nat Sustain (2023). https://doi.org/10.1038/s41893-023-01080-1\n\nThe following are self-citations:\n\nMahoney, MJ In Review. waywiser: Ergonomic methods for assessing spatial models. https://arxiv.org/abs/2303.11312\n\n\ncitation(\"spatialsample\")\n\n\nTo cite spatialsample in publications please use:\n\n  Mahoney M. J., Johnson, L. K., Silge, J., Frick, H., Kuhn, M., and\n  Beier C. M. (2023). Assessing the performance of spatial\n  cross-validation approaches for models of spatially structured data.\n  arXiv. https://doi.org/10.48550/arXiv.2303.07334\n\nA BibTeX entry for LaTeX users is\n\n  @Misc{,\n    title = {Assessing the performance of spatial cross-validation approaches for models of spatially structured data},\n    author = {Michael J Mahoney and Lucas K Johnson and Julia Silge and Hannah Frick and Max Kuhn and Colin M Beier},\n    year = {2023},\n    eprint = {2303.07334},\n    archiveprefix = {arXiv},\n    primaryclass = {stat.CO},\n    doi = {10.48550/arXiv.2303.07334},\n    url = {https://arxiv.org/abs/2303.07334},\n  }\n\n\n\n\nrsample\nI am an author on rsample after implementing functions for grouped resampling and clustered cross-validation (as well as the “common resampling patterns” vignette and a few other improvements). I only track citations here for versions of the package I’m an author on (>= 1.1.1).\n\nLundell, J. 2023. EZtune: A Package for Automated Hyperparameter Tuning in R. https://arxiv.org/abs/2303.12177\nYing R., Monteiro, F. M., Wilson, J. D., and Schmidt, D. N. 2023. ForamEcoGEnIE 2.0: incorporating symbiosis and spine traits into a trait-based global planktic foraminiferal model. Geosci. Model Dev., 16, 813–832, https://doi.org/10.5194/gmd-16-813-2023\nLyu H, Grafton M, Ramilan T, Irwin M, Sandoval E. Assessing the Leaf Blade Nutrient Status of Pinot Noir Using Hyperspectral Reflectance and Machine Learning Models. Remote Sensing. 2023; 15(6):1497. https://doi.org/10.3390/rs15061497\n\nThe following are self-citations:\n\nMahoney, MJ, Johnson, L. K., Silge, J., Frick, H., Kuhn, M., and Beier, C. M. In Review. Assessing the performance of spatial cross-validation approaches for models of spatially structured data. https://arxiv.org/abs/2303.07334\nMahoney, MJ In Review. waywiser: Ergonomic methods for assessing spatial models. https://arxiv.org/abs/2303.11312\n\nrsample is used in (but not cited in) Wang et al 2023; Lienard et al 2022; Adu-Oppong et al 2022; Zappaterra et al 2022; Mull et al 2022; Dematheis et al 2022; Bellows et al 2022; Yates et al; Stevelink et al 2022; Bartz-Beielstein et al 2023; Han Nh 2022; Yan et al 2023; dos Santos et al 2022; Badonyi et al 2022; Badonyi et al 2022; Peguero et al 2023; Howard et al 2023.\n\ncitation(\"rsample\")\n\n\nTo cite package 'rsample' in publications use:\n\n  Frick H, Chow F, Kuhn M, Mahoney M, Silge J, Wickham H (2022).\n  _rsample: General Resampling Infrastructure_. R package version\n  1.1.1, <https://CRAN.R-project.org/package=rsample>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {rsample: General Resampling Infrastructure},\n    author = {Hannah Frick and Fanny Chow and Max Kuhn and Michael Mahoney and Julia Silge and Hadley Wickham},\n    year = {2022},\n    note = {R package version 1.1.1},\n    url = {https://CRAN.R-project.org/package=rsample},\n  }\n\n\n\n\ngeojsonio\nI took over maintaining geojsonio in mid-2022, in order to keep the package on CRAN. Most functionality was either contributed by Scott and Andy or by community contributors; I primarily fix issues to keep the package on CRAN and shepherd community contributions prior to merging them into the project. As with rsample, I only track citations since I joined the project.\ngeojsonio is used in (but not cited in) Chauhan et al 2022; Wang 2023; de Souza et al 2022.\n\ncitation(\"geojsonio\")\n\n\nTo cite package 'geojsonio' in publications use:\n\n  Chamberlain S, Teucher A, Mahoney M (2023). _geojsonio: Convert Data\n  from and to 'GeoJSON' or 'TopoJSON'_. R package version 0.11.1,\n  <https://CRAN.R-project.org/package=geojsonio>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {geojsonio: Convert Data from and to 'GeoJSON' or 'TopoJSON'},\n    author = {Scott Chamberlain and Andy Teucher and Michael Mahoney},\n    year = {2023},\n    note = {R package version 0.11.1},\n    url = {https://CRAN.R-project.org/package=geojsonio},\n  }"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Papers\n\n\n\n\nPapers\n\nHaga click en “[PDF]” para descargar el documento.\n\n\n2017\n\n\nBruce E. Hansen: Time series econometrics for the 21st century https://doi.org/10.48550/arXiv.2304.02632 [PDF]\n\n\n2020\n\n\nJuan, I. M: Modelos de tipo caja negra para estudio de series temporales Financieras. [PDF]"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "courses",
    "section": "",
    "text": "Saludos! Bienvenido a esta sección de mi website. Aquí podra encontrar información sobre algnos cursos en los cuales soy colaborador.\n\n2023\n\n\nEstadística Aplicada - Maestría UNAH\n\nEste apartado está dedicado a los estudiantes de la maestría en Formulación, Gestión y Evaluación de Proyectos de la UNAH. Donde podrán encontrar el contenido sobre el taller de fundamentos estadísticos utilizando el Software de R. Mismo que tuve la oportunidad de impartirles.\n\nFundamentos estadísticos utilizando el Software de R\nLibro sobre fundamentos estadísticos con R - Juan Isaula\n\nBases de datos utilizadas\n\nco2.csv\nAAPL.csv\n\n\n\n\nMicroeconomía II - UNAH\n\n\nPrograma del curso\n\n\nModulo I - Teoria del Consumidor\n\n\nkaggle - Comandos Generales Python\nMaterial de repaso - Matemáticas de la optimización\nAula virtual Classroom\n\n\nModulo II - Teoria del Productor\n\n\nElasticidad Sustitución, Función de costos y matriz sustitución\n\n\nVideos\n\n\nComandos Generales Python\n\n\nModulo III - Estructuras de Mercado\n\n\nEjercicios equilibro a corto plazo - Competencia Perfecta\nCompetencia Perfecta: Largo y Corto Plazo\nPresentaciones sobre Monopolios\n\n\nAsignaciones\n\n\nTarea I\nTarea 3 - Teoría del Productor\nTarea Final - Estructuras de Mercado utilizando Python\n\n\nBibliografía Recomendada\n\n\nAdvanced Microeconomic Theory, Geoffrey A. Jehle Philip J.Reny\nTeoría Microeconómica (Principios básicos y ampliaciones-Walter Nikolso)\nMicroeconomia II - UNAH - Juan Isaula"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Subscribe to new posts\n\nGet new content in a weekly email"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "App/Dashboards",
    "section": "",
    "text": "En este sitio encontraras algunos Dashboards que he realizado en lenguajes de programación R y Python para algunas instituciones del País (Honduras) .\n\nGeoRGe\nEs una App que elaboré en el lenguaje de programación R, utilizando principalmente los paquetes shiny | shinydashboard | DT | tidyverse | shinyWidgets | fresh. En la actualizad es utilizada en el Instituto Hondureño de Seguridad Social (IHSS).\nIA-JIM\nApp que ayuda con el cómputo de gran parte de los indicadores actuariales primarios y secundarios del IHSS, correspondientes al Régimen del Seguro de Previsión Social (RSPS)."
  },
  {
    "objectID": "posts/port_mg/estructuras_mercado/index.html",
    "href": "posts/port_mg/estructuras_mercado/index.html",
    "title": "Estructuras de Mercado con Python",
    "section": "",
    "text": "Este post tiene como objetivo dar a conocer la importancia del software de Python en el ambito microeconomico, particularmente en este caso hablamos de las diferentes estructuras de mercado; competencia perfecta, monopolio y oligopolio."
  },
  {
    "objectID": "posts/port_mg/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "href": "posts/port_mg/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "title": "Estructuras de Mercado con Python",
    "section": "Condiciones necesaria para la competencia perfecta",
    "text": "Condiciones necesaria para la competencia perfecta\n\nMuchos productores, ninguno de los cuales tiene una gran cuota de mercado.\nUna industria puede ser perfectamente competitiva sólo si los consumidores consideran como equivalentes a los productos de todos los productores (producto homogéneo)"
  },
  {
    "objectID": "posts/port_mg/estructuras_mercado/index.html#libre-entrada-y-salida",
    "href": "posts/port_mg/estructuras_mercado/index.html#libre-entrada-y-salida",
    "title": "Estructuras de Mercado con Python",
    "section": "Libre entrada y salida",
    "text": "Libre entrada y salida\nExiste libre entrada y salida en una industria cuando nuevos productores pueden entrar facilmente en esa industria a los que ya estan en ella pueden abondonarla sin coste alguno.\n\nRegla de Producción Optima\nLa regla de producción optima dice que el beneficio se maximiza cuando se produce la cantidad de output para la cual el ingreso marginal de la última unidad de output producida es igual a su coste marginal.\n\\[\nIMg = CMg\n\\]\n\n\nFunción de Benenficios\nLa función de beneficios \\((\\pi)\\) representa las diferencias entre los costos totales, \\(C(Q)\\) e ingresos totales,\\(R(Q)\\) , de las empresas\n\\[\n\\pi = R(Q) - C(Q)\n\\]\n\n\nTomador de Precios\nPrecio igual al costo marginal\n\\[\n\\begin{eqnarray*}\nCMg = IMg = P\n\\end{eqnarray*}\n\\]\nPor tanto, se dice que el beneficio de una empresa precio-aceptante se maximiza produciendo la cantidad de output para la cual el costo marginal de la última unidad producida es igual al precio de mercado, tal como se aprecia en el siguiente gráfico\n\n\n\nCantidad de producto que maximiza el beneficio de una empresa precio-aceptante"
  },
  {
    "objectID": "posts/port_mg/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "href": "posts/port_mg/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Costes y Producción en el Corto Plazo",
    "text": "Costes y Producción en el Corto Plazo\nEn el corto plazo tenemos las siguientes condiciones de producción de empresas competitivas\n\n\n\n\n\n\n\nCondiciones\nResultados\n\n\n\n\nP > CVMe mínimo\nLa empresa produce en el corto plazo. Si P < CTMe mínimo, la empresa cubre sus costos variables y parte de sus costes fijos pero no todos. Si P > CTMe mínimo, la empresa cubre todos sus costes variables y sus costes fijos.\n\n\nP = CVMe mínimo\nLa empresa es indiferente entre producir en el corto plazo o no producir. Cubre exactamente sus costes variables.\n\n\nP < CVMe mínimo\nLa empresa cierra en el corto plazo. No cubre sus costes variables."
  },
  {
    "objectID": "posts/port_mg/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "href": "posts/port_mg/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Ejemplo # 1- Corto Plazo",
    "text": "Ejemplo # 1- Corto Plazo\nPrimero resolveremos el siguiente ejercicio de manera manual y posteriormente lo resolveremos en Python.\nSuponga que la empresa tiene una curva de costos de corto plazo dada por\n\\[\nC(Q) = 100 + 20Q + Q^2\n\\]\n\n¿Cuál es la ecuación para el costo variable Medio?\n¿Cuál es el valor mínimo para el costo variable promedio?\n¿Cuál es la curva de oferta de corto plazo?\n\nSolución\n\nDada la función de costo \\(C(Q) = 100 + 20Q + Q^2\\) es claro que el costo variable, CV, esta dado por \\[CV = 20Q + Q^2\\] por tanto su costo variable promedio es \\[CVMe = \\frac{CV}{Q} = 20 + Q\\]\nAhora bien, su costo marginal sabemos que unicamente requiere aplicar la regla de diferenciación, ya que \\[CMg = \\frac{\\partial C(Q)}{\\partial Q} = 20 + 2Q\\]\nSi queremos encontrar el costo variable promedio mínimo, \\[CVMe_{\\min}\\], se obtiene como \\[CMg = CVMe \\longrightarrow Q = \\fbox{0}\\]\nEntonces la función de oferta es: \\[\\begin{eqnarray*}CMg &=& p\\\\[0.2cm] 20 + 2Q &=& P\\\\[0.2cm] Q(P) &=& \\frac{P}{2} - 10 \\end{eqnarray*}\\]\n\nPor tanto, también podemos obtener el precio de equilibrio, ya que \\[0 = \\frac{P}{2} - 10 \\longrightarrow P = \\fbox{20}\\]\nAhora, encontremos estos resultados en Python:\n\n# Paquete previo \nfrom sympy import *\nQ = symbols(\"Q\")\n\n\n# función de costo de corto plazo \nCT = 100 + 20*Q + Q**2\n# costo variale promedio \nCV = 20 + Q \n# Encontrar el costo variable minimo \n# Primero: costo marginal\n\nCM = diff(CT,Q)\n\n\n# igualar costo marginal y costo variable promedio \nsolve(Eq(CM,CV))\n\n[0]\n\n\n\ncantidad = solve(Eq(CM,CV))\ncantidad[0]\n\n\\(\\displaystyle 0\\)\n\n\n\nP = CV.subs({Q:cantidad[0]})\nP\n\n\\(\\displaystyle 20\\)\n\n\n\nplot(CT, CT/Q, CV, CM, (Q,0,100), xlim = (0, 100), ylim = (0,100), xlabel = \"Q\", ylabel = \"P\")\n\n\n\n\n<sympy.plotting.plot.Plot at 0x27e122c6aa0>\n\n\nPuedes notar lo rápido y fácil que resulta realizar estos procedimientos con Python y la utilidad que puede brindarte en caso de que trabajes con volumnes de datos.\n\nEjemplo # 2 - Corto Plazo\nAhora suponga que la empresa tiene una curva costos en el corto plazo de la siguiente forma:\n\\[\nC(Q) = 1 + 10Q + Q^2\n\\]\nSi la empresa opera en un mercado perfectamente competitivo, donde \\(P = 12\\), ¿Cuál será los beneficios de la empresa en el corto plazo?\nSolución\nSabemos que la función de beneficios esta dada por\n\\[\n\\pi = R - C\n\\]\nentonces,\n\\[\n\\frac{\\partial \\pi}{\\partial Q} = IMg - CMg = 0\n\\]\nasí pues,\n\\[\nCMg = 10 + 2Q \\hspace{1cm}y\\hspace{1cm} IMg = P\n\\]\npor tanto,\n\\[\n\\begin{eqnarray*}\nCMg &=& IMg\\\\[0.2cm]\n10 + 2Q &=& P\\\\[0.2cm]\nQ &=& \\frac{P}{2} - 5\\\\[0.2cm]\nQ &=& \\frac{12}{2} - 5, \\hspace{2cm}\\mbox{ya que P = 12}\\\\[0.2cm]\nQ &=& \\fbox{1}\n\\end{eqnarray*}\n\\]\nentonces,\n\\[\n\\pi = 12 - (1 + 10 +1) = \\fbox{0}\n\\]\nAhora veamos esta solución en Python:\n\n# Función de costos a corto plazo \nQ = symbols(\"Q\")\nCT = Q**2 + 10*Q + 1\nP = 12\nR = P*Q\n# costo marginal\nCM = diff(CT,Q)\nCM\nIM = diff(R,Q)\nIM\ncantidad = solve(Eq(IM,CM))\nprint(\"El valor de la producción que garantiza un equilibrio será:\", cantidad[0])\n\nEl valor de la producción que garantiza un equilibrio será: 1\n\n\nEste resultado lo que nos dice es que la empresa oferta una unidad de producción \\(Q = 1\\).\n\n# Beneficio = IT - CT\ncosto = CT.subs({Q:cantidad[0]})\ncosto\n\n\\(\\displaystyle 12\\)\n\n\n\ningreso = R.subs({Q:cantidad[0]})\ningreso \n\n\\(\\displaystyle 12\\)\n\n\n\nBeneficios = R - CT\npi = Beneficios.subs({Q:cantidad[0]})\npi\n\n\\(\\displaystyle 0\\)\n\n\n\nplot(CT,CM,CT/Q,(Q,0,60), xlim=(0,5), ylim=(0,30), xlabel='Q', ylabel='CT,CM')\n\n\n\n\n<sympy.plotting.plot.Plot at 0x27e0c90e830>\n\n\nRecuerde que todo este análisis se realizo para un mercado en competencia perfecta a corto plazo.\nPronto actualizare para el mercado en competencia perfecta a largo plazo, monopolio, e introducirnos un poco a la teoria de juegos."
  },
  {
    "objectID": "posts/RN/index.html",
    "href": "posts/RN/index.html",
    "title": "Redes Neuronales Recurrentes (LSTM)",
    "section": "",
    "text": "Las redes neuronales recurrentes son una arquitectura bastante empleada puesto que emplean los output de salida para retroalimentarse y continuar prediciendo un output.\n\n\n\n\nPrincipalmente sirve para contextos donde el orden importa y sirve como predictor recurrente (series de tiempo, textos, audios)\nSin embargo, ¿que tanto recordar el pasado? ¿cuál es la consecuencia en el rendimiento y demora de estimación?\nSus tipos más utilizados son LSTM y GRU\n\nEn este post nos enfocaremos en utilizar, describir y simular la red LSTM.\n\n\n\n\nPredictor de la siguiente palabra\nSeries de tiempo\nNo recomendable para transversal"
  },
  {
    "objectID": "posts/RN/index.html#red-de-memoria-de-corto-y-largo-plazo-lstm",
    "href": "posts/RN/index.html#red-de-memoria-de-corto-y-largo-plazo-lstm",
    "title": "Redes Neuronales Recurrentes (LSTM)",
    "section": "Red de Memoria de Corto y Largo Plazo (LSTM)",
    "text": "Red de Memoria de Corto y Largo Plazo (LSTM)\nLas redes LSTM (Long Short-Term Memory) son un tipo especial de redes neuronales recurrentes diseñadas con celdas de memoria que mantienen su estado a largo plazo. El principal objetivo de este tipo de redes es la solución del desvanecimiento del gradiente experimentado en las redes recurrentes. Globalmente, el flujo computacional de LSTM se ve de la siguiente manera:\n\n\n\nFlujo computacional de LSTM\n\n\nLas redes neuronales recurrentes pasan solo un estado oculto \\(h_t\\) a través de cada iteración. Pero LSTM pasa dos vectores: \\(h_t-\\)estado oculto (memoria a corto plazo) y \\(c_t-\\)estado celular (memoria a largo plazo).\nLas salidas de la celda LSTM se calculan a traves de las fórmulas que se muestran a continuación:\n\\[\\begin{eqnarray}\ni_t &=& \\sigma(w_{ii}x_t + b_ii + w_{hi}h_{(t-1)} + b_{hi})\\\\[0.2cm]\nf_t &=& \\sigma(w_{if}x_t + b_{if} + w_{hj}h_{(t-1)} + b_{hf})\\\\[0.2cm]\ng_t &=& \\tanh(w_{ig}x_t + b_{ig} + w_{hg}h_{(t-1)} + b_{hn})\\\\[0.2cm]\no_t &=& \\sigma(w_{io}x_t + b_{io} + w_{ho}h_{(t-1)} + b_{ho})\\\\[0.2cm]\nc_t &=& f_t \\circ c_{t-1} + i_t\\circ g_t\\\\[0.2cm]\nh_t &=& o_t \\circ \\tanh(c_t)\n\\end{eqnarray}\\]\ndonde:\n\n\\(\\sigma\\) es la función sigmoidea\n\\(\\circ\\) es el producto de Hadamard, que es:\n\\[\n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\na_3\n\\end{bmatrix} \\circ\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3\n\\end{bmatrix} = \\begin{bmatrix}\na_1b_1\\\\\na_2b_2\\\\\na_3b_3\n\\end{bmatrix}\n\\]\n\n\nVariables\n\n\\(i_t\\) (puerta de entrada) es la variable que se utiliza para actualizar el estado de la celda \\(c_t\\). El estado previamente oculto \\(c_t\\) y la entrada secuencial actual \\(x_t\\) se dan como entrada a una función sigmoidea. Si la salida está cerca a uno, más importante es la información.\n\\(f_t\\) (puerta de olvido) es la variable que decide que información debe olvidarse en el estado de celda \\(c_t\\). El estadp previamente oculto \\(h_t\\) y la entrada de secuencia \\(x_t\\) se dan como entradas a una función sigmoidea. Si la salida \\(f_t\\) está cerca de cero, entonces la información puede olvidarse, mientras que si la salida esta cerca de uno, la información debe almacenarse.\n\\(g_t\\) representa información importante potencialmente nueva para el estado celular \\(c_t\\).\n\\(c_t\\) (estado celular) es una suma de:\n\nEstado de celada anterior \\(c_{t-1}\\) con alguna información olvidada \\(f_t\\).\nNueva información de \\(g_t\\).\n\n\n\n\n\\(o_t\\) (puerta de salida) es la variable para actualizar el estado oculto \\(h_t\\).\n\\(h_t\\) (estado oculto) es el siguiente estado oculto que se calcula seleccionando la información importante \\(o_t\\) del estado de celda \\(c_t\\).\n\nLa siguiente figura muestra el gráfico computacional de la celda LSTM:\n\n\n\nGráfico computacional de LSTM\n\n\nLa red LSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n\\(w_{ii}, w_{hi}, w_{if}, w_{hf}, w_{ig}, w_{hg}, w_{io}, w_{ho}\\) - Pesos\n\\(b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{io}, b_{ho}\\) - Sesgos\n\nLos modelos LSTM son lo suficientemente potentes como para aprender los comportamientos pasados más importantes y comprender si esos comportamientos pasados son características importantes para hacer predicciones futuras. Hay varias aplicaciones en las que las LSTM se utilizan mucho. Aplicaciones como reconocimiento de voz, composición musical, reconocimiento de escritura a mano.\nParticularmente considero que las LSTM es como un modelo que tiene su propia memoria y que puede comportarse como un humano inteligente en la toma de desiciones."
  },
  {
    "objectID": "posts/RN/index.html#simulación-de-pronósticon-usando-lstm-con-python",
    "href": "posts/RN/index.html#simulación-de-pronósticon-usando-lstm-con-python",
    "title": "Redes Neuronales Recurrentes (LSTM)",
    "section": "Simulación de Pronósticon usando LSTM con Python",
    "text": "Simulación de Pronósticon usando LSTM con Python\n\n# Librerías \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\n\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_19320\\3683430178.py:5: FutureWarning:\n\nThe pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n\n\n\n\n# Cargamos nuestro Dataset\ndf = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n\n\n# Graficamos el precio de cierre (Close) de AAPL\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.legend(loc='best')\n\n<matplotlib.legend.Legend at 0x2708c43af50>\n\n\n\n\n\n\n# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual\n# se estara trabajando el resto del proyecto\ndata_df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      open\n      close\n      low\n      high\n      volume\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-01-05\n      WLTW\n      123.430000\n      125.839996\n      122.309998\n      126.250000\n      2163600.0\n    \n    \n      2016-01-06\n      WLTW\n      125.239998\n      119.980003\n      119.940002\n      125.540001\n      2386400.0\n    \n    \n      2016-01-07\n      WLTW\n      116.379997\n      114.949997\n      114.930000\n      119.739998\n      2489500.0\n    \n    \n      2016-01-08\n      WLTW\n      115.480003\n      116.620003\n      113.500000\n      117.440002\n      2006300.0\n    \n    \n      2016-01-11\n      WLTW\n      117.010002\n      114.970001\n      114.089996\n      117.330002\n      1408600.0\n    \n  \n\n\n\n\n\ndata_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()\n\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_19320\\1763401402.py:2: FutureWarning:\n\nIn a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n\n\n\n\n\n\n\n  \n    \n      \n      open\n      close\n      low\n      high\n      volume\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      30.490000\n      30.572857\n      30.340000\n      30.642857\n      123432400.0\n    \n    \n      2010-01-05\n      30.657143\n      30.625713\n      30.464285\n      30.798571\n      150476200.0\n    \n    \n      2010-01-06\n      30.625713\n      30.138571\n      30.107143\n      30.747143\n      138040000.0\n    \n    \n      2010-01-07\n      30.250000\n      30.082857\n      29.864286\n      30.285715\n      119282800.0\n    \n    \n      2010-01-08\n      30.042856\n      30.282858\n      29.865715\n      30.285715\n      111902700.0\n    \n  \n\n\n\n\n\ndata_df['date'] = data_df.index\ndata_df.head()\n\n\n\n\n\n  \n    \n      \n      open\n      close\n      low\n      high\n      volume\n      date\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      30.490000\n      30.572857\n      30.340000\n      30.642857\n      123432400.0\n      2010-01-04\n    \n    \n      2010-01-05\n      30.657143\n      30.625713\n      30.464285\n      30.798571\n      150476200.0\n      2010-01-05\n    \n    \n      2010-01-06\n      30.625713\n      30.138571\n      30.107143\n      30.747143\n      138040000.0\n      2010-01-06\n    \n    \n      2010-01-07\n      30.250000\n      30.082857\n      29.864286\n      30.285715\n      119282800.0\n      2010-01-07\n    \n    \n      2010-01-08\n      30.042856\n      30.282858\n      29.865715\n      30.285715\n      111902700.0\n      2010-01-08\n    \n  \n\n\n\n\n\ndata_df['date'] = pd.to_datetime(data_df['date'])\n\n\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))\n\n\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n\n1233 529\n\n\n\n# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n\nx_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\n\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\n(1217, 15)\n(1217,)\n(513, 15)\n(513,)\n\n\n\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n\n\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(40, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=60, batch_size=12, verbose=2)\n\nEpoch 1/60\n\n\n102/102 - 2s - loss: 0.0150 - 2s/epoch - 23ms/step\n\n\nEpoch 2/60\n\n\n102/102 - 0s - loss: 7.6262e-04 - 183ms/epoch - 2ms/step\n\n\nEpoch 3/60\n\n\n102/102 - 0s - loss: 6.6221e-04 - 192ms/epoch - 2ms/step\n\n\nEpoch 4/60\n\n\n102/102 - 0s - loss: 6.3328e-04 - 183ms/epoch - 2ms/step\n\n\nEpoch 5/60\n\n\n102/102 - 0s - loss: 6.0453e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 6/60\n\n\n102/102 - 0s - loss: 5.8064e-04 - 188ms/epoch - 2ms/step\n\n\nEpoch 7/60\n\n\n102/102 - 0s - loss: 5.4382e-04 - 186ms/epoch - 2ms/step\n\n\nEpoch 8/60\n\n\n102/102 - 0s - loss: 5.1208e-04 - 180ms/epoch - 2ms/step\n\n\nEpoch 9/60\n\n\n102/102 - 0s - loss: 4.8958e-04 - 202ms/epoch - 2ms/step\n\n\nEpoch 10/60\n\n\n102/102 - 0s - loss: 4.6846e-04 - 190ms/epoch - 2ms/step\n\n\nEpoch 11/60\n\n\n102/102 - 0s - loss: 4.4484e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 12/60\n\n\n102/102 - 0s - loss: 4.1551e-04 - 211ms/epoch - 2ms/step\n\n\nEpoch 13/60\n\n\n102/102 - 0s - loss: 4.1124e-04 - 191ms/epoch - 2ms/step\n\n\nEpoch 14/60\n\n\n102/102 - 0s - loss: 4.0314e-04 - 190ms/epoch - 2ms/step\n\n\nEpoch 15/60\n\n\n102/102 - 0s - loss: 3.8374e-04 - 199ms/epoch - 2ms/step\n\n\nEpoch 16/60\n\n\n102/102 - 0s - loss: 3.5150e-04 - 196ms/epoch - 2ms/step\n\n\nEpoch 17/60\n\n\n102/102 - 0s - loss: 3.7239e-04 - 205ms/epoch - 2ms/step\n\n\nEpoch 18/60\n\n\n102/102 - 0s - loss: 3.2483e-04 - 190ms/epoch - 2ms/step\n\n\nEpoch 19/60\n\n\n102/102 - 0s - loss: 2.9874e-04 - 183ms/epoch - 2ms/step\n\n\nEpoch 20/60\n\n\n102/102 - 0s - loss: 2.8785e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 21/60\n\n\n102/102 - 0s - loss: 2.8977e-04 - 185ms/epoch - 2ms/step\n\n\nEpoch 22/60\n\n\n102/102 - 0s - loss: 2.6227e-04 - 186ms/epoch - 2ms/step\n\n\nEpoch 23/60\n\n\n102/102 - 0s - loss: 2.6883e-04 - 181ms/epoch - 2ms/step\n\n\nEpoch 24/60\n\n\n102/102 - 0s - loss: 2.4219e-04 - 210ms/epoch - 2ms/step\n\n\nEpoch 25/60\n\n\n102/102 - 0s - loss: 2.4725e-04 - 189ms/epoch - 2ms/step\n\n\nEpoch 26/60\n\n\n102/102 - 0s - loss: 2.3647e-04 - 181ms/epoch - 2ms/step\n\n\nEpoch 27/60\n\n\n102/102 - 0s - loss: 2.5362e-04 - 185ms/epoch - 2ms/step\n\n\nEpoch 28/60\n\n\n102/102 - 0s - loss: 2.2308e-04 - 185ms/epoch - 2ms/step\n\n\nEpoch 29/60\n\n\n102/102 - 0s - loss: 2.0847e-04 - 183ms/epoch - 2ms/step\n\n\nEpoch 30/60\n\n\n102/102 - 0s - loss: 2.2562e-04 - 189ms/epoch - 2ms/step\n\n\nEpoch 31/60\n\n\n102/102 - 0s - loss: 2.0874e-04 - 186ms/epoch - 2ms/step\n\n\nEpoch 32/60\n\n\n102/102 - 0s - loss: 2.1588e-04 - 185ms/epoch - 2ms/step\n\n\nEpoch 33/60\n\n\n102/102 - 0s - loss: 2.1389e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 34/60\n\n\n102/102 - 0s - loss: 1.9209e-04 - 180ms/epoch - 2ms/step\n\n\nEpoch 35/60\n\n\n102/102 - 0s - loss: 1.9363e-04 - 179ms/epoch - 2ms/step\n\n\nEpoch 36/60\n\n\n102/102 - 0s - loss: 2.2127e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 37/60\n\n\n102/102 - 0s - loss: 1.9697e-04 - 183ms/epoch - 2ms/step\n\n\nEpoch 38/60\n\n\n102/102 - 0s - loss: 1.8795e-04 - 189ms/epoch - 2ms/step\n\n\nEpoch 39/60\n\n\n102/102 - 0s - loss: 1.9005e-04 - 186ms/epoch - 2ms/step\n\n\nEpoch 40/60\n\n\n102/102 - 0s - loss: 1.7190e-04 - 191ms/epoch - 2ms/step\n\n\nEpoch 41/60\n\n\n102/102 - 0s - loss: 1.8237e-04 - 188ms/epoch - 2ms/step\n\n\nEpoch 42/60\n\n\n102/102 - 0s - loss: 2.1899e-04 - 180ms/epoch - 2ms/step\n\n\nEpoch 43/60\n\n\n102/102 - 0s - loss: 1.9020e-04 - 192ms/epoch - 2ms/step\n\n\nEpoch 44/60\n\n\n102/102 - 0s - loss: 1.6493e-04 - 188ms/epoch - 2ms/step\n\n\nEpoch 45/60\n\n\n102/102 - 0s - loss: 1.7761e-04 - 182ms/epoch - 2ms/step\n\n\nEpoch 46/60\n\n\n102/102 - 0s - loss: 1.6017e-04 - 192ms/epoch - 2ms/step\n\n\nEpoch 47/60\n\n\n102/102 - 0s - loss: 1.9724e-04 - 192ms/epoch - 2ms/step\n\n\nEpoch 48/60\n\n\n102/102 - 0s - loss: 1.8514e-04 - 194ms/epoch - 2ms/step\n\n\nEpoch 49/60\n\n\n102/102 - 0s - loss: 1.8597e-04 - 180ms/epoch - 2ms/step\n\n\nEpoch 50/60\n\n\n102/102 - 0s - loss: 1.7456e-04 - 187ms/epoch - 2ms/step\n\n\nEpoch 51/60\n\n\n102/102 - 0s - loss: 1.9399e-04 - 193ms/epoch - 2ms/step\n\n\nEpoch 52/60\n\n\n102/102 - 0s - loss: 1.4870e-04 - 179ms/epoch - 2ms/step\n\n\nEpoch 53/60\n\n\n102/102 - 0s - loss: 1.7235e-04 - 173ms/epoch - 2ms/step\n\n\nEpoch 54/60\n\n\n102/102 - 0s - loss: 1.6592e-04 - 192ms/epoch - 2ms/step\n\n\nEpoch 55/60\n\n\n102/102 - 0s - loss: 1.6269e-04 - 173ms/epoch - 2ms/step\n\n\nEpoch 56/60\n\n\n102/102 - 0s - loss: 1.9207e-04 - 182ms/epoch - 2ms/step\n\n\nEpoch 57/60\n\n\n102/102 - 0s - loss: 1.4854e-04 - 184ms/epoch - 2ms/step\n\n\nEpoch 58/60\n\n\n102/102 - 0s - loss: 1.6661e-04 - 189ms/epoch - 2ms/step\n\n\nEpoch 59/60\n\n\n102/102 - 0s - loss: 1.5341e-04 - 180ms/epoch - 2ms/step\n\n\nEpoch 60/60\n\n\n102/102 - 0s - loss: 1.5276e-04 - 181ms/epoch - 2ms/step\n\n\n<keras.callbacks.History at 0x2708cf178e0>\n\n\n\ntrainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invertimos las predicciones\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculate root mean squared e\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n 1/39 [..............................] - ETA: 13s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/39 [======================>.......] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/39 [==============================] - 0s 2ms/step\n\n\n 1/17 [>.............................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/17 [==============================] - 0s 2ms/step\n\n\nTrain Score: 1.26 RMSE\nTest Score: 1.96 RMSE\n\n\n\n# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(min_max_scaler.inverse_transform(dataset), label = \"Precio Historico\")\nplt.plot(trainPredictPlot, label = \"Datos Entrenamiento\")\nplt.plot(testPredictPlot, label = \"Predicción de Precio\")\nplt.legend()\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.show()"
  },
  {
    "objectID": "posts/RN/index.html#simulación-de-pronóstico-usando-lstm-con-python",
    "href": "posts/RN/index.html#simulación-de-pronóstico-usando-lstm-con-python",
    "title": "Redes Neuronales Recurrentes (LSTM)",
    "section": "Simulación de Pronóstico Usando LSTM con Python",
    "text": "Simulación de Pronóstico Usando LSTM con Python\nEl pronóstico que realizamos para poner en marcha nuestro modelo LSTM es sobre el precio de cierre (close) del índice bursátil de Apple (AAPL). La librería o paquete principal para construcción de nuestro modelo fue Tensorflow. A continuación el lector puede apreciar la codificación que se llevó a cabo para poder construir nuestro modelo.\n\n# Librerías \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\n\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_22428\\3683430178.py:5: FutureWarning:\n\nThe pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n\n\n\n\n# Cargamos nuestro Dataset\ndf = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n\n\n# Graficamos el precio de cierre (Close) de AAPL\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.legend(loc='best')\n\n<matplotlib.legend.Legend at 0x1e62400af50>\n\n\n\n\n\n\n# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual\n# se estara trabajando el resto del proyecto\ndata_df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      open\n      close\n      low\n      high\n      volume\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-01-05\n      WLTW\n      123.430000\n      125.839996\n      122.309998\n      126.250000\n      2163600.0\n    \n    \n      2016-01-06\n      WLTW\n      125.239998\n      119.980003\n      119.940002\n      125.540001\n      2386400.0\n    \n    \n      2016-01-07\n      WLTW\n      116.379997\n      114.949997\n      114.930000\n      119.739998\n      2489500.0\n    \n    \n      2016-01-08\n      WLTW\n      115.480003\n      116.620003\n      113.500000\n      117.440002\n      2006300.0\n    \n    \n      2016-01-11\n      WLTW\n      117.010002\n      114.970001\n      114.089996\n      117.330002\n      1408600.0\n    \n  \n\n\n\n\n\ndata_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()\n\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_22428\\1763401402.py:2: FutureWarning:\n\nIn a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n\n\n\n\n\n\n\n  \n    \n      \n      open\n      close\n      low\n      high\n      volume\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      30.490000\n      30.572857\n      30.340000\n      30.642857\n      123432400.0\n    \n    \n      2010-01-05\n      30.657143\n      30.625713\n      30.464285\n      30.798571\n      150476200.0\n    \n    \n      2010-01-06\n      30.625713\n      30.138571\n      30.107143\n      30.747143\n      138040000.0\n    \n    \n      2010-01-07\n      30.250000\n      30.082857\n      29.864286\n      30.285715\n      119282800.0\n    \n    \n      2010-01-08\n      30.042856\n      30.282858\n      29.865715\n      30.285715\n      111902700.0\n    \n  \n\n\n\n\n\ndata_df['date'] = data_df.index\ndata_df.head()\n\n\n\n\n\n  \n    \n      \n      open\n      close\n      low\n      high\n      volume\n      date\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      30.490000\n      30.572857\n      30.340000\n      30.642857\n      123432400.0\n      2010-01-04\n    \n    \n      2010-01-05\n      30.657143\n      30.625713\n      30.464285\n      30.798571\n      150476200.0\n      2010-01-05\n    \n    \n      2010-01-06\n      30.625713\n      30.138571\n      30.107143\n      30.747143\n      138040000.0\n      2010-01-06\n    \n    \n      2010-01-07\n      30.250000\n      30.082857\n      29.864286\n      30.285715\n      119282800.0\n      2010-01-07\n    \n    \n      2010-01-08\n      30.042856\n      30.282858\n      29.865715\n      30.285715\n      111902700.0\n      2010-01-08\n    \n  \n\n\n\n\n\ndata_df['date'] = pd.to_datetime(data_df['date'])\n\n\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))\n\n\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n\n1233 529\n\n\n\n# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n\nx_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\n\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\n(1217, 15)\n(1217,)\n(513, 15)\n(513,)\n\n\n\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n\n\n# Creamos y entrenamos el modelo LSTM\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(40, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=80, batch_size=12, verbose=2)\n\nEpoch 1/80\n\n\n102/102 - 3s - loss: 0.0113 - 3s/epoch - 32ms/step\n\n\nEpoch 2/80\n\n\n102/102 - 0s - loss: 6.3842e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 3/80\n\n\n102/102 - 0s - loss: 5.7844e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 4/80\n\n\n102/102 - 0s - loss: 5.6023e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 5/80\n\n\n102/102 - 0s - loss: 5.1903e-04 - 263ms/epoch - 3ms/step\n\n\nEpoch 6/80\n\n\n102/102 - 0s - loss: 4.8845e-04 - 257ms/epoch - 3ms/step\n\n\nEpoch 7/80\n\n\n102/102 - 0s - loss: 4.5870e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 8/80\n\n\n102/102 - 0s - loss: 4.3559e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 9/80\n\n\n102/102 - 0s - loss: 4.0958e-04 - 261ms/epoch - 3ms/step\n\n\nEpoch 10/80\n\n\n102/102 - 0s - loss: 3.9100e-04 - 266ms/epoch - 3ms/step\n\n\nEpoch 11/80\n\n\n102/102 - 0s - loss: 3.6612e-04 - 263ms/epoch - 3ms/step\n\n\nEpoch 12/80\n\n\n102/102 - 0s - loss: 3.5953e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 13/80\n\n\n102/102 - 0s - loss: 3.2104e-04 - 249ms/epoch - 2ms/step\n\n\nEpoch 14/80\n\n\n102/102 - 0s - loss: 3.0965e-04 - 257ms/epoch - 3ms/step\n\n\nEpoch 15/80\n\n\n102/102 - 0s - loss: 2.8168e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 16/80\n\n\n102/102 - 0s - loss: 2.5952e-04 - 251ms/epoch - 2ms/step\n\n\nEpoch 17/80\n\n\n102/102 - 0s - loss: 2.3784e-04 - 252ms/epoch - 2ms/step\n\n\nEpoch 18/80\n\n\n102/102 - 0s - loss: 2.3917e-04 - 255ms/epoch - 2ms/step\n\n\nEpoch 19/80\n\n\n102/102 - 0s - loss: 2.6529e-04 - 253ms/epoch - 2ms/step\n\n\nEpoch 20/80\n\n\n102/102 - 0s - loss: 2.2736e-04 - 251ms/epoch - 2ms/step\n\n\nEpoch 21/80\n\n\n102/102 - 0s - loss: 2.2051e-04 - 257ms/epoch - 3ms/step\n\n\nEpoch 22/80\n\n\n102/102 - 0s - loss: 2.1977e-04 - 265ms/epoch - 3ms/step\n\n\nEpoch 23/80\n\n\n102/102 - 0s - loss: 2.4310e-04 - 261ms/epoch - 3ms/step\n\n\nEpoch 24/80\n\n\n102/102 - 0s - loss: 2.3080e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 25/80\n\n\n102/102 - 0s - loss: 1.9992e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 26/80\n\n\n102/102 - 0s - loss: 2.1494e-04 - 251ms/epoch - 2ms/step\n\n\nEpoch 27/80\n\n\n102/102 - 0s - loss: 2.1917e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 28/80\n\n\n102/102 - 0s - loss: 2.0118e-04 - 269ms/epoch - 3ms/step\n\n\nEpoch 29/80\n\n\n102/102 - 0s - loss: 2.1571e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 30/80\n\n\n102/102 - 0s - loss: 1.9306e-04 - 255ms/epoch - 2ms/step\n\n\nEpoch 31/80\n\n\n102/102 - 0s - loss: 2.5637e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 32/80\n\n\n102/102 - 0s - loss: 1.9886e-04 - 256ms/epoch - 3ms/step\n\n\nEpoch 33/80\n\n\n102/102 - 0s - loss: 2.1006e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 34/80\n\n\n102/102 - 0s - loss: 1.8493e-04 - 266ms/epoch - 3ms/step\n\n\nEpoch 35/80\n\n\n102/102 - 0s - loss: 1.9396e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 36/80\n\n\n102/102 - 0s - loss: 1.9777e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 37/80\n\n\n102/102 - 0s - loss: 1.8756e-04 - 259ms/epoch - 3ms/step\n\n\nEpoch 38/80\n\n\n102/102 - 0s - loss: 1.8287e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 39/80\n\n\n102/102 - 0s - loss: 1.7400e-04 - 255ms/epoch - 3ms/step\n\n\nEpoch 40/80\n\n\n102/102 - 0s - loss: 1.8051e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 41/80\n\n\n102/102 - 0s - loss: 1.8086e-04 - 267ms/epoch - 3ms/step\n\n\nEpoch 42/80\n\n\n102/102 - 0s - loss: 2.1853e-04 - 267ms/epoch - 3ms/step\n\n\nEpoch 43/80\n\n\n102/102 - 0s - loss: 1.7517e-04 - 256ms/epoch - 3ms/step\n\n\nEpoch 44/80\n\n\n102/102 - 0s - loss: 1.9113e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 45/80\n\n\n102/102 - 0s - loss: 1.6536e-04 - 269ms/epoch - 3ms/step\n\n\nEpoch 46/80\n\n\n102/102 - 0s - loss: 1.7508e-04 - 264ms/epoch - 3ms/step\n\n\nEpoch 47/80\n\n\n102/102 - 0s - loss: 1.8322e-04 - 256ms/epoch - 3ms/step\n\n\nEpoch 48/80\n\n\n102/102 - 0s - loss: 1.6636e-04 - 259ms/epoch - 3ms/step\n\n\nEpoch 49/80\n\n\n102/102 - 0s - loss: 1.7943e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 50/80\n\n\n102/102 - 0s - loss: 1.8184e-04 - 255ms/epoch - 2ms/step\n\n\nEpoch 51/80\n\n\n102/102 - 0s - loss: 1.8314e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 52/80\n\n\n102/102 - 0s - loss: 1.6062e-04 - 259ms/epoch - 3ms/step\n\n\nEpoch 53/80\n\n\n102/102 - 0s - loss: 1.6473e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 54/80\n\n\n102/102 - 0s - loss: 1.6214e-04 - 255ms/epoch - 2ms/step\n\n\nEpoch 55/80\n\n\n102/102 - 0s - loss: 1.7255e-04 - 260ms/epoch - 3ms/step\n\n\nEpoch 56/80\n\n\n102/102 - 0s - loss: 1.5994e-04 - 268ms/epoch - 3ms/step\n\n\nEpoch 57/80\n\n\n102/102 - 0s - loss: 1.6509e-04 - 271ms/epoch - 3ms/step\n\n\nEpoch 58/80\n\n\n102/102 - 0s - loss: 1.6058e-04 - 258ms/epoch - 3ms/step\n\n\nEpoch 59/80\n\n\n102/102 - 0s - loss: 1.5973e-04 - 257ms/epoch - 3ms/step\n\n\nEpoch 60/80\n\n\n102/102 - 0s - loss: 1.6962e-04 - 261ms/epoch - 3ms/step\n\n\nEpoch 61/80\n\n\n102/102 - 0s - loss: 1.7296e-04 - 257ms/epoch - 3ms/step\n\n\nEpoch 62/80\n\n\n102/102 - 0s - loss: 1.8389e-04 - 250ms/epoch - 2ms/step\n\n\nEpoch 63/80\n\n\n102/102 - 0s - loss: 1.6510e-04 - 255ms/epoch - 2ms/step\n\n\nEpoch 64/80\n\n\n102/102 - 0s - loss: 1.5845e-04 - 255ms/epoch - 3ms/step\n\n\nEpoch 65/80\n\n\n102/102 - 0s - loss: 1.5989e-04 - 255ms/epoch - 3ms/step\n\n\nEpoch 66/80\n\n\n102/102 - 0s - loss: 1.6230e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 67/80\n\n\n102/102 - 0s - loss: 1.9168e-04 - 253ms/epoch - 2ms/step\n\n\nEpoch 68/80\n\n\n102/102 - 0s - loss: 1.4577e-04 - 273ms/epoch - 3ms/step\n\n\nEpoch 69/80\n\n\n102/102 - 0s - loss: 1.5320e-04 - 264ms/epoch - 3ms/step\n\n\nEpoch 70/80\n\n\n102/102 - 0s - loss: 1.7691e-04 - 252ms/epoch - 2ms/step\n\n\nEpoch 71/80\n\n\n102/102 - 0s - loss: 1.5635e-04 - 268ms/epoch - 3ms/step\n\n\nEpoch 72/80\n\n\n102/102 - 0s - loss: 1.4415e-04 - 265ms/epoch - 3ms/step\n\n\nEpoch 73/80\n\n\n102/102 - 0s - loss: 1.6942e-04 - 262ms/epoch - 3ms/step\n\n\nEpoch 74/80\n\n\n102/102 - 0s - loss: 1.4780e-04 - 266ms/epoch - 3ms/step\n\n\nEpoch 75/80\n\n\n102/102 - 0s - loss: 1.5037e-04 - 266ms/epoch - 3ms/step\n\n\nEpoch 76/80\n\n\n102/102 - 0s - loss: 1.6177e-04 - 259ms/epoch - 3ms/step\n\n\nEpoch 77/80\n\n\n102/102 - 0s - loss: 1.5421e-04 - 253ms/epoch - 2ms/step\n\n\nEpoch 78/80\n\n\n102/102 - 0s - loss: 1.6364e-04 - 254ms/epoch - 2ms/step\n\n\nEpoch 79/80\n\n\n102/102 - 0s - loss: 1.6383e-04 - 264ms/epoch - 3ms/step\n\n\nEpoch 80/80\n\n\n102/102 - 0s - loss: 1.4784e-04 - 266ms/epoch - 3ms/step\n\n\n<keras.callbacks.History at 0x1e624aeb910>\n\n\n\n# Metricas utilizadas en el modelo \n\ntrainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invertimos las predicciones\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculate root mean squared \ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n 1/39 [..............................] - ETA: 22s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/39 [===================>..........] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/39 [==============================] - 1s 2ms/step\n\n\n 1/17 [>.............................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/17 [==============================] - 0s 2ms/step\n\n\nTrain Score: 1.49 RMSE\nTest Score: 2.44 RMSE\n\n\n\n# Graficamos los resultados \n\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\nplt.plot(min_max_scaler.inverse_transform(dataset), label = \"Precio Historico\")\nplt.plot(trainPredictPlot, label = \"Datos Entrenamiento\")\nplt.plot(testPredictPlot, label = \"Predicción de Precio\")\nplt.legend()\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.show()\n\n\n\n\nUn ejercicio interesante para el lector podría ser, asumir el reto de tratar de mejorar las métricas RMSE para Train como para el Test. Una posible alternativa es utilizar más capas intermedias. Intenta asumir el reto y comenta tus resultados aquí en el post."
  },
  {
    "objectID": "sobre_juan.html",
    "href": "sobre_juan.html",
    "title": "Historias",
    "section": "",
    "text": "En esta sección de mi website conoceras un poco de las diferentes actividades que realizó o he realizado en algunos de mis días.\nLinda Experiencia, impartiendo el curso sobre fundamentos estadísticos utilizando el software de R a estudiantes de la maestría en Gestión de Proyectos de la UNAH.\n\n\n\n\n\nParticipación en conversatorio estudiantes egresados de la Lic. Matemáticas UNAH 2023\n\nNoche de Cine - 8/07/2023\n\nEquipo de Trabajo - IHSS\n\nFestejando Graduación (Lic. Matemáticas) de Amigos"
  },
  {
    "objectID": "posts/RN/index.html#pronósticos-para-el-índice-de-aapl-utilizando-una-red-lstm",
    "href": "posts/RN/index.html#pronósticos-para-el-índice-de-aapl-utilizando-una-red-lstm",
    "title": "Redes Neuronales Recurrentes (LSTM)",
    "section": "Pronósticos para el índice de AAPL utilizando una red LSTM",
    "text": "Pronósticos para el índice de AAPL utilizando una red LSTM\nA continuación realizaremos un ejercicio donde intentamos pronósticar el precio de cierre (Close) del índice de AAPL, la data la puede encontrar dando click en prices-split-adjusted.csv .\nComenzamos importando las librerías a utilizar.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_18008\\2001173417.py:4: FutureWarning:\n\nThe pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n\n\n\nCargamos el DataSet y lo almacenamos en la variable df y data_df. La data almacenada en data_df es la que utilizaremos para realizar todo nuestro estudio.\n\ndf = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n\n\ndata_df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()\n\n\n\n\n\n\n\n\nsymbol\nopen\nclose\nlow\nhigh\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2016-01-05\nWLTW\n123.430000\n125.839996\n122.309998\n126.250000\n2163600.0\n\n\n2016-01-06\nWLTW\n125.239998\n119.980003\n119.940002\n125.540001\n2386400.0\n\n\n2016-01-07\nWLTW\n116.379997\n114.949997\n114.930000\n119.739998\n2489500.0\n\n\n2016-01-08\nWLTW\n115.480003\n116.620003\n113.500000\n117.440002\n2006300.0\n\n\n2016-01-11\nWLTW\n117.010002\n114.970001\n114.089996\n117.330002\n1408600.0\n\n\n\n\n\n\n\nEste bloque de código fue de mucha utilidad, dado que aquí filtramos de nuestro dataset unicamente la información para el indice de AAPL y visualizamos la data.\n\ndata_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()\n\n\n\n\n\n\n\n\nopen\nclose\nlow\nhigh\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n2010-01-04\n30.490000\n30.572857\n30.340000\n30.642857\n123432400.0\n\n\n2010-01-05\n30.657143\n30.625713\n30.464285\n30.798571\n150476200.0\n\n\n2010-01-06\n30.625713\n30.138571\n30.107143\n30.747143\n138040000.0\n\n\n2010-01-07\n30.250000\n30.082857\n29.864286\n30.285715\n119282800.0\n\n\n2010-01-08\n30.042856\n30.282858\n29.865715\n30.285715\n111902700.0\n\n\n\n\n\n\n\nPreliminarmente con este bloque de código podemos ver como se comporta la serie correspondiente al precio de cierre de las acciones de AAPL.\n\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x236b99436a0&gt;\n\n\n\n\n\nNecesitamos manipular nuestro campo de fecha para poder manipularlas como lo que son en realidad (fechas).\n\ndata_df['date'] = data_df.index\ndata_df.head()\n\n\n\n\n\n\n\n\nopen\nclose\nlow\nhigh\nvolume\ndate\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2010-01-04\n30.490000\n30.572857\n30.340000\n30.642857\n123432400.0\n2010-01-04\n\n\n2010-01-05\n30.657143\n30.625713\n30.464285\n30.798571\n150476200.0\n2010-01-05\n\n\n2010-01-06\n30.625713\n30.138571\n30.107143\n30.747143\n138040000.0\n2010-01-06\n\n\n2010-01-07\n30.250000\n30.082857\n29.864286\n30.285715\n119282800.0\n2010-01-07\n\n\n2010-01-08\n30.042856\n30.282858\n29.865715\n30.285715\n111902700.0\n2010-01-08\n\n\n\n\n\n\n\n\ndata_df['date'] = pd.to_datetime(data_df['date'])\n\nTransformamos los datos con MinMaxScaler() para que se distribuyan normal estándar, recuerde que esto es con media cero y varianza 1.\n\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))\n\nDividimos la data en datos de entrenamiento (train), tomando el 70% de los datos para entrenar nuestro modelo y un 20% para prueba (test).\n\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n\n1233 529\n\n\n\n# convertir una matriz de valores en una matriz de conjunto de datos\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n\nx_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\n\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\n(1217, 15)\n(1217,)\n(513, 15)\n(513,)\n\n\n\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n\nComenzamos a crear y entrenar nuestro modelo LSTM.\n\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(20, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=90, batch_size=8, verbose=2)\n\nEpoch 1/90\n153/153 - 2s - loss: 0.0174 - 2s/epoch - 16ms/step\nEpoch 2/90\n153/153 - 0s - loss: 5.3418e-04 - 273ms/epoch - 2ms/step\nEpoch 3/90\n153/153 - 0s - loss: 4.4450e-04 - 278ms/epoch - 2ms/step\nEpoch 4/90\n153/153 - 0s - loss: 4.2874e-04 - 277ms/epoch - 2ms/step\nEpoch 5/90\n153/153 - 0s - loss: 4.0739e-04 - 268ms/epoch - 2ms/step\nEpoch 6/90\n153/153 - 0s - loss: 3.8709e-04 - 284ms/epoch - 2ms/step\nEpoch 7/90\n153/153 - 0s - loss: 3.7214e-04 - 273ms/epoch - 2ms/step\nEpoch 8/90\n153/153 - 0s - loss: 3.6078e-04 - 276ms/epoch - 2ms/step\nEpoch 9/90\n153/153 - 0s - loss: 3.5206e-04 - 290ms/epoch - 2ms/step\nEpoch 10/90\n153/153 - 0s - loss: 3.2790e-04 - 269ms/epoch - 2ms/step\nEpoch 11/90\n153/153 - 0s - loss: 3.0376e-04 - 288ms/epoch - 2ms/step\nEpoch 12/90\n153/153 - 0s - loss: 3.0369e-04 - 272ms/epoch - 2ms/step\nEpoch 13/90\n153/153 - 0s - loss: 3.1673e-04 - 272ms/epoch - 2ms/step\nEpoch 14/90\n153/153 - 0s - loss: 2.6855e-04 - 288ms/epoch - 2ms/step\nEpoch 15/90\n153/153 - 0s - loss: 2.6134e-04 - 274ms/epoch - 2ms/step\nEpoch 16/90\n153/153 - 0s - loss: 2.7831e-04 - 266ms/epoch - 2ms/step\nEpoch 17/90\n153/153 - 0s - loss: 2.3250e-04 - 280ms/epoch - 2ms/step\nEpoch 18/90\n153/153 - 0s - loss: 2.4377e-04 - 277ms/epoch - 2ms/step\nEpoch 19/90\n153/153 - 0s - loss: 2.1988e-04 - 300ms/epoch - 2ms/step\nEpoch 20/90\n153/153 - 0s - loss: 2.1443e-04 - 309ms/epoch - 2ms/step\nEpoch 21/90\n153/153 - 0s - loss: 2.0727e-04 - 308ms/epoch - 2ms/step\nEpoch 22/90\n153/153 - 0s - loss: 2.1768e-04 - 303ms/epoch - 2ms/step\nEpoch 23/90\n153/153 - 0s - loss: 1.9675e-04 - 301ms/epoch - 2ms/step\nEpoch 24/90\n153/153 - 0s - loss: 1.8817e-04 - 298ms/epoch - 2ms/step\nEpoch 25/90\n153/153 - 0s - loss: 2.0940e-04 - 302ms/epoch - 2ms/step\nEpoch 26/90\n153/153 - 0s - loss: 1.8725e-04 - 273ms/epoch - 2ms/step\nEpoch 27/90\n153/153 - 0s - loss: 1.9996e-04 - 268ms/epoch - 2ms/step\nEpoch 28/90\n153/153 - 0s - loss: 1.8666e-04 - 269ms/epoch - 2ms/step\nEpoch 29/90\n153/153 - 0s - loss: 2.0690e-04 - 289ms/epoch - 2ms/step\nEpoch 30/90\n153/153 - 0s - loss: 1.8579e-04 - 290ms/epoch - 2ms/step\nEpoch 31/90\n153/153 - 0s - loss: 1.8400e-04 - 327ms/epoch - 2ms/step\nEpoch 32/90\n153/153 - 0s - loss: 2.0776e-04 - 370ms/epoch - 2ms/step\nEpoch 33/90\n153/153 - 0s - loss: 1.7955e-04 - 285ms/epoch - 2ms/step\nEpoch 34/90\n153/153 - 0s - loss: 1.8656e-04 - 283ms/epoch - 2ms/step\nEpoch 35/90\n153/153 - 0s - loss: 1.7452e-04 - 363ms/epoch - 2ms/step\nEpoch 36/90\n153/153 - 0s - loss: 1.8671e-04 - 291ms/epoch - 2ms/step\nEpoch 37/90\n153/153 - 0s - loss: 2.0337e-04 - 276ms/epoch - 2ms/step\nEpoch 38/90\n153/153 - 0s - loss: 1.8612e-04 - 290ms/epoch - 2ms/step\nEpoch 39/90\n153/153 - 0s - loss: 1.7868e-04 - 287ms/epoch - 2ms/step\nEpoch 40/90\n153/153 - 0s - loss: 1.6047e-04 - 288ms/epoch - 2ms/step\nEpoch 41/90\n153/153 - 0s - loss: 1.7351e-04 - 274ms/epoch - 2ms/step\nEpoch 42/90\n153/153 - 0s - loss: 1.7513e-04 - 277ms/epoch - 2ms/step\nEpoch 43/90\n153/153 - 0s - loss: 1.6558e-04 - 286ms/epoch - 2ms/step\nEpoch 44/90\n153/153 - 0s - loss: 1.6894e-04 - 273ms/epoch - 2ms/step\nEpoch 45/90\n153/153 - 0s - loss: 1.9839e-04 - 270ms/epoch - 2ms/step\nEpoch 46/90\n153/153 - 0s - loss: 1.6972e-04 - 283ms/epoch - 2ms/step\nEpoch 47/90\n153/153 - 0s - loss: 1.9237e-04 - 287ms/epoch - 2ms/step\nEpoch 48/90\n153/153 - 0s - loss: 1.7812e-04 - 276ms/epoch - 2ms/step\nEpoch 49/90\n153/153 - 0s - loss: 1.6258e-04 - 283ms/epoch - 2ms/step\nEpoch 50/90\n153/153 - 0s - loss: 1.5556e-04 - 296ms/epoch - 2ms/step\nEpoch 51/90\n153/153 - 0s - loss: 1.5876e-04 - 279ms/epoch - 2ms/step\nEpoch 52/90\n153/153 - 0s - loss: 1.5916e-04 - 266ms/epoch - 2ms/step\nEpoch 53/90\n153/153 - 0s - loss: 1.7933e-04 - 363ms/epoch - 2ms/step\nEpoch 54/90\n153/153 - 0s - loss: 1.6076e-04 - 298ms/epoch - 2ms/step\nEpoch 55/90\n153/153 - 0s - loss: 1.5508e-04 - 276ms/epoch - 2ms/step\nEpoch 56/90\n153/153 - 0s - loss: 2.0915e-04 - 270ms/epoch - 2ms/step\nEpoch 57/90\n153/153 - 0s - loss: 1.6386e-04 - 340ms/epoch - 2ms/step\nEpoch 58/90\n153/153 - 0s - loss: 1.8083e-04 - 276ms/epoch - 2ms/step\nEpoch 59/90\n153/153 - 0s - loss: 1.5268e-04 - 274ms/epoch - 2ms/step\nEpoch 60/90\n153/153 - 0s - loss: 1.4961e-04 - 275ms/epoch - 2ms/step\nEpoch 61/90\n153/153 - 0s - loss: 1.8783e-04 - 271ms/epoch - 2ms/step\nEpoch 62/90\n153/153 - 0s - loss: 1.4447e-04 - 280ms/epoch - 2ms/step\nEpoch 63/90\n153/153 - 0s - loss: 1.4833e-04 - 275ms/epoch - 2ms/step\nEpoch 64/90\n153/153 - 0s - loss: 1.5035e-04 - 270ms/epoch - 2ms/step\nEpoch 65/90\n153/153 - 0s - loss: 1.6430e-04 - 272ms/epoch - 2ms/step\nEpoch 66/90\n153/153 - 0s - loss: 1.5268e-04 - 280ms/epoch - 2ms/step\nEpoch 67/90\n153/153 - 0s - loss: 1.6926e-04 - 282ms/epoch - 2ms/step\nEpoch 68/90\n153/153 - 0s - loss: 1.4859e-04 - 270ms/epoch - 2ms/step\nEpoch 69/90\n153/153 - 0s - loss: 1.6119e-04 - 285ms/epoch - 2ms/step\nEpoch 70/90\n153/153 - 0s - loss: 1.6663e-04 - 262ms/epoch - 2ms/step\nEpoch 71/90\n153/153 - 0s - loss: 1.6530e-04 - 280ms/epoch - 2ms/step\nEpoch 72/90\n153/153 - 0s - loss: 1.7051e-04 - 285ms/epoch - 2ms/step\nEpoch 73/90\n153/153 - 0s - loss: 1.6448e-04 - 275ms/epoch - 2ms/step\nEpoch 74/90\n153/153 - 0s - loss: 1.6769e-04 - 278ms/epoch - 2ms/step\nEpoch 75/90\n153/153 - 0s - loss: 1.6086e-04 - 271ms/epoch - 2ms/step\nEpoch 76/90\n153/153 - 0s - loss: 1.6478e-04 - 279ms/epoch - 2ms/step\nEpoch 77/90\n153/153 - 0s - loss: 1.5804e-04 - 263ms/epoch - 2ms/step\nEpoch 78/90\n153/153 - 0s - loss: 1.5207e-04 - 334ms/epoch - 2ms/step\nEpoch 79/90\n153/153 - 0s - loss: 1.3981e-04 - 295ms/epoch - 2ms/step\nEpoch 80/90\n153/153 - 0s - loss: 1.3907e-04 - 287ms/epoch - 2ms/step\nEpoch 81/90\n153/153 - 0s - loss: 1.5976e-04 - 307ms/epoch - 2ms/step\nEpoch 82/90\n153/153 - 0s - loss: 1.6238e-04 - 339ms/epoch - 2ms/step\nEpoch 83/90\n153/153 - 0s - loss: 1.4658e-04 - 497ms/epoch - 3ms/step\nEpoch 84/90\n153/153 - 0s - loss: 1.6490e-04 - 310ms/epoch - 2ms/step\nEpoch 85/90\n153/153 - 0s - loss: 1.5613e-04 - 283ms/epoch - 2ms/step\nEpoch 86/90\n153/153 - 0s - loss: 1.5913e-04 - 302ms/epoch - 2ms/step\nEpoch 87/90\n153/153 - 0s - loss: 1.5089e-04 - 288ms/epoch - 2ms/step\nEpoch 88/90\n153/153 - 0s - loss: 1.6357e-04 - 276ms/epoch - 2ms/step\nEpoch 89/90\n153/153 - 0s - loss: 1.4136e-04 - 271ms/epoch - 2ms/step\nEpoch 90/90\n153/153 - 0s - loss: 1.4603e-04 - 275ms/epoch - 2ms/step\n\n\n&lt;keras.callbacks.History at 0x236b9801000&gt;\n\n\nEvaluamos nuestro modelo, utilizando la métrica del RMSE para los datos de train y test.\n\ntrainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# Invertimos las predicciones, dado que las habiamos transformado con MinMaxScaler\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculamos el RMSE\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n 1/39 [..............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/39 [==============================] - 0s 1ms/step\n 1/17 [&gt;.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/17 [==============================] - 0s 1ms/step\nTrain Score: 1.20 RMSE\nTest Score: 1.91 RMSE\n\n\nFinalmente, observamos nuestro resultados gráficamente\n\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\nplt.plot(min_max_scaler.inverse_transform(dataset), label = \"Precio Historico\")\nplt.plot(trainPredictPlot, label = \"Datos Entrenamiento\")\nplt.plot(testPredictPlot, label = \"Predicción de Precio\")\nplt.legend()\nplt.show()\n\n\n\n\nNote que nuestros resultados son muy buenos, sin embargo, podrían mejorarse, quizas aplicando más capas ocultas a nuestra red, recuerde que solo utilizamos 20. Este podría ser un buen ejercicio para que usted intente mejorar estos resultados.\nRecuerda que puedes comentar este post, agradeceria que lo hagas ya sea para alguna sugerencia u observación. Saludos espero hayas conocido un poco sobre este tipo de Red Neuronal en particular y su potente poder predectivo."
  }
]