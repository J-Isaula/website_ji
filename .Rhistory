print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=60, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=50, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=40, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=70, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=75, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=80, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
# Librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
# Librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Graficamos el precio de cierre (Close) de AAPL
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);
plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.legend(loc='best')
# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual
# se estara trabajando el resto del proyecto
data_df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df.head()
data_df = data_df[data_df.symbol == 'AAPL']
data_df.drop(['symbol'],1,inplace=True)
data_df.head()
data_df['date'] = data_df.index
data_df.head()
data_df['date'] = pd.to_datetime(data_df['date'])
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))
# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos
def create_dataset(dataset, look_back=15):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
dataX.append(a)
dataY.append(dataset[i + look_back, 0])
return np.array(dataX), np.array(dataY)
x_train, y_train = create_dataset(train, look_back=15)
x_test, y_test = create_dataset(test, look_back=15)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=80, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
msleep
data(msleep)
data(msleep)
msleep
msleep
library(dplyr)
data(msleep)
library(ggplot2)
data("msleep")
msleep
library(ggplot2)
library(ggplot2)
library(ggplot2)
library(ggplot2)
data(msleep)
library(ggplot2)
data("msleep")
data
msleep
install.packages("ggplot2")
library(ggplot2)
data("msleep")
View(msleep)
# Histograma de las horas de sueño de los mamiferos
library(tidyverse)
ggplot()
msleep %>%
ggplot(aes(sleep_total))
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram()
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram() +
theme_bw()
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram() +
theme_classic()
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram() +
theme_minimal()
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram() +
theme_minimal() +
labs(title = "Distribución de las horas de sueño para los mamiferos")
msleep %>%
ggplot(aes(sleep_total)) +
geom_histogram() +
theme_minimal() +
labs(title = "Distribución de las horas de sueño para los mamiferos",
subtitle = "by Juan Isaula")
mean(sleep_total)
mean(msleep$sleep_total)
median(msleep$sleep_total)
msleep %>% count(sleep_total, sort = TRUE)
msleep %>% filter(vore == "insecti) %>% view()
knjkgbnklbvlb
9u798yt89yt98y
msleep %>% filter(vore == "insecti") %>% view()
msleep %>% filter(vore == "insecti") %>% view()
msleep %>% filter(vore == "insecti") %>%
summarise(media_sleep_insc = mean(sleep_total),
mediana_sleep_insc = median(sleep_total))
msleep %>% filter(vore == "insecti") %>%
summarise(media_sleep_insc = mean(sleep_total),
mediana_sleep_insc = median(sleep_total)) %>% view()
consumo_alimentos <- readRDS("C:/Users/juani/OneDrive/Documentos/2023/Presentación MSc/food_consumption.rds")
View(consumo_alimentos)
# 1. uno que contenga las filas de consumo de Bélgica
consumo_belgica <- consumo_alimentos %>%
filter(country == "Belgium")
# 1. uno que contenga las filas de consumo de Bélgica
consumo_belgica <- consumo_alimentos %>%
filter(country == "Belgium") %>% view()
consumo_usa <- consumo_alimentos %>%
filter(country == "USA") %>% view()
mean(consumo_belgica$consumption)
median(consumo_belgica$consumption)
# USA
mean(consumo_usa$consumption)
median(consumo_usa$consumption)
consumo_alimentos %>%
# Filtramos para Bélgica y EE.UU
filter(country %in% c("Belgium", "USA")) %>% view()
consumo_alimentos %>%
# Filtramos para Bélgica y EE.UU
filter(country %in% c("Belgium", "USA")) %>%
# Agrupamos por país
group_by(country) %>% view()
# Obtenemos resumen de consumo meduo y mediano
summarise(mean_consumption   = mean(consumption),
median_consumption = median(consumption)) %>% view()
consumo_alimentos %>%
# Filtramos para Bélgica y EE.UU
filter(country %in% c("Belgium", "USA")) %>%
# Agrupamos por país
group_by(country) %>%
# Obtenemos resumen de consumo meduo y mediano
summarise(mean_consumption   = mean(consumption),
median_consumption = median(consumption)) %>% view()
consumo_alimentos %>%
filter(food_category == "rice") %>%
ggplot(aes(co2_emission)) +
geom_histogram()
AAPL <- read_csv(C:/Users/juani/OneDrive/Documentos/2023/Presentación MSc/AAPL.csv)
AAPL <- read_csv("C:/Users/juani/OneDrive/Documentos/2023/Presentación MSc/AAPL.csv")
View(AAPL)
pairs(AAPL)
cor(AAPL)
cor(AAPL$Open, AAPL$Close)
modelo <- lm(Close ~ OPen, data = AAPL)
modelo <- lm(Close ~ Open, data = AAPL)
summary(modelo)
ggplot(AAPL, aes(Open, Close)) +
geom_point() +
stat_smooth(method = lm)+
theme_minimal()
reticulate::repl_python()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
reticulate::repl_python()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df.head()
data_df = data_df[data_df.symbol == 'AAPL']
data_df.drop(['symbol'],1,inplace=True)
data_df.head()
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);
plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.legend(loc='best')
data_df['date'] = data_df.index
data_df.head()
data_df['date'] = pd.to_datetime(data_df['date'])
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
train_size = int(len(dataset) * 0.7)
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))
def create_dataset(dataset, look_back=15):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos
def create_dataset(dataset, look_back=15):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
dataX.append(a)
dataY.append(dataset[i + look_back, 0])
return np.array(dataX), np.array(dataY)
x_train, y_train = create_dataset(train, look_back=15)
x_test, y_test = create_dataset(test, look_back=15)
print(x_train.shape)
print(x_test.shape)
print(y_test.shape)
x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
look_back = 15
model = Sequential()
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=90, batch_size=8, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.show()
