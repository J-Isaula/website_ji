look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=40, batch_size=8, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=45, batch_size=8, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=45, batch_size=12, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=50, batch_size=12, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=60, batch_size=12, verbose=2)
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared e
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.show()
# shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
quit
direccion_data <- "C:/Users/juani/Instituto Hondureño de Seguridad Social/Tratamiento de Data - General/Bases sin tratar/Indicadores Actuariales/Marzo 2023/"
View(data)
library(tidyverse)
library(purrr)
library(lubridate)
library(data.table)
library(vroom)
direccion_data <- "C:/Users/juani/Instituto Hondureño de Seguridad Social/Tratamiento de Data - General/Bases sin tratar/Indicadores Actuariales/Marzo 2023/"
direccion_rnp  <- "C:/Users/juani/OneDrive - Universidad Nacional Autónoma de Honduras/Tratamiento_Data/RNP/"
archivos_data <-
tibble(archivos = list.files(direccion_data))
View(archivos_data)
archivos_data <-
tibble(archivos = list.files(direccion_data)) %>%
mutate(existe = str_detect(archivos, "_2023"))
View(archivos_data)
archivos_data <-
tibble(archivos = list.files(direccion_data)) %>%
mutate(existe = str_detect(archivos, "_2023")) %>%
filter(existe == TRUE)
archivos_data <-
tibble(archivos = list.files(direccion_data)) %>%
mutate(existe = str_detect(archivos, "_2023")) %>%
filter(existe == TRUE) %>%
select(archivos) %>% pull()
archivos_rnp <-
tibble(archivos = list.files(direccion_rnp)) %>%
mutate(existe = str_detect(archivos, "nac_sexo_fec")) %>%
filter(existe == TRUE) %>%
select(archivos) %>% pull()
archivos_rnp
info_rnp <-
map_df(.x = archivos_rnp,
.f = function(x)
vroom(file =  paste0(direccion_rnp, x), delim = "," , col_names = TRUE) %>%
mutate(IDENTIDAD =  as.character(IDENTIDAD)
)
)
info_rnp
data <-
map(.x = archivos_data,
.f = function(x)
vroom(file =  paste0(direccion_data, x), delim = "$" , col_names = TRUE)
)
data
names(data) <-
str_remove(archivos_data, "_Marzo_2023.txt")
data
rm(direccion_data,direccion_rnp,archivos_data,archivos_rnp)
reticulate::repl_python()
# Librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Graficamos el precio de cierre (Close) de AAPL
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);
plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.legend(loc='best')
# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual
# se estara trabajando el resto del proyecto
data_df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df.head()
data_df = data_df[data_df.symbol == 'AAPL']
data_df.drop(['symbol'],1,inplace=True)
data_df.head()
data_df['date'] = data_df.index
data_df.head()
data_df['date'] = pd.to_datetime(data_df['date'])
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))
# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos
def create_dataset(dataset, look_back=15):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
dataX.append(a)
dataY.append(dataset[i + look_back, 0])
return np.array(dataX), np.array(dataY)
x_train, y_train = create_dataset(train, look_back=15)
x_test, y_test = create_dataset(test, look_back=15)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=60, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=50, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=40, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=70, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=75, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=80, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
# Librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
# Librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import datetime
import math, time
import itertools
from sklearn import preprocessing
import datetime
from operator import itemgetter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
from keras.models import load_model
import keras
import h5py
import requests
import os
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Cargamos nuestro Dataset
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
# Graficamos el precio de cierre (Close) de AAPL
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);
plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.legend(loc='best')
# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual
# se estara trabajando el resto del proyecto
data_df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
data_df.head()
data_df = data_df[data_df.symbol == 'AAPL']
data_df.drop(['symbol'],1,inplace=True)
data_df.head()
data_df['date'] = data_df.index
data_df.head()
data_df['date'] = pd.to_datetime(data_df['date'])
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))
train_size = int(len(dataset) * 0.7)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))
# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos
def create_dataset(dataset, look_back=15):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
dataX.append(a)
dataY.append(dataset[i + look_back, 0])
return np.array(dataX), np.array(dataY)
x_train, y_train = create_dataset(train, look_back=15)
x_test, y_test = create_dataset(test, look_back=15)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
# Creamos y entrenamos el modelo LSTM
look_back = 15
model = Sequential()
model.add(LSTM(40, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=80, batch_size=12, verbose=2)
# Metricas utilizadas en el modelo
trainPredict = model.predict(x_train)
testPredict = model.predict(x_test)
# invertimos las predicciones
trainPredict = min_max_scaler.inverse_transform(trainPredict)
trainY = min_max_scaler.inverse_transform([y_train])
testPredict = min_max_scaler.inverse_transform(testPredict)
testY = min_max_scaler.inverse_transform([y_test])
# calculate root mean squared
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# Graficamos los resultados
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
plt.plot(min_max_scaler.inverse_transform(dataset), label = "Precio Historico")
plt.plot(trainPredictPlot, label = "Datos Entrenamiento")
plt.plot(testPredictPlot, label = "Predicción de Precio")
plt.legend()
plt.title('Indice de AAPL')
plt.xlabel('días')
plt.ylabel('precio de cierre (Close)')
plt.show()
