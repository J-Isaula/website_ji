{
  "hash": "b5d1bf510861ed4b1cbdebbeff9b59e2",
  "result": {
    "markdown": "---\ntitle: \"Redes Neuronales Recurrentes (LSTM)\"\nsubtitle: \"Pronósticos de índice AAPL\"\nauthor: \"Juan Isaula\"\ndate: \"2023-05-23\"\ncategories: [RNN, LSTM, Tensorflow, Python]\nimage: \"fondo.png\"\n---\n\n## \n\n## Red de Memoria de Corto y Largo Plazo (LSTM)\n\nLas redes LSTM (Long Short-Term Memory) son un tipo especial de redes neuronales recurrentes diseñadas con celdas de memoria que mantienen su estado a largo plazo. El principal objetivo de este tipo de redes es la solución del desvanecimiento del gradiente experimentado en las redes recurrentes. Globalmente, el flujo computacional de LSTM se ve de la siguiente manera:\n\n![Flujo computacional de LSTM](Figure%204.16.png)\n\nLas redes neuronales recurrentes pasan solo un estado oculto $h_t$ a través de cada iteración. Pero LSTM pasa dos vectores: $h_t-$estado oculto *(memoria a corto plazo) y* $c_t-$estado celular *(memoria a largo plazo).*\n\nLas salidas de la celda LSTM se calculan a traves de las fórmulas que se muestran a continuación:\n\n$$\\begin{eqnarray}\ni_t &=& \\sigma(w_{ii}x_t + b_ii + w_{hi}h_{(t-1)} + b_{hi})\\\\[0.2cm]\nf_t &=& \\sigma(w_{if}x_t + b_{if} + w_{hj}h_{(t-1)} + b_{hf})\\\\[0.2cm]\ng_t &=& \\tanh(w_{ig}x_t + b_{ig} + w_{hg}h_{(t-1)} + b_{hn})\\\\[0.2cm]\no_t &=& \\sigma(w_{io}x_t + b_{io} + w_{ho}h_{(t-1)} + b_{ho})\\\\[0.2cm]\nc_t &=& f_t \\circ c_{t-1} + i_t\\circ g_t\\\\[0.2cm]\nh_t &=& o_t \\circ \\tanh(c_t)\n\\end{eqnarray}$$\n\ndonde:\n\n-   $\\sigma$ es la función sigmoidea\n\n-   $\\circ$ es el producto de Hadamard, que es:\n\n    $$\n    \\begin{bmatrix}\n    a_1 \\\\\n    a_2 \\\\\n    a_3\n    \\end{bmatrix} \\circ \n    \\begin{bmatrix}\n    b_1 \\\\\n    b_2 \\\\\n    b_3 \n    \\end{bmatrix} = \\begin{bmatrix}\n    a_1b_1\\\\\n    a_2b_2\\\\\n    a_3b_3\n    \\end{bmatrix}\n    $$\n\n### Variables\n\n1.  $i_t$ `(puerta de entrada)` es la variable que se utiliza para actualizar el estado de la celda $c_t$. El estado previamente oculto $c_t$ y la entrada secuencial actual $x_t$ se dan como entrada a una función sigmoidea. Si la salida está cerca a uno, más importante es la información.\n\n2.  $f_t$ `(puerta de olvido)` es la variable que decide que información debe olvidarse en el estado de celda $c_t$. El estadp previamente oculto $h_t$ y la entrada de secuencia $x_t$ se dan como entradas a una función sigmoidea. Si la salida $f_t$ está cerca de cero, entonces la información puede olvidarse, mientras que si la salida esta cerca de uno, la información debe almacenarse.\n\n3.  $g_t$ representa información importante potencialmente nueva para el estado celular $c_t$.\n\n4.  $c_t$ `(estado celular)` es una suma de:\n\n    -   Estado de celada anterior $c_{t-1}$ con alguna información olvidada $f_t$.\n\n    -   Nueva información de $g_t$.\n\n<!-- -->\n\n5.  $o_t$ `(puerta de salida)` es la variable para actualizar el estado oculto $h_t$.\n\n6.  $h_t$ `(estado oculto)` es el siguiente estado oculto que se calcula seleccionando la información importante $o_t$ del estado de celda $c_t$.\n\nLa siguiente figura muestra el gráfico computacional de la `celda` `LSTM:`\n\n![Gráfico computacional de LSTM](Figure%204.17.png)\n\nLa red LSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n-   $w_{ii}, w_{hi}, w_{if}, w_{hf}, w_{ig}, w_{hg}, w_{io}, w_{ho}$ - Pesos\n\n-   $b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{io}, b_{ho}$ - Sesgos\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}