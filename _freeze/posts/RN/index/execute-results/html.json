{
  "hash": "5f20b05d409a1390c9ef1d086b29dd40",
  "result": {
    "markdown": "---\ntitle: \"Redes Neuronales Recurrentes (LSTM)\"\nsubtitle: \"Pronósticos de índice AAPL\"\nauthor: \"Juan Isaula\"\ndate: \"2023-05-23\"\ncategories: [RNN, LSTM, Tensorflow, Python]\nimage: \"fondo.png\"\n---\n\n## \n\nLas redes neuronales recurrentes son una arquitectura bastante empleada puesto que emplean los output de salida para retroalimentarse y continuar prediciendo un output.\n\n![](Figure.png)\n\n### Usos\n\n-   Principalmente sirve para contextos donde el orden importa y sirve como predictor recurrente (series de tiempo, textos, audios)\n\n-   Sin embargo, ¿que tanto recordar el pasado? ¿cuál es la consecuencia en el rendimiento y demora de estimación?\n\n-   Sus tipos más utilizados son `LSTM` y `GRU`\n\nEn este post nos enfocaremos en utilizar, describir y simular la red LSTM.\n\n### Aplicaciones \n\n-   Predictor de la siguiente palabra\n\n-   Series de tiempo\n\n-   No recomendable para transversal\n\n## Red de Memoria de Corto y Largo Plazo (LSTM)\n\nLas redes LSTM (Long Short-Term Memory) son un tipo especial de redes neuronales recurrentes diseñadas con celdas de memoria que mantienen su estado a largo plazo. El principal objetivo de este tipo de redes es la solución del desvanecimiento del gradiente experimentado en las redes recurrentes. Globalmente, el flujo computacional de LSTM se ve de la siguiente manera:\n\n![Flujo computacional de LSTM](Figure%204.16.png)\n\nLas redes neuronales recurrentes pasan solo un estado oculto $h_t$ a través de cada iteración. Pero LSTM pasa dos vectores: $h_t-$estado oculto *(memoria a corto plazo) y* $c_t-$estado celular *(memoria a largo plazo).*\n\nLas salidas de la celda LSTM se calculan a traves de las fórmulas que se muestran a continuación:\n\n$$\\begin{eqnarray}\ni_t &=& \\sigma(w_{ii}x_t + b_ii + w_{hi}h_{(t-1)} + b_{hi})\\\\[0.2cm]\nf_t &=& \\sigma(w_{if}x_t + b_{if} + w_{hj}h_{(t-1)} + b_{hf})\\\\[0.2cm]\ng_t &=& \\tanh(w_{ig}x_t + b_{ig} + w_{hg}h_{(t-1)} + b_{hn})\\\\[0.2cm]\no_t &=& \\sigma(w_{io}x_t + b_{io} + w_{ho}h_{(t-1)} + b_{ho})\\\\[0.2cm]\nc_t &=& f_t \\circ c_{t-1} + i_t\\circ g_t\\\\[0.2cm]\nh_t &=& o_t \\circ \\tanh(c_t)\n\\end{eqnarray}$$\n\ndonde:\n\n-   $\\sigma$ es la función sigmoidea\n\n-   $\\circ$ es el producto de Hadamard, que es:\n\n    $$\n    \\begin{bmatrix}\n    a_1 \\\\\n    a_2 \\\\\n    a_3\n    \\end{bmatrix} \\circ \n    \\begin{bmatrix}\n    b_1 \\\\\n    b_2 \\\\\n    b_3 \n    \\end{bmatrix} = \\begin{bmatrix}\n    a_1b_1\\\\\n    a_2b_2\\\\\n    a_3b_3\n    \\end{bmatrix}\n    $$\n\n### Variables\n\n1.  $i_t$ `(puerta de entrada)` es la variable que se utiliza para actualizar el estado de la celda $c_t$. El estado previamente oculto $c_t$ y la entrada secuencial actual $x_t$ se dan como entrada a una función sigmoidea. Si la salida está cerca a uno, más importante es la información.\n\n2.  $f_t$ `(puerta de olvido)` es la variable que decide que información debe olvidarse en el estado de celda $c_t$. El estadp previamente oculto $h_t$ y la entrada de secuencia $x_t$ se dan como entradas a una función sigmoidea. Si la salida $f_t$ está cerca de cero, entonces la información puede olvidarse, mientras que si la salida esta cerca de uno, la información debe almacenarse.\n\n3.  $g_t$ representa información importante potencialmente nueva para el estado celular $c_t$.\n\n4.  $c_t$ `(estado celular)` es una suma de:\n\n    -   Estado de celada anterior $c_{t-1}$ con alguna información olvidada $f_t$.\n\n    -   Nueva información de $g_t$.\n\n<!-- -->\n\n5.  $o_t$ `(puerta de salida)` es la variable para actualizar el estado oculto $h_t$.\n\n6.  $h_t$ `(estado oculto)` es el siguiente estado oculto que se calcula seleccionando la información importante $o_t$ del estado de celda $c_t$.\n\nLa siguiente figura muestra el gráfico computacional de la `celda` `LSTM:`\n\n![Gráfico computacional de LSTM](Figure%204.17.png)\n\nLa red LSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n-   $w_{ii}, w_{hi}, w_{if}, w_{hf}, w_{ig}, w_{hg}, w_{io}, w_{ho}$ - Pesos\n\n-   $b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{io}, b_{ho}$ - Sesgos\n\n## Simulación de Pronóstico Usando LSTM con Python\n\nEl pronóstico que realizamos para poner en marcha nuestro modelo LSTM es sobre el precio de cierre (close) del índice bursátil de Apple (AAPL). La librería o paquete principal para construcción de nuestro modelo fue `Tensorflow`. A continuación el lector puede apreciar la codificación que se llevó a cabo para poder construir nuestro modelo.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Librerías \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_15680\\3683430178.py:5: FutureWarning:\n\nThe pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Cargamos nuestro Dataset\ndf = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Graficamos el precio de cierre (Close) de AAPL\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.legend(loc='best')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<matplotlib.legend.Legend at 0x1eed0f0af50>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=580 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# data_df es nuestra data que comenzara a tratarse y posterioremente con la cual\n# se estara trabajando el resto del proyecto\ndata_df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-01-05</th>\n      <td>WLTW</td>\n      <td>123.430000</td>\n      <td>125.839996</td>\n      <td>122.309998</td>\n      <td>126.250000</td>\n      <td>2163600.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-06</th>\n      <td>WLTW</td>\n      <td>125.239998</td>\n      <td>119.980003</td>\n      <td>119.940002</td>\n      <td>125.540001</td>\n      <td>2386400.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-07</th>\n      <td>WLTW</td>\n      <td>116.379997</td>\n      <td>114.949997</td>\n      <td>114.930000</td>\n      <td>119.739998</td>\n      <td>2489500.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-08</th>\n      <td>WLTW</td>\n      <td>115.480003</td>\n      <td>116.620003</td>\n      <td>113.500000</td>\n      <td>117.440002</td>\n      <td>2006300.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-11</th>\n      <td>WLTW</td>\n      <td>117.010002</td>\n      <td>114.970001</td>\n      <td>114.089996</td>\n      <td>117.330002</td>\n      <td>1408600.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndata_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_15680\\1763401402.py:2: FutureWarning:\n\nIn a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>30.490000</td>\n      <td>30.572857</td>\n      <td>30.340000</td>\n      <td>30.642857</td>\n      <td>123432400.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>30.657143</td>\n      <td>30.625713</td>\n      <td>30.464285</td>\n      <td>30.798571</td>\n      <td>150476200.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>30.625713</td>\n      <td>30.138571</td>\n      <td>30.107143</td>\n      <td>30.747143</td>\n      <td>138040000.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>30.250000</td>\n      <td>30.082857</td>\n      <td>29.864286</td>\n      <td>30.285715</td>\n      <td>119282800.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>30.042856</td>\n      <td>30.282858</td>\n      <td>29.865715</td>\n      <td>30.285715</td>\n      <td>111902700.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata_df['date'] = data_df.index\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n      <th>date</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>30.490000</td>\n      <td>30.572857</td>\n      <td>30.340000</td>\n      <td>30.642857</td>\n      <td>123432400.0</td>\n      <td>2010-01-04</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>30.657143</td>\n      <td>30.625713</td>\n      <td>30.464285</td>\n      <td>30.798571</td>\n      <td>150476200.0</td>\n      <td>2010-01-05</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>30.625713</td>\n      <td>30.138571</td>\n      <td>30.107143</td>\n      <td>30.747143</td>\n      <td>138040000.0</td>\n      <td>2010-01-06</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>30.250000</td>\n      <td>30.082857</td>\n      <td>29.864286</td>\n      <td>30.285715</td>\n      <td>119282800.0</td>\n      <td>2010-01-07</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>30.042856</td>\n      <td>30.282858</td>\n      <td>29.865715</td>\n      <td>30.285715</td>\n      <td>111902700.0</td>\n      <td>2010-01-08</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndata_df['date'] = pd.to_datetime(data_df['date'])\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1233 529\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# create_dataset: convertir una matriz de valores en una matriz de conjunto de datos\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nx_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1217, 15)\n(1217,)\n(513, 15)\n(513,)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Creamos y entrenamos el modelo LSTM\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(40, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=60, batch_size=12, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 3s - loss: 0.0338 - 3s/epoch - 31ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 9.9663e-04 - 222ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 6.5925e-04 - 214ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 6.0674e-04 - 215ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 5.8148e-04 - 215ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 6/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 5.5606e-04 - 213ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 7/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 5.4358e-04 - 209ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 8/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 5.1103e-04 - 210ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 9/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 4.8448e-04 - 211ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 10/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 4.6867e-04 - 208ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 11/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 4.4561e-04 - 209ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 12/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 4.3198e-04 - 215ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 13/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 4.1709e-04 - 228ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 14/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.9856e-04 - 247ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 15/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.9495e-04 - 253ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 16/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.5599e-04 - 259ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 17/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.6534e-04 - 260ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 18/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.2957e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 19/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.1943e-04 - 260ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 20/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 3.2481e-04 - 256ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 21/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.9763e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 22/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.8552e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 23/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.8551e-04 - 260ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 24/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.7440e-04 - 263ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 25/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.4704e-04 - 262ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 26/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.4839e-04 - 280ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 27/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.5600e-04 - 283ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 28/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.4723e-04 - 263ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 29/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.3212e-04 - 265ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 30/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.3427e-04 - 260ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 31/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.3414e-04 - 265ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 32/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.5744e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 33/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.1674e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 34/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.0715e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 35/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.1523e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 36/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.1403e-04 - 274ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 37/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.9267e-04 - 289ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 38/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8953e-04 - 285ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 39/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.9211e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 40/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.1431e-04 - 267ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 41/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8277e-04 - 275ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 42/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8529e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 43/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7988e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 44/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8537e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 45/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8835e-04 - 262ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 46/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.9919e-04 - 269ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 47/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8098e-04 - 265ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 48/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8758e-04 - 274ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 49/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7596e-04 - 272ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 50/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 2.0812e-04 - 273ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 51/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.8111e-04 - 295ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 52/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7374e-04 - 264ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 53/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.6604e-04 - 268ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 54/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7562e-04 - 293ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 55/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7096e-04 - 269ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 56/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.6818e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 57/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.5963e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 58/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.7307e-04 - 261ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 59/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.6153e-04 - 266ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 60/60\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n102/102 - 0s - loss: 1.6028e-04 - 274ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<keras.callbacks.History at 0x1eed19e78e0>\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Metricas utilizadas en el modelo \n\ntrainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invertimos las predicciones\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculate root mean squared \ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/39 [..............................] - ETA: 21s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/39 [====================>.........] - ETA: 0s \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r39/39 [==============================] - 1s 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/17 [>.............................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17/17 [==============================] - 0s 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain Score: 1.88 RMSE\nTest Score: 3.80 RMSE\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Graficamos los resultados \n\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\nplt.plot(min_max_scaler.inverse_transform(dataset), label = \"Precio Historico\")\nplt.plot(trainPredictPlot, label = \"Datos Entrenamiento\")\nplt.plot(testPredictPlot, label = \"Predicción de Precio\")\nplt.legend()\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=593 height=449}\n:::\n:::\n\n\nUn ejercicio interesante para el lector podría ser, asumir el reto de tratar de mejorar las métricas `RMSE` para Train como para el Test. Una posible alternativa es utilizar más capas intermedias. Intenta asumir el reto y comenta tus resultados aquí en el post.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}