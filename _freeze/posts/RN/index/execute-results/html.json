{
  "hash": "ff2f73d6a80a3512895747d186b38baf",
  "result": {
    "markdown": "---\ntitle: \"Redes Neuronales Recurrentes (LSTM)\"\nsubtitle: \"Pronósticos de índice AAPL\"\nauthor: \"Juan Isaula\"\ndate: \"2023-05-23\"\ncategories: [RNN, LSTM, Tensorflow, Python]\nimage: \"fondo.png\"\n---\n\n## \n\n## Red de Memoria de Corto y Largo Plazo (LSTM)\n\nLas redes LSTM (Long Short-Term Memory) son un tipo especial de redes neuronales recurrentes diseñadas con celdas de memoria que mantienen su estado a largo plazo. El principal objetivo de este tipo de redes es la solución del desvanecimiento del gradiente experimentado en las redes recurrentes. Globalmente, el flujo computacional de LSTM se ve de la siguiente manera:\n\n![Flujo computacional de LSTM](Figure%204.16.png)\n\nLas redes neuronales recurrentes pasan solo un estado oculto $h_t$ a través de cada iteración. Pero LSTM pasa dos vectores: $h_t-$estado oculto *(memoria a corto plazo) y* $c_t-$estado celular *(memoria a largo plazo).*\n\nLas salidas de la celda LSTM se calculan a traves de las fórmulas que se muestran a continuación:\n\n$$\\begin{eqnarray}\ni_t &=& \\sigma(w_{ii}x_t + b_ii + w_{hi}h_{(t-1)} + b_{hi})\\\\[0.2cm]\nf_t &=& \\sigma(w_{if}x_t + b_{if} + w_{hj}h_{(t-1)} + b_{hf})\\\\[0.2cm]\ng_t &=& \\tanh(w_{ig}x_t + b_{ig} + w_{hg}h_{(t-1)} + b_{hn})\\\\[0.2cm]\no_t &=& \\sigma(w_{io}x_t + b_{io} + w_{ho}h_{(t-1)} + b_{ho})\\\\[0.2cm]\nc_t &=& f_t \\circ c_{t-1} + i_t\\circ g_t\\\\[0.2cm]\nh_t &=& o_t \\circ \\tanh(c_t)\n\\end{eqnarray}$$\n\ndonde:\n\n-   $\\sigma$ es la función sigmoidea\n\n-   $\\circ$ es el producto de Hadamard, que es:\n\n    $$\n    \\begin{bmatrix}\n    a_1 \\\\\n    a_2 \\\\\n    a_3\n    \\end{bmatrix} \\circ \n    \\begin{bmatrix}\n    b_1 \\\\\n    b_2 \\\\\n    b_3 \n    \\end{bmatrix} = \\begin{bmatrix}\n    a_1b_1\\\\\n    a_2b_2\\\\\n    a_3b_3\n    \\end{bmatrix}\n    $$\n\n### Variables\n\n1.  $i_t$ `(puerta de entrada)` es la variable que se utiliza para actualizar el estado de la celda $c_t$. El estado previamente oculto $c_t$ y la entrada secuencial actual $x_t$ se dan como entrada a una función sigmoidea. Si la salida está cerca a uno, más importante es la información.\n\n2.  $f_t$ `(puerta de olvido)` es la variable que decide que información debe olvidarse en el estado de celda $c_t$. El estadp previamente oculto $h_t$ y la entrada de secuencia $x_t$ se dan como entradas a una función sigmoidea. Si la salida $f_t$ está cerca de cero, entonces la información puede olvidarse, mientras que si la salida esta cerca de uno, la información debe almacenarse.\n\n3.  $g_t$ representa información importante potencialmente nueva para el estado celular $c_t$.\n\n4.  $c_t$ `(estado celular)` es una suma de:\n\n    -   Estado de celada anterior $c_{t-1}$ con alguna información olvidada $f_t$.\n\n    -   Nueva información de $g_t$.\n\n<!-- -->\n\n5.  $o_t$ `(puerta de salida)` es la variable para actualizar el estado oculto $h_t$.\n\n6.  $h_t$ `(estado oculto)` es el siguiente estado oculto que se calcula seleccionando la información importante $o_t$ del estado de celda $c_t$.\n\nLa siguiente figura muestra el gráfico computacional de la `celda` `LSTM:`\n\n![Gráfico computacional de LSTM](Figure%204.17.png)\n\nLa red LSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n-   $w_{ii}, w_{hi}, w_{if}, w_{hf}, w_{ig}, w_{hg}, w_{io}, w_{ho}$ - Pesos\n\n-   $b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{io}, b_{ho}$ - Sesgos\n\nLos modelos LSTM son lo suficientemente potentes como para aprender los comportamientos pasados más importantes y comprender si esos comportamientos pasados son características importantes para hacer predicciones futuras. Hay varias aplicaciones en las que las LSTM se utilizan mucho. Aplicaciones como reconocimiento de voz, composición musical, reconocimiento de escritura a mano.\n\nParticularmente considero que las LSTM es como un modelo que tiene su propia memoria y que puede comportarse como un humano inteligente en la toma de desiciones.\n\n## Pronósticos para el índice de AAPL utilizando una red LSTM\n\nA continuación realizaremos un ejercicio donde intentamos pronósticar el precio de cierre (Close) del índice de AAPL, la data la puede encontrar dando click en [prices-split-adjusted.csv](https://drive.google.com/file/d/1xtSPf1AcXKzOgpe8pvzQZN5HB40F-w5y/view?usp=sharing) .\n\nComenzamos importando las librerías a utilizar.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_18008\\2001173417.py:4: FutureWarning:\n\nThe pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n\n```\n:::\n:::\n\n\nCargamos el DataSet y lo almacenamos en la variable `df` y `data_df`. La data almacenada en `data_df` es la que utilizaremos para realizar todo nuestro estudio.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata_df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-01-05</th>\n      <td>WLTW</td>\n      <td>123.430000</td>\n      <td>125.839996</td>\n      <td>122.309998</td>\n      <td>126.250000</td>\n      <td>2163600.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-06</th>\n      <td>WLTW</td>\n      <td>125.239998</td>\n      <td>119.980003</td>\n      <td>119.940002</td>\n      <td>125.540001</td>\n      <td>2386400.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-07</th>\n      <td>WLTW</td>\n      <td>116.379997</td>\n      <td>114.949997</td>\n      <td>114.930000</td>\n      <td>119.739998</td>\n      <td>2489500.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-08</th>\n      <td>WLTW</td>\n      <td>115.480003</td>\n      <td>116.620003</td>\n      <td>113.500000</td>\n      <td>117.440002</td>\n      <td>2006300.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-11</th>\n      <td>WLTW</td>\n      <td>117.010002</td>\n      <td>114.970001</td>\n      <td>114.089996</td>\n      <td>117.330002</td>\n      <td>1408600.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEste bloque de código fue de mucha utilidad, dado que aquí filtramos de nuestro dataset unicamente la información para el indice de AAPL y visualizamos la data.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndata_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>30.490000</td>\n      <td>30.572857</td>\n      <td>30.340000</td>\n      <td>30.642857</td>\n      <td>123432400.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>30.657143</td>\n      <td>30.625713</td>\n      <td>30.464285</td>\n      <td>30.798571</td>\n      <td>150476200.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>30.625713</td>\n      <td>30.138571</td>\n      <td>30.107143</td>\n      <td>30.747143</td>\n      <td>138040000.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>30.250000</td>\n      <td>30.082857</td>\n      <td>29.864286</td>\n      <td>30.285715</td>\n      <td>119282800.0</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>30.042856</td>\n      <td>30.282858</td>\n      <td>29.865715</td>\n      <td>30.285715</td>\n      <td>111902700.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPreliminarmente con este bloque de código podemos ver como se comporta la serie correspondiente al precio de cierre de las acciones de AAPL.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.title('Indice de AAPL')\nplt.xlabel('días')\nplt.ylabel('precio de cierre (Close)')\nplt.legend(loc='best')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<matplotlib.legend.Legend at 0x236b99436a0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=580 height=449}\n:::\n:::\n\n\nNecesitamos manipular nuestro campo de fecha para poder manipularlas como lo que son en realidad (fechas).\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata_df['date'] = data_df.index\ndata_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n      <th>date</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>30.490000</td>\n      <td>30.572857</td>\n      <td>30.340000</td>\n      <td>30.642857</td>\n      <td>123432400.0</td>\n      <td>2010-01-04</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>30.657143</td>\n      <td>30.625713</td>\n      <td>30.464285</td>\n      <td>30.798571</td>\n      <td>150476200.0</td>\n      <td>2010-01-05</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>30.625713</td>\n      <td>30.138571</td>\n      <td>30.107143</td>\n      <td>30.747143</td>\n      <td>138040000.0</td>\n      <td>2010-01-06</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>30.250000</td>\n      <td>30.082857</td>\n      <td>29.864286</td>\n      <td>30.285715</td>\n      <td>119282800.0</td>\n      <td>2010-01-07</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>30.042856</td>\n      <td>30.282858</td>\n      <td>29.865715</td>\n      <td>30.285715</td>\n      <td>111902700.0</td>\n      <td>2010-01-08</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndata_df['date'] = pd.to_datetime(data_df['date'])\n```\n:::\n\n\nTransformamos los datos con `MinMaxScaler()` para que se distribuyan normal estándar, recuerde que esto es con media cero y varianza 1.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))\n```\n:::\n\n\nDividimos la data en datos de entrenamiento (train), tomando el 70% de los datos para entrenar nuestro modelo y un 20% para prueba (test).\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1233 529\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# convertir una matriz de valores en una matriz de conjunto de datos\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nx_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1217, 15)\n(1217,)\n(513, 15)\n(513,)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n```\n:::\n\n\nComenzamos a crear y entrenar nuestro modelo `LSTM`.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(20, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=90, batch_size=8, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/90\n153/153 - 2s - loss: 0.0174 - 2s/epoch - 16ms/step\nEpoch 2/90\n153/153 - 0s - loss: 5.3418e-04 - 273ms/epoch - 2ms/step\nEpoch 3/90\n153/153 - 0s - loss: 4.4450e-04 - 278ms/epoch - 2ms/step\nEpoch 4/90\n153/153 - 0s - loss: 4.2874e-04 - 277ms/epoch - 2ms/step\nEpoch 5/90\n153/153 - 0s - loss: 4.0739e-04 - 268ms/epoch - 2ms/step\nEpoch 6/90\n153/153 - 0s - loss: 3.8709e-04 - 284ms/epoch - 2ms/step\nEpoch 7/90\n153/153 - 0s - loss: 3.7214e-04 - 273ms/epoch - 2ms/step\nEpoch 8/90\n153/153 - 0s - loss: 3.6078e-04 - 276ms/epoch - 2ms/step\nEpoch 9/90\n153/153 - 0s - loss: 3.5206e-04 - 290ms/epoch - 2ms/step\nEpoch 10/90\n153/153 - 0s - loss: 3.2790e-04 - 269ms/epoch - 2ms/step\nEpoch 11/90\n153/153 - 0s - loss: 3.0376e-04 - 288ms/epoch - 2ms/step\nEpoch 12/90\n153/153 - 0s - loss: 3.0369e-04 - 272ms/epoch - 2ms/step\nEpoch 13/90\n153/153 - 0s - loss: 3.1673e-04 - 272ms/epoch - 2ms/step\nEpoch 14/90\n153/153 - 0s - loss: 2.6855e-04 - 288ms/epoch - 2ms/step\nEpoch 15/90\n153/153 - 0s - loss: 2.6134e-04 - 274ms/epoch - 2ms/step\nEpoch 16/90\n153/153 - 0s - loss: 2.7831e-04 - 266ms/epoch - 2ms/step\nEpoch 17/90\n153/153 - 0s - loss: 2.3250e-04 - 280ms/epoch - 2ms/step\nEpoch 18/90\n153/153 - 0s - loss: 2.4377e-04 - 277ms/epoch - 2ms/step\nEpoch 19/90\n153/153 - 0s - loss: 2.1988e-04 - 300ms/epoch - 2ms/step\nEpoch 20/90\n153/153 - 0s - loss: 2.1443e-04 - 309ms/epoch - 2ms/step\nEpoch 21/90\n153/153 - 0s - loss: 2.0727e-04 - 308ms/epoch - 2ms/step\nEpoch 22/90\n153/153 - 0s - loss: 2.1768e-04 - 303ms/epoch - 2ms/step\nEpoch 23/90\n153/153 - 0s - loss: 1.9675e-04 - 301ms/epoch - 2ms/step\nEpoch 24/90\n153/153 - 0s - loss: 1.8817e-04 - 298ms/epoch - 2ms/step\nEpoch 25/90\n153/153 - 0s - loss: 2.0940e-04 - 302ms/epoch - 2ms/step\nEpoch 26/90\n153/153 - 0s - loss: 1.8725e-04 - 273ms/epoch - 2ms/step\nEpoch 27/90\n153/153 - 0s - loss: 1.9996e-04 - 268ms/epoch - 2ms/step\nEpoch 28/90\n153/153 - 0s - loss: 1.8666e-04 - 269ms/epoch - 2ms/step\nEpoch 29/90\n153/153 - 0s - loss: 2.0690e-04 - 289ms/epoch - 2ms/step\nEpoch 30/90\n153/153 - 0s - loss: 1.8579e-04 - 290ms/epoch - 2ms/step\nEpoch 31/90\n153/153 - 0s - loss: 1.8400e-04 - 327ms/epoch - 2ms/step\nEpoch 32/90\n153/153 - 0s - loss: 2.0776e-04 - 370ms/epoch - 2ms/step\nEpoch 33/90\n153/153 - 0s - loss: 1.7955e-04 - 285ms/epoch - 2ms/step\nEpoch 34/90\n153/153 - 0s - loss: 1.8656e-04 - 283ms/epoch - 2ms/step\nEpoch 35/90\n153/153 - 0s - loss: 1.7452e-04 - 363ms/epoch - 2ms/step\nEpoch 36/90\n153/153 - 0s - loss: 1.8671e-04 - 291ms/epoch - 2ms/step\nEpoch 37/90\n153/153 - 0s - loss: 2.0337e-04 - 276ms/epoch - 2ms/step\nEpoch 38/90\n153/153 - 0s - loss: 1.8612e-04 - 290ms/epoch - 2ms/step\nEpoch 39/90\n153/153 - 0s - loss: 1.7868e-04 - 287ms/epoch - 2ms/step\nEpoch 40/90\n153/153 - 0s - loss: 1.6047e-04 - 288ms/epoch - 2ms/step\nEpoch 41/90\n153/153 - 0s - loss: 1.7351e-04 - 274ms/epoch - 2ms/step\nEpoch 42/90\n153/153 - 0s - loss: 1.7513e-04 - 277ms/epoch - 2ms/step\nEpoch 43/90\n153/153 - 0s - loss: 1.6558e-04 - 286ms/epoch - 2ms/step\nEpoch 44/90\n153/153 - 0s - loss: 1.6894e-04 - 273ms/epoch - 2ms/step\nEpoch 45/90\n153/153 - 0s - loss: 1.9839e-04 - 270ms/epoch - 2ms/step\nEpoch 46/90\n153/153 - 0s - loss: 1.6972e-04 - 283ms/epoch - 2ms/step\nEpoch 47/90\n153/153 - 0s - loss: 1.9237e-04 - 287ms/epoch - 2ms/step\nEpoch 48/90\n153/153 - 0s - loss: 1.7812e-04 - 276ms/epoch - 2ms/step\nEpoch 49/90\n153/153 - 0s - loss: 1.6258e-04 - 283ms/epoch - 2ms/step\nEpoch 50/90\n153/153 - 0s - loss: 1.5556e-04 - 296ms/epoch - 2ms/step\nEpoch 51/90\n153/153 - 0s - loss: 1.5876e-04 - 279ms/epoch - 2ms/step\nEpoch 52/90\n153/153 - 0s - loss: 1.5916e-04 - 266ms/epoch - 2ms/step\nEpoch 53/90\n153/153 - 0s - loss: 1.7933e-04 - 363ms/epoch - 2ms/step\nEpoch 54/90\n153/153 - 0s - loss: 1.6076e-04 - 298ms/epoch - 2ms/step\nEpoch 55/90\n153/153 - 0s - loss: 1.5508e-04 - 276ms/epoch - 2ms/step\nEpoch 56/90\n153/153 - 0s - loss: 2.0915e-04 - 270ms/epoch - 2ms/step\nEpoch 57/90\n153/153 - 0s - loss: 1.6386e-04 - 340ms/epoch - 2ms/step\nEpoch 58/90\n153/153 - 0s - loss: 1.8083e-04 - 276ms/epoch - 2ms/step\nEpoch 59/90\n153/153 - 0s - loss: 1.5268e-04 - 274ms/epoch - 2ms/step\nEpoch 60/90\n153/153 - 0s - loss: 1.4961e-04 - 275ms/epoch - 2ms/step\nEpoch 61/90\n153/153 - 0s - loss: 1.8783e-04 - 271ms/epoch - 2ms/step\nEpoch 62/90\n153/153 - 0s - loss: 1.4447e-04 - 280ms/epoch - 2ms/step\nEpoch 63/90\n153/153 - 0s - loss: 1.4833e-04 - 275ms/epoch - 2ms/step\nEpoch 64/90\n153/153 - 0s - loss: 1.5035e-04 - 270ms/epoch - 2ms/step\nEpoch 65/90\n153/153 - 0s - loss: 1.6430e-04 - 272ms/epoch - 2ms/step\nEpoch 66/90\n153/153 - 0s - loss: 1.5268e-04 - 280ms/epoch - 2ms/step\nEpoch 67/90\n153/153 - 0s - loss: 1.6926e-04 - 282ms/epoch - 2ms/step\nEpoch 68/90\n153/153 - 0s - loss: 1.4859e-04 - 270ms/epoch - 2ms/step\nEpoch 69/90\n153/153 - 0s - loss: 1.6119e-04 - 285ms/epoch - 2ms/step\nEpoch 70/90\n153/153 - 0s - loss: 1.6663e-04 - 262ms/epoch - 2ms/step\nEpoch 71/90\n153/153 - 0s - loss: 1.6530e-04 - 280ms/epoch - 2ms/step\nEpoch 72/90\n153/153 - 0s - loss: 1.7051e-04 - 285ms/epoch - 2ms/step\nEpoch 73/90\n153/153 - 0s - loss: 1.6448e-04 - 275ms/epoch - 2ms/step\nEpoch 74/90\n153/153 - 0s - loss: 1.6769e-04 - 278ms/epoch - 2ms/step\nEpoch 75/90\n153/153 - 0s - loss: 1.6086e-04 - 271ms/epoch - 2ms/step\nEpoch 76/90\n153/153 - 0s - loss: 1.6478e-04 - 279ms/epoch - 2ms/step\nEpoch 77/90\n153/153 - 0s - loss: 1.5804e-04 - 263ms/epoch - 2ms/step\nEpoch 78/90\n153/153 - 0s - loss: 1.5207e-04 - 334ms/epoch - 2ms/step\nEpoch 79/90\n153/153 - 0s - loss: 1.3981e-04 - 295ms/epoch - 2ms/step\nEpoch 80/90\n153/153 - 0s - loss: 1.3907e-04 - 287ms/epoch - 2ms/step\nEpoch 81/90\n153/153 - 0s - loss: 1.5976e-04 - 307ms/epoch - 2ms/step\nEpoch 82/90\n153/153 - 0s - loss: 1.6238e-04 - 339ms/epoch - 2ms/step\nEpoch 83/90\n153/153 - 0s - loss: 1.4658e-04 - 497ms/epoch - 3ms/step\nEpoch 84/90\n153/153 - 0s - loss: 1.6490e-04 - 310ms/epoch - 2ms/step\nEpoch 85/90\n153/153 - 0s - loss: 1.5613e-04 - 283ms/epoch - 2ms/step\nEpoch 86/90\n153/153 - 0s - loss: 1.5913e-04 - 302ms/epoch - 2ms/step\nEpoch 87/90\n153/153 - 0s - loss: 1.5089e-04 - 288ms/epoch - 2ms/step\nEpoch 88/90\n153/153 - 0s - loss: 1.6357e-04 - 276ms/epoch - 2ms/step\nEpoch 89/90\n153/153 - 0s - loss: 1.4136e-04 - 271ms/epoch - 2ms/step\nEpoch 90/90\n153/153 - 0s - loss: 1.4603e-04 - 275ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<keras.callbacks.History at 0x236b9801000>\n```\n:::\n:::\n\n\nEvaluamos nuestro modelo, utilizando la métrica del RMSE para los datos de train y test.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ntrainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# Invertimos las predicciones, dado que las habiamos transformado con MinMaxScaler\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculamos el RMSE\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/39 [..............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r39/39 [==============================] - 0s 1ms/step\n\r 1/17 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17/17 [==============================] - 0s 1ms/step\nTrain Score: 1.20 RMSE\nTest Score: 1.91 RMSE\n```\n:::\n:::\n\n\nFinalmente, observamos nuestro resultados gráficamente\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\nplt.plot(min_max_scaler.inverse_transform(dataset), label = \"Precio Historico\")\nplt.plot(trainPredictPlot, label = \"Datos Entrenamiento\")\nplt.plot(testPredictPlot, label = \"Predicción de Precio\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=575 height=411}\n:::\n:::\n\n\nNote que nuestros resultados son muy buenos, sin embargo, podrían mejorarse, quizas aplicando más capas ocultas a nuestra red, recuerde que solo utilizamos 20. Este podría ser un buen ejercicio para que usted intente mejorar estos resultados.\n\nRecuerda que puedes comentar este post, agradeceria que lo hagas ya sea para alguna sugerencia u observación. Saludos espero hayas conocido un poco sobre este tipo de Red Neuronal en particular y su potente poder predectivo.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}