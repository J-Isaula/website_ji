{
  "hash": "4868ffed4fa172bf586f2857f0c1c1d4",
  "result": {
    "markdown": "---\ntitle: \"Time Series Forecasting with XGBoost\"\nsubtitle: \"Utilizando Machine Learning Forecast\"\nauthor: \"Juan Isaula\"\ndate: \"2023-08-07\"\ncategories: [XGBoost, MLForecast, Python, sklearn]\nimage: \"pinguino.jpeg\"\n---\n\n# MLForecast\n\n`MLForecast` es un marco de datos para realizar pronósticos de series de tiempo utilizando modelos de aprendizaje automático, con la opción de escalar a cantidades masivas de datos utilizando clústeres remotos.\n\n### ¿Por qué utilizar MLForecast?\n\nLas alternativas actuales de Python para los modelos de aprendizaje automático son lentas, imprecisas y no escalan bien. Así que se crearón una biblioteca que se puede usar para hacer pronósticos en entornos de producción. `MLForecast` incluye ingeniería de características eficientes para entrenar cualquier modelo de aprendizaje automático (con `fit` y `predict` métodos como `sklearn`) para adaptarse a millones de series temporales.\n\n## Características\n\n-   Las implementaciones más rapidas de ingeniería de funciones para la previsión de series temporales en Python.\n\n-   Compatibilidad lista para usar con Spark, Dask y Ray\n\n-   Pronósticos probabilísticos con predicción conforme.\\|\n\n-   Soporte para variables exógenas y covariables estáticas.\n\n-   Sintaxis familiar `sklearn:` `.fit` y `.predict`\n\n## Validación Cruzada\n\nLa validación cruzada de series temporales es un método para evaluar cómo se habría comportado un modelo en el pasado. Funciona definiendo una ventana deslizante a través de los datos históricos y prediciendo el período que le sigue.\n\n`MLForecast` tiene una implementación de validación cruzada de series de tiempo que es rápida y fácil de usar. Esta implementación hace que la validación cruzada sea una operación eficiente, lo que hace que consuma menos tiempo.\n\n### ¿Cómo realizar validación cruzada de series de tiempo?\n\nUna vez que se ha instanciado el objeto `MLForecast`, podemos usar el método `cross_validation`, que toma los siguientes argumentos:\n\n-   `data:` marco de datos de entrenamiento con formato MLForecast.\n\n-   `window_size`(int)`:` representa los ***h*** pasos hacia el futuro que se pronosticarán\n\n-   `n_windows` (int)`:` cantidad de ventanas utilizadas para la validación cruzada, es decir, la cantidad de procesos de pronóstico en el pasado que desea evaluar.\n\n-   `id_col:` identifica cada serie temporal\n\n-   `time_col:` identifica la columna de la serie temporal\n\n-   `target_col:` identifica la columna a modelar\n\nEl objeto **`crossvaldation_df`** es un nuevo marco de datos que incluye las siguientes columnas:\n\n-   `unique_id:` identifica cada serie temporal\n\n-   `ds:` marca de fecha o índice temporal\n\n-   `cutoff:` la última marca de fecha o índice temporal del ***n_windows***\n\n-   `model:` columnas con el nombre del modelo y el valor ajustado.\n\n### Evaluar Resultados\n\nUna vez hecho todo lo anterior, podremos calcular la precisión del pronóstico utilizando una métrica de precisión adecuada. En mi caso particular uso el error cuadrático medio (RMSE). Para hacer esto, primero debemos instalar `datasetsforecast`, una biblioteca de Python desarrollada por **`Nixtla`** que incluye una función para calcular el RMSE.\n\nLa función para calcular el RMSE toma dos argumentos:\n\n1.  Los valores reales\n2.  Las predicciones\n\nEsta medida debería reflejar mejor las capacidades predictivas de nuestro modelo, ya que utilizo diferentes periodos de tiempo para probar su precisión.\n\n# Modelo XGBoost\n\nEl modelo XGBoost es un algoritmo de aprendizaje automático supervisado basado en árboles de decisión. Se utiliza para problemas de **regresión** y **clasificación**, se ha convertido en uno de los modelos más importantes y efectivos en las competencias de aprendizaje automático.\n\n## Toerema del Modelo XGBoost\n\nDados los datos de entrenamiento $D = (x_1, y_1), (x_2, y_2), . . . , (x_n, y_n)$ donde cada $x_i$ es un vector de características de entrada y cada $y_i$ es la etiqueta de salida correspondiente, el objetivo es encontrar una función $f(x)$ que mapee los vectores de características a las etiquetas de salida y minimice el error de predicción en el conjunto de entrenamiento.\n\n## \n\nLa construcción del modelo implica la creación de un conjunto de árboles de decisión, donde cada árbol se construye de manera secuencial para minimizar la función de costo global, que es la suma de las funciones de costo individuales de cada árbol.\n\nEl algoritmo XGBoost también utiliza técnicas de regularización para evitar el sobreajuste, lo que ayuda a mejorar el rendimiento en conjuntos de datos no visto. Estás técnicas incluyen la poda de árboles, la penalización $L_1$ y $L_2$, y el muestreo aleatorio de observaciones y variables.\n\nUna característica del modelo XGBoost es que puede trabajar con tados a lo bruto (es decir, bases que contengan datos faltantes), es tan robusto que puede trabajar con este tipo de datos.\n\n## Modelo XGBoost Matemáticamente\n\nEl modelo XGBoost se construye a través de una combinación de árboles de decisión y técnicas de optimización de gradiente. En términos matemáticos el modelo XGBoost se puede escribir como:\n\n$$\nf(x) = \\sum T(x;\\theta_j)\n$$\n\ndonde $f(x)$ es la función de predicción para el conjunto de características de entrada $x$, $T(x;\\theta_j)$ es un árbol de decisión con parámetros $\\theta_j$, y la suma se realiza sobre un conjunto de árboles de decisión.\n\nCada árbol de decisión se construye de manera secuencial para minimizar la función de costo global, que es la suma de las funciones de costo individuales de cada árbol, es decir:\n\n$$L = \\sum l(y_i,f_i(x_i)) + \\sum \\Omega(\\theta_j)$$\n\ndonde $l(y_i, f_i(x_i))$ es la función de costo para el i-ésimo ejemplo de entrenamiento, $f_i(x_i)$ es la predicción del modelo para el i-ésimo ejemplo de entrenamiento,y $\\Omega(\\theta_j)$ es la penalización para el j-ésimo árbol de decisión, diseñada para evitar el sobreajuste.\n\nLa función de costo se puede escribir de varias maneras, dependiendo del problema específico. Por ejemplo, en un problema de regresión, la función de costo podría ser el `error cuadrático medio (MSE)`, mientras que en un problema de clasificación, la función de costo podría ser la `antropía cruzada`.\n\nPara construir el modelo XGBoost, se utilizan técnicas de optimización de gradiente para minimizar la función de costo global. Estas técnicas implican calcular las derivadas de la función de costo con respecto a los parámetros del modelo, y ajustar los parámetros en consecuencia.\n\nAdemás, XGBoost utiliza técnicas de regularización para evitar el sobreajuste, como la poda de árboles, la penalización $L_1$ y $L_2$, y el muestreo aleatorio de observaciones y variables.\n\nEn resumen, el modelo `XGBoost` se construye a través de una combinación de árboles de decisión y técnicas de optimización de gradiente, con el objetivo de minimizar la función de costo global mientras se aplica la regularización para evitar el sobreajuste.\n\n## XGBoost para series de tiempo\n\nXGBoost también puede ser aplicado a problemas de series de tiempo. Sin embargo, es importante tener en cuenta que la aplicación del modelo a datos de series de tiempo requiere un enfoque ligeramente diferente en comparación con los problemas de clasificación y regresión estándar.\n\nPara aplicar XGBoost a datos de series de tiempo, es necesario crear características adecuadas para el modelo, lo que puede incluir características basadas en ventanas moviles, diferencias y tasas de cambio. También es importante considerar la estacionalidad y las tendencias en los datos de series de tiempo y aplicar técnicas de preprocesamiento de datos y validación cruzada adecuadas para este tipo de problemas.\n\nAdemás, se deben tener en cuenta algunas consideraciones especiales en la configuración de características relevantes para la serie de tiempo y la selección de la función de costo y métricas de evaluación adecuadas.\n\n# Caso de estudio\n\n### Librerías \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# 1. Manipulación y Tratamiento de Datos\nimport numpy as np\nimport pandas as pd\n\n# Convertir a un formato de datetime \nfrom datetime import datetime \n\n# Ocultar Warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# 2. Modelación XGBoost\nimport xgboost as xgb\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom sklearn.ensemble import RandomForestRegressor\nfrom numba import njit\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\n\n# 3. Métricas para evaluación del modelo \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# 4. Gráficos o Plots\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"lines.linewidth\"] = 1.5\ndark_style = {\n    \"figure.facecolor\": \"#212946\", \"axes.facecolor\": \"212946\",\n    \"savefig.facecolor\":\"212946\", \"axes.grid\":True,\"axes.grid.which\": \"both\",\n    \"axes.spines.top\": False, \"axes.spines.bottom\": False,\"grid.color\": \"#2A3459\",\n    \"grid.linewidth\": \"1\", \"text.color\":\"0.9\", \"axes.labelcolor\": \"0.9\",\n    \"xtick.color\": \"0.9\", \"ytick.color\": \"0.9\", \"font.size\": 12}\nplt.rcParams.update(dark_style)\n\n# Tamaño del gráfico \nfrom pylab import rcParams\nrcParams[\"figure.figsize\"] = (8,5)\n```\n:::\n\n\n###  Data\n\nComo ya lo he mencionado, la entrada a MLForecast siempre es un marco de datos en formato de columnas: `unique_id`, `ds` y `y`.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv(\"ipc_honduras.csv\",  parse_dates=['Date']) \ndata.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Valor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>371</th>\n      <td>2022-12-01</td>\n      <td>9.80</td>\n    </tr>\n    <tr>\n      <th>372</th>\n      <td>2023-01-01</td>\n      <td>8.93</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>2023-02-01</td>\n      <td>9.80</td>\n    </tr>\n    <tr>\n      <th>374</th>\n      <td>2023-03-01</td>\n      <td>9.05</td>\n    </tr>\n    <tr>\n      <th>375</th>\n      <td>2023-04-01</td>\n      <td>7.44</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Exploremos la serie graficamente\nsns.lineplot(data = data, x = \"Date\", y = \"Valor\", label = \"Inflación\",\n             color = \"lime\")\nplt.xlabel(\"Fecha\")\nplt.ylabel(\"% Inflación\")\nplt.title(\"Variación Interanual Honduras 1992-2023\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=738 height=475}\n:::\n:::\n\n\n### Transformando los datos al formato MLForecast\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndatos = data.copy()\ndatos = data[[\"Date\", \"Valor\"]]\nprint(datos.shape)\n\ndatos[\"unique_id\"] = \"1\"\ndatos.columns = [\"ds\", \"y\", \"unique_id\"]\ndatos.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(376, 2)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ds</th>\n      <th>y</th>\n      <th>unique_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>16.53</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-02-01</td>\n      <td>14.09</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-03-01</td>\n      <td>12.83</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-04-01</td>\n      <td>10.37</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-05-01</td>\n      <td>9.52</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Creando el Modelo \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodels = [\n    #lgb.LGBMRegressor(),\n    xgb.XGBRegressor(),\n    #RandomForestRegressor(random_state=0)\n]\n\nmodelo = MLForecast(\n    models = models, \n    freq = \"MS\",\n    lags = [1,12],\n    lag_transforms = {\n        1: [expanding_mean],\n        12:[(rolling_mean,5)]},\n    differences=[12],\n    date_features = [\"month\", \"year\"]\n    )\n    \nmodelo.fit(datos, id_col = \"unique_id\", time_col = \"ds\", target_col = \"y\")\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nMLForecast(models=[XGBRegressor], freq=<MonthBegin>, lag_features=['lag1', 'lag12', 'expanding_mean_lag1', 'rolling_mean_lag12_window_size5'], date_features=['month', 'year'], num_threads=1)\n```\n:::\n:::\n\n\nEstamos listos para poder realizar las predicciones, en este caso, vamos a pronosticar 8 pasos adelante, es decir, 8 fechas a futuro comenzando a partir del `2023-05-01 a 2023-12-01`\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\npreds = modelo.predict(8)\npreds\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>XGBRegressor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2023-05-01</td>\n      <td>6.524219</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2023-06-01</td>\n      <td>5.673842</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2023-07-01</td>\n      <td>5.086069</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2023-08-01</td>\n      <td>4.643029</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2023-09-01</td>\n      <td>4.179777</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>2023-10-01</td>\n      <td>4.326499</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>2023-11-01</td>\n      <td>4.554456</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>2023-12-01</td>\n      <td>4.326124</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nVeamos visualmente nuestros pronósticos\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\npd.concat([datos, preds]).set_index(\"ds\").plot(figsize = (9,6));\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=796 height=528}\n:::\n:::\n\n\n### Predicción de Intervalos \n\nCon `MLForecast`, puede generar intervalos de predicción. Para configurar la predicción, debe pasar una instancia de la clase `PredictionIntervals` a la clase `prediction_interval` el cual es un argumento del método `.fit`.\n\nLa clase toma tres parámetros: `n_windows`, `h` y `method.`\n\n-   `n_windows` representa el número de ventanas de validación cruzada utilizadas para calibrar los intervalos.\n\n-   `h` es el horizonte de pronóstico\n\n-   `method` puede ser `conformal_distribution` o `conformal_error`\n\nTenga en cuenta que se deben utilizar mínimo 2 ventanas de validación cruzada.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom mlforecast.utils import PredictionIntervals\nmodelo.fit(\n    datos, \n    prediction_intervals=PredictionIntervals(n_windows=9, h=8)\n);\n\n```\n:::\n\n\nDespués de eso, solo tiene que incluir los niveles de confianza deseados en el `predict` u sando el argumento `level`. Los levels deben estar entre 0 y 100.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\npredictions_w_intervals = modelo.predict(8, level=[95]) \npredictions_w_intervals.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>ds</th>\n      <th>XGBRegressor</th>\n      <th>XGBRegressor-lo-95</th>\n      <th>XGBRegressor-hi-95</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2023-05-01</td>\n      <td>6.524219</td>\n      <td>5.193575</td>\n      <td>7.854862</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2023-06-01</td>\n      <td>5.673842</td>\n      <td>4.591230</td>\n      <td>6.756455</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2023-07-01</td>\n      <td>5.086069</td>\n      <td>3.446764</td>\n      <td>6.725373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2023-08-01</td>\n      <td>4.643029</td>\n      <td>1.342779</td>\n      <td>7.943279</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2023-09-01</td>\n      <td>4.179777</td>\n      <td>-0.042535</td>\n      <td>8.402088</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\npredictions_w_intervals.set_index(\"ds\").plot(figsize = (8,6));\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=731 height=545}\n:::\n:::\n\n\nNote que las bandas de confianza para los primeros tres meses de pronóstico no son tan anchas, sin embargo, a medida que vemos mas lejos la incertidumbre se hace aún mayor, eso es lo que podemos apreciar con las bandas de confianza.\n\n# Referencias \n\n[Rob J.Hyndman y George Athanasopoulos (2018). \"Principios y Prácticas de pronósticos, validación cruzada de series de tiempo\"](https://otexts.com/fpp3/tscv.html#)\n\n<https://nixtla.github.io/mlforecast/forecast.html>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}