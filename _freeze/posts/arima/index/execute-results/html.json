{
  "hash": "b55dd3e5129b200e33978841daa57ad6",
  "result": {
    "markdown": "---\ntitle: \"Series de Tiempo\"\nauthor: \"Juan Isaula\"\ndate: \"2021-05-14\"\ncategories: [Time Series, RStudio,R]\nimage: \"Fondo4.PNG\"\n---\n\n\n# Series Temporales\n\nUna serie temporal es una colección de observaciones indexadas por la fecha de cada observación, denotada por $y_t$\n\n$$\n\\{y_1, y_2, . . . . , y_T\\}\n$$\n\nEn la práctica se asume que lo anterior es sólo una muestra, pero que la serie pudo haber sido observada en más periodos.\n\n$$\n\\{y_t\\}_{t = -\\infty}^\\infty = \\{. . . , y_{-1}, y_0, y_1, y_2, . . . , y_T,y_{T+1},y_{T+2}, . . .\\}\n$$\n\nEn el análisis de series temporales el objetivo es extraer parámetros relevantes o características de ellas. Estas características pueden ser luego utilizadas para generar un modelo matemático que describa la serie y pueda ser utilizado para realizar predicciones.\n\nDe acuerdo a la cantidad de variables variables o características que la serie temporal contenga se considera univariada para el caso de una variable, o multivariada para el caso de multiples variables. De esta fundamental característica depende los pasos a seguir para su análisis y prediccón.\n\n## Datos de Series Temporales\n\nUn paquete que utilizaremos mucho durante este post es `astsa`, *que significa Análisis de series de tiempo estadístico aplicado.*\n\nAhora, para comenzar, exploremos la naturaleza de los datos de series de tiempo. Aquí tenemos la serie de ganancias trimestrales por acción de Johnson & Johnson.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(astsa)\n\nplot(jj, main = \"Ganancias Trimestrales por Acción de Johnson & Johnson\",type = \"c\")\ntext(jj, labels = 1:4, col = 1:4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nTiene algunas características comunes de los datos de series de tiempo, tendencia al alza, estacionalidad en el sentido de que el seguendo y el tercer trimestre suelen subir, mientras que el cuarto trimestre suele bajar. Además existe heterosticidad porque, a medida que crece el valor del activo, los pequeños cambios porcentuales se convierten en grandes cambios absolutos.\n\nMuy bien previamente mencionamos dos palabras importantes en el análisis de series temporales, las cuales son *heterosticidad* y *estacionariedad*, antes de continuar definiremos esta terminología:\n\n### Estacionariedad\n\nLa base del análisis de series de tiempo es la estacionariedad. Se dice que una serie de tiempo $\\{y_t\\}$ es estrictamente estacionaria si la distribución conjunta $(y_{t_1}, . . . , y_{t_k})$ es idéntica a la de $(y_{t_1+t}, . . . , y_{t_k+t})$, para todo $t$, donde $k$ es un entero positivo arbitrario y $(t_1, . . . , t_k)$ es una colección de $k$ enteros positivos. En otras palabras la estacionariedad estricta requiere que la distribución conjunta de $(y_{t_1}, . . . , y_{t_k})$ sea invariante bajo el cambio de tiempo. Esta es una condición muy fuere que es dificil de verificar empíricamente. A menudo se asume una versión más débil de la estacionariedad. Una serie de tiempo $\\{y_t\\}$ es debilmente estacionaria si tanto la media de $y_t$ como la varianza entre $y_t$ y $y_{t-\\ell}$ son invariantes en el tiempo, donde $\\ell$ es un número entero arbitrario. Más específicamente $\\{y_t\\}$ es débilmente estacionaria si:\n\n$$\n\\begin{eqnarray}\nE(y_t) &=& \\mu\\\\[0.2cm]\nCov(y_t, y_{t-\\ell}) &=& \\gamma_t\n\\end{eqnarray}\n$$\n\nLa covarianza $\\gamma_t$ se llama autocovarianza lag(rezago)-$\\ell$ de $y_t$. Tiene dos propiedades importantes:\n\n1.  $\\gamma_0 = Var(y_t)$\n2.  $\\gamma_{-\\ell} = \\gamma_t$\n\nLa segunda propiedad se mantiene, ya que\n\n$$\nCov(y_t, y_{t - (-\\ell)}) = Cov(y_{t-(-\\ell)},y_t) = Cov(y_{t+\\ell},y_t) = Cov(y_{t_1},y_{t_1-1})\n$$\n\ndonde $t_1 = t + \\ell$.\n\n**Observación:** Supongamos que $y_t \\sim N(\\mu_t, \\sigma_t^2)$, decimos que es estacionario si $\\mu_t$ y $\\sigma_t^2$ son constantes, entonces:\n\n-   No estacionaria si $\\mu_t$ está cambiado con el tiempo.\n\n-   No estacionaria si $\\sigma_t^2$ está cambiando con el tiempo.\n\nEn las aplicaciones, la estacionariedad débil le permite a uno hacer inferencias con respecto a observaciones futuras.\n\n### Heterocedasticidad\n\nEn estadística se dice que un modelo de regresión presenta heterocedasticidad cuando la varianza de los errores no es constante en todas las observaciones realizadas.\n\nExisten diferentes razones o situaciones en las que cabe encontrarse con perturbaciones heteroscedásticas. La situación más frecuente es en el análisis de datos de corte transversal, ya que los individuos o empresas o unidades económicas no suelen tener un comportamiento homogéneo.\n\nOtra situación en la que se presenta hereroscedasticidad es en muestras cuyos datos son valores que se han obtenido agregando o promediando datos individuales.\n\nCon esto en mente, veamos nuestra segunda seria, la cual presenta las desviaciones anuales de temperatura global. Los datos son desviaciones de la temperatura promedio entre 1960 y 1980. Notará que los datos tienen una tendencia generalmente positiva, pero la tendencia no siempre es positiva. A diferencia de los datos de Johnson y Johnson, esta serie no tiene un componente estacional y es homocedástica\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(globtemp, main = \"Desviaciones de Temperatura Global\",type = \"o\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLa tercera serie son los rendimientos semanales del S&P 500. El S&P 500 es un índice bursátil estadounidense basado en 500 grandes corporaciones. Los rendimientos son el cambio porcentual por periodos de tiempo. A diferencia de las otras series, esta no tiene ninguna tendencia o estacionalidad. De hecho, parece que no hay ningún patrón en la serie (excepto que, de vez en cuando, la variación es grande). Este es un ejemplo de un tipo particular de proceso llamado ruido blanco.\n\n### Ruido Blanco\n\nCuando se estiman modelos de series de tiempo, es importante evaluar si los residuos de la estimación correspoonden a un ruido blanco.\n\nSea $\\epsilon_t$ una sucesión cuyos elementos tiene media cero y varianza $\\sigma^2$\n\n$$\n\\begin{eqnarray}\nE(\\epsilon_t) &=& 0\\hspace{2.5cm} (media\\hspace{0.1cm}cero)\\\\[0.2cm]\nE(\\epsilon_t^2) &=& \\sigma^2 \\hspace{2.3cm} (varianza\\hspace{0.1cm} constante)\\\\[0.2cm]\nE(\\epsilon_t\\epsilon_\\tau) &=& 0\\hspace{0.2cm}para\\hspace{0.1cm} t\\neq \\tau\\hspace{0.4cm}(terminos\\hspace{0.1cm}no\\hspace{0.1cm}correlacionados)\n\\end{eqnarray} \n$$\n\nSi los términos están normalmente distribuidos, es decir:\n\n$$\n\\epsilon_t \\sim N(0,\\sigma^2)\n$$\n\nentonces diremos que $\\epsilon_t$ es ruido blanco gaussiano. Una forma de evaluar si los residuos son ruido blanco es determinar si las autocorrelaciones\n\n$$\n\\rho_1 = \\rho_2 = . . . = \\rho_\\tau = 0\n$$\n\npara todo $\\tau\\geq 1$.\n\n## Test para prueba de ruido blanco\n\n### Test de Box Pierce\n\n¿Es esta serie un caso de ruido blanco?\n\n$$\nH_0: \\rho_1 = \\rho_2 = . . . = \\rho_m = 0\\hspace{0.5cm} \\fbox{si es ruido blanco}\n$$\n\n$$\nQ^* = T\\sum_{j = 1}^m \\hat{\\rho}^2\\sim \\chi_{m-k}^2\n$$\n\nSi $Q^* > \\chi_{m-k}(1-\\alpha)$, rechazar $H_0$ con $100\\alpha\\%$ de significancia: la serie no es ruido blanco.\n\nLa intuición es que si la serie no es ruido blanco, algunos $\\hat{\\rho}_j$ serán muy grandes, y entonces $Q^*$ también lo será.\n\n### Test de Ljung-Box\n\n¿Es esta serie un caso de ruido blanco?\n\n$$\nH_0: \\rho_1 = \\rho_2 = . . . = \\rho_m = 0\\hspace{0.5cm} \\fbox{si es ruido blanco}\n$$\n\n$$\nQ = T(T+2)\\sum_{j=1}^m \\frac{\\hat{\\rho}_j^2}{T - j}\\sim \\chi_{m-k}^2\n$$\n\nSi $Q > \\chi_{m-k}(1-\\alpha)$, rechazar $H_0$ con $100\\alpha\\%$ de significancia: la serie no es ruido blanco. Este test es similar al de Box-Pierce, peero ajustada para muestras pequeñas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xts)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'xts' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: zoo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'zoo' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'zoo'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n:::\n\n```{.r .cell-code}\nplot(sp500w, main = \"S&P 500 Rendimientos Semanales\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Modelo Autorregresivo de Orden $p$\n\nEn este modelo, influye los rezagos previos (es decir, observaciones pasadas). Consideremos el ruido blanco $\\epsilon_t$. El modelo $AR(P)$ es:\n\n$$\ny_t = c + \\phi y_{t-1} + . . . + \\phi_p y_{t-p} + \\epsilon_t\n$$\n\nNote que la ecuación previa la podemos escribir como:\n\n$$\n\\begin{eqnarray}\ny_t - \\phi_1 y_{t-1} - . . . - \\phi_p y_{t-p} &=& c + \\epsilon_t\\\\[0.2cm]\n\\underbrace{(1 + \\phi_1 L^1 - . . . - \\phi_p L^p)}_{\\Phi(L)} &=& c + \\epsilon_t\n\\end{eqnarray}\n$$\n\nEl proceso $AR(p)$ es una ecuación en diferencia de orden $p$. Esta ecuación es estable si y solo si las raíces del polinomio $1 - \\phi_1 z^1 - . . . - \\phi_p z^p$ están todas fuera del círculo unitario.\n\nSi el proceso es estable, resolvemos para $y_t$,\n\n$$\n\\begin{eqnarray}\ny_t &=& \\Phi^{-1}(L)(c+\\epsilon_t)\\\\[0.2cm]\n    &=& \\Phi^{-1}(1)c + \\Phi^{-1}(L)\\epsilon_t\\\\[0.2cm]\n    &=& \\frac{c}{1 - \\phi_1 - . . . - \\phi_p} + \\Phi^{-1}(L)\\epsilon_t \n\\end{eqnarray}\n$$\n\nSu valor esperado es:\n\n$$E(y_t) = \\frac{c}{1 - \\phi_1 - . . . - \\phi_p}$$\n\nPodemos escribir el proceso $AR(p)$ en términos de desviación de la media: $\\widetilde{y} = y - \\mu$\n\n$$\\widetilde{y} = \\phi_1 \\widetilde{y}_{t-1} + . . . + \\phi_p \\widetilde{y}_{t-p} + \\epsilon_{t}$$\n\nPara obtener su varianza y autocovarianza multiplicamos la expresion anterior por $\\widetilde{y}_{t-j}$ con $j \\geq 0$, y calculamos el valor esperado\n\n$$\n\\begin{eqnarray}\nE[\\widetilde{y}\\widetilde{y}_{t-j}]&=& E[\\phi_1 \\widetilde{y}_{t-1}\\widetilde{y}_{t-j} + . . . + \\phi_p \\widetilde{y}_{t-p}\\widetilde{y}_{t-j} + \\epsilon_{t}\\widetilde{y}_{t-j}]\\\\[0.2cm]\n\\gamma_{j} &=&  \\left\\{\\begin{array}{c}\\phi_{1}\\gamma_{1} + . . . + \\phi_{p}\\gamma_{p} + \\sigma^2, \\hspace{0.5cm}j=0\\\\\\phi_{1}\\gamma_{j-1} + . . . + \\phi_{p}\\gamma_{j-p},\\hspace{0.7cm}j>0\\end{array}\\right.\n\\end{eqnarray}\n$$\n\n### Función de Autocorrelación Parcial (PACF)\n\nLa autocorrelación parcial mide la correlación restante entre $y_t$ y $y_{t-1}$ una vez que se ha eliminado la influencia de $y_{t-1}, y_{t-2}, . . . , y_{t-k+1}$,\n\n$$\ny_t = a_1^{(1)}y_{t-1} + a_2^{(2)}y_{t-2} + . . . + a_{k-1}^{(k)}y_{t-k+1} + a_k^{(k)}y_{t-k}\n$$\n\nEs decir, las primeras $m$ autocorrelaciones parciales, vienen de\n\n$$\n\\begin{eqnarray*}y_{t} &=& a_{(1)}^{1}y_{t-1}:\\hspace{2.9cm} a_{1}^{(1)}\\mbox{ Autocorrelación parcial de un rezago}\\\\[0.1cm]y_{t} &=& a_{1}^{(2)}y_{t-1} + a_{2}^{(2)}y_{t-2}:\\hspace{1cm}a_{2}^{(2)}\\mbox{ Segunda autocorrelación parcial}.\\\\[0.1cm]&\\vdots&\\\\[0.1cm]y_{t} &=& a_{1}^{(m-1)}y_{t-1} + a_{2}^{(m-2)}y_{t-2} + . . . + a_{m-1}^{(m-1)}y_{t-m+1}\\\\[0.1cm]y_{t} &=& a_{1}^{(m)}y_{t-1} + a_{2}^{(m)}y_{t-2} + . . . + a_{m-1}^{(m)}y_{t-m+1} + a_{m}^{(m)}y_{t-m}\\end{eqnarray*}\n$$\n\nPara encontrar el valor de $a_k^{(k)}$ basta con resolver:\n\n$$\n\\left[\\begin{array}{ccccc}1 & \\rho_{1} & \\rho_{2} & \\cdots & \\rho_{k-1} \\\\\\rho_{1} & 1 & \\rho_{1} & \\cdots & \\rho_{k-2} \\\\\\rho_{2} & \\rho_{1} & 1 & \\cdots & \\rho_{k-3} \\\\\\vdots & \\vdots & \\vdots & \\ddots & \\cdots \\\\\\rho_{k-1} & \\rho_{k-2} & \\rho_{k-3} & \\cdots & 1\\end{array}\\right]\\left[\\begin{array}{c}a_{1}^{(1)} \\\\a_{2}^{(2)} \\\\a_{3}^{(3)} \\\\\\vdots \\\\a_{k}^{(k)}\\end{array}\\right] = \\left[\\begin{array}{c}\\rho_{1} \\\\\\rho_{2} \\\\\\rho_{3} \\\\\\vdots \\\\\\rho_{k}\\end{array}\\right]\n$$\n\nDenotaremos la $k$-ésima correlación parcial $\\varphi(k)=a_{k}^{(k)}$\n\nComparando las ecuaciones del proceso $AR(p)$ y de la autocorrelación parcial $k$:\n\n$$\n\\begin{eqnarray*}y_{t} &=& \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + . . . + \\phi_{p}y_{t-p} + \\epsilon_{t}\\\\[0.2cm]y_{t} &=& a_{1}^{(k)}y_{t-1} + a_{2}^{(k)}y_{t-2}+ . . . + a_{k-1}^{(k)}y_{t-k+1} + a_{k}^{(k)}y_{t-k}\\end{eqnarray*}\n$$\n\nVemos que si:\n\n-   Si $k = p$, entonces $\\varphi_{k} = \\phi_{p}$\n\n-   Si $k > p$, entonces $\\varphi_{k}=0$\n\n-   Si $k = 1$, entonces $\\varphi_{1}=\\rho_{1}$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}